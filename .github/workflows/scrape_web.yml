name: Telegram Web Scraper (自动/手动运行 - 上海时区)

on:

  schedule:
    - cron: '*/130 * * * *'
    

  workflow_dispatch:
  

  push:
    branches:
      - main
    paths:
      - 'scraper_web.py'
      - 'requirements.txt'

jobs:
  scrape_and_save:
    runs-on: ubuntu-latest
    
    # 设置运行环境为上海时区
    env:
      TZ: Asia/Shanghai

    steps:
      - name: 检出代码 (Fetch Full History)
        # 必须开启 fetch-depth: 0，以便 git-auto-commit 能看到所有历史目录结构
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 
          
      - name: 安装 Python 3.9
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      - name: 安装依赖 (requests, BeautifulSoup, pytz)
        run: pip install -r requirements.txt

      # =========================================================
      # 修正步骤：清理超过 7 天的旧文件 (只保留 .md 文件的清理)
      # =========================================================
      - name: 清理超过7天的文件
        run: |
          echo "开始清理超过 7 天的旧文件..."
          # 只保留对 .md 文件的清理
          find . -type f -path '*/*/*.md' -mtime +7 -delete
          # 移除 .json 和 media/ 文件的清理
          # 删除空的日期目录
          find . -type d -empty -delete
          echo "清理完成。"
      # =========================================================
      
      - name: 运行 Web 抓取脚本
        # 脚本将创建并写入新文件到 YEAR-MONTH/DAY/ 目录
        run: python scraper_web.py
        
      - name: 提交生成的 Markdown 文件
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          # 提交信息将包含清理和新增文件
          commit_message: "自动维护 [Schedule]: 清理旧文件并抓取最新内容 (${{ env.TZ }})"
          
          # 使用更通用的匹配模式，确保所有新创建的文件/目录都被追踪
          file_pattern: |
            */*/*.md
            */*/*
          
          # 使用 -A 选项，强制添加所有新增、修改和删除的文件
          add_options: '-A'
          
          commit_author: "GitHub Actions Bot <github-actions[bot]@users.noreply.github.com>"
          
          # ⭐ 核心修正：添加 --rebase 选项，解决推送时非快进式 (non-fast-forward) 冲突
          commit_options: '--no-verify --allow-empty --rebase'
