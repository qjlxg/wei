import os
import requests
import re
import shutil
from bs4 import BeautifulSoup
from datetime import datetime
import pytz
from requests.adapters import HTTPAdapter
# æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ requests.packages å…¼å®¹æ—§ç‰ˆæœ¬ï¼Œ
# å»ºè®®åŒæ—¶åœ¨ requirements.txt ä¸­åŒ…å« urllib3
from requests.packages.urllib3.util.retry import Retry 
import json

# =========================================================
# ã€é…ç½®åŒºã€‘è¦æŠ“å–çš„é¢‘é“åˆ—è¡¨
# =========================================================
CHANNEL_USERNAMES = [
    # ç°æœ‰æ ¸å¿ƒé‡‘èé¢‘é“
    'FinanceNewsDaily', 
    'SubscriptionShare', 
    'clsvip', 
    'ywcqdz',
    # æ–°å¢çš„åˆ†æä¸ç§‘æŠ€é¢‘é“
    'ushas_analysis',
    'safe_trader_academy',
    'zh_technews', 
    'MacroFinanceHub',
    'GlobalMarketUpdates',
    'AshareDailyBrief'
]
# =========================================================
# =========================================================


# è®¾ç½®ä¸Šæµ·æ—¶åŒº
SH_TZ = pytz.timezone('Asia/Shanghai')
now_shanghai = datetime.now(SH_TZ)

# --- è·¯å¾„å’Œæ–‡ä»¶åç”Ÿæˆé€»è¾‘ ---
# 1. åˆ›å»ºæ—¥æœŸç›®å½•ç»“æ„ (ä¾‹å¦‚: 2025-10/09)
DATE_DIR = now_shanghai.strftime("%Y-%m/%d")

# 2. å®Œæ•´ä¿å­˜è·¯å¾„ (ä¾‹å¦‚: 2025-10/09/media)
BASE_DIR = os.path.join(os.getcwd(), DATE_DIR)
MEDIA_DIR = os.path.join(BASE_DIR, 'media')

# 3. æ–‡ä»¶å (ä¾‹å¦‚: 15-20-00_telegram_web_content.md)
FILENAME_BASE = now_shanghai.strftime("%H-%M-%S_telegram_web_content.md")
FULL_FILENAME_PATH = os.path.join(BASE_DIR, FILENAME_BASE)
# --- è·¯å¾„å’Œæ–‡ä»¶åç”Ÿæˆé€»è¾‘ç»“æŸ ---

# --- å¸‚åœºå½±å“åˆ†æé…ç½® ---
IMPACT_KEYWORDS = {
    # ç§¯æå…³é”®è¯ (åˆ†æ•° +2)
    'positive': ['æ¶¨', 'ä¸Šæ¶¨', 'å¤§æ¶¨', 'é£™æ¶¨', 'çªç ´', 'åˆ©å¥½', 'æ–°é«˜', 'çœ‹å¥½', 'å¢æŒ', 'èµ°å¼º', 'å¤è‹', 'ç«™ä¸Š', 'æ‰©å¤§', 'åˆ©å¤š', 'é¢†å…ˆ'],
    # æ¶ˆæå…³é”®è¯ (åˆ†æ•° -2)
    'negative': ['è·Œ', 'ä¸‹è·Œ', 'å¤§è·Œ', 'èµ°ä½', 'åˆ©ç©º', 'ä¸‹è¡Œ', 'é£é™©', 'æ‹…å¿§', 'ç–²è½¯', 'æ”¶çª„', 'èµ°å¼±', 'ç¼©å‡', 'äºæŸ', 'åšç©º'],
    # ä¸­æ€§/å…³æ³¨å…³é”®è¯ (åˆ†æ•° +1 æˆ– -1)
    'neutral_positive': ['å›å‡', 'åå¼¹', 'æ¸©å’Œ', 'ä¼ç¨³', 'æ”¾é‡', 'å›è´­'],
    'neutral_negative': ['å‹åŠ›', 'æ”¾ç¼“', 'éœ‡è¡', 'å›è°ƒ', 'ç›˜æ•´', 'é«˜ä½'],
}

SECTOR_KEYWORDS = {
    'é»„é‡‘/è´µé‡‘å±': ['é»„é‡‘', 'æ²ªé‡‘', 'ç™½é“¶', 'é’¯é‡‘', 'é‡‘ä»·', 'è´µé‡‘å±'],
    'Aè‚¡/å¤§ç›˜': ['Aè‚¡', 'æ²ªæŒ‡', 'æ·±æˆæŒ‡', 'åˆ›ä¸šæ¿', 'æ²ªæ·±', 'å¸‚åœº', 'äº¬ä¸‰å¸‚', 'åŒ—å‘èµ„é‡‘'],
    'æœŸè´§/å¤§å®—å•†å“': ['æœŸè´§', 'æ£•æ¦ˆæ²¹', 'ç”ŸçŒª', 'é¸¡è›‹', 'LPG', 'é›†è¿', 'æ¶²åŒ–å¤©ç„¶æ°”', 'ç¢³é…¸é”‚', 'é“œä»·', 'åŸæ²¹', 'å¤§å®—å•†å“', 'å·¥ä¸šå“'],
    'ç§‘æŠ€/åŠå¯¼ä½“': ['èŠ¯ç‰‡', 'ç§‘åˆ›50', 'ä¸­èŠ¯å›½é™…', 'åè™¹å…¬å¸', 'å…ˆè¿›å°è£…', 'å†…å­˜', 'SSD', 'AI', 'å¤§æ¨¡å‹', 'ç®—åŠ›', 'åŠå¯¼ä½“'],
    'æ–°èƒ½æº/å‚¨èƒ½': ['ç¢³é…¸é”‚', 'å‚¨èƒ½', 'å…‰ä¼', 'ç”µæ± çº§', 'HVDC', 'æ–°èƒ½æºæ±½è½¦', 'é£ç”µ'],
    'å®è§‚/å¤®è¡Œ': ['ç¾è”å‚¨', 'å¤®è¡Œ', 'é™æ¯', 'åŠ æ¯', 'é€†å›è´­', 'SHIBOR', 'æ”¿åºœé¢„ç®—', 'ç¾å›½å›½å€º', 'å…³ç¨', 'é€šèƒ€', 'GDP', 'PMI'],
    'æ¸¯è‚¡/æ±‡ç‡': ['æ’ç”ŸæŒ‡æ•°', 'æ’æŒ‡', 'æ³°é“¢', 'ç¾å…ƒ', 'å¢æ¯”', 'æ–°åŠ å¡å…ƒ', 'æ±‡ç‡', 'æ¸¯è‚¡', 'ç¦»å²¸äººæ°‘å¸'],
    'ç¨€åœŸ': ['ç¨€åœŸ', 'å‡ºå£ç®¡åˆ¶'],
    'æ•°å­—è´§å¸': ['æ¯”ç‰¹å¸', 'ä»¥å¤ªåŠ', 'BTC', 'ETH', 'åŠ å¯†è´§å¸', 'åŒºå—é“¾'],
}


def analyze_market_impact(text, hashtags):
    """
    åŸºäºå…³é”®è¯å’Œæ ‡ç­¾å¯¹æ–‡æœ¬è¿›è¡ŒåŸºæœ¬çš„å¸‚åœºå½±å“åˆ†æã€‚
    è¿”å›ä¸€ä¸ªåŒ…å«å½±å“ã€è¡Œä¸šå’Œæƒ…ç»ªçš„å­—å…¸ã€‚
    """
    score = 0
    impact_sectors = set()
    
    # 1. è¯†åˆ«è¡Œä¸š/èµ„äº§ (åŸºäºæ–‡æœ¬å’Œæ ‡ç­¾)
    combined_content = text + " ".join(hashtags)
    for sector, keywords in SECTOR_KEYWORDS.items():
        for keyword in keywords:
            if keyword in combined_content:
                impact_sectors.add(sector)
                break
    
    # 2. è®¡ç®—æƒ…ç»ªåˆ†æ•°
    for word in IMPACT_KEYWORDS['positive']:
        score += combined_content.count(word) * 2
    for word in IMPACT_KEYWORDS['neutral_positive']:
        score += combined_content.count(word) * 1
        
    for word in IMPACT_KEYWORDS['negative']:
        score -= combined_content.count(word) * 2
    for word in IMPACT_KEYWORDS['neutral_negative']:
        score -= combined_content.count(word) * 1

    # 3. ç¡®å®šæœ€ç»ˆå½±å“æ ‡ç­¾
    if score >= 3:
        impact_label = "æ˜¾è‘—åˆ©å¥½ (Bullish)"
        impact_color = "ğŸŸ¢"
    elif score >= 1:
        impact_label = "æ½œåœ¨åˆ©å¥½ (Positive)"
        impact_color = "ğŸŸ¡"
    elif score <= -3:
        impact_label = "æ˜¾è‘—åˆ©ç©º (Bearish)"
        impact_color = "ğŸ”´"
    elif score <= -1:
        impact_label = "æ½œåœ¨åˆ©ç©º (Negative)"
        impact_color = "ğŸŸ "
    else:
        impact_label = "ä¸­æ€§/éœ€å…³æ³¨ (Neutral)"
        impact_color = "âšª"
        
    # 4. æ ¼å¼åŒ–è¾“å‡º
    if not impact_sectors:
        sector_str = "æœªè¯†åˆ«è¡Œä¸š"
    else:
        sector_str = "ã€".join(list(impact_sectors))

    summary = f"**å¸‚åœºå½±å“** {impact_color} **{impact_label}**"
    if impact_sectors:
        summary += f" - å…³æ³¨æ¿å—ï¼š{sector_str}"
        
    return summary

# --- å®ç”¨å·¥å…·å‡½æ•° ---

def setup_directories():
    """è®¾ç½®ç›®å½•å¹¶æ¸…ç†æ—§çš„åª’ä½“æ–‡ä»¶"""
    # ç¡®ä¿ä¸»ç›®å½•å­˜åœ¨ (ä¾‹å¦‚: 2025-10/09)
    os.makedirs(BASE_DIR, exist_ok=True)
    
    # æ¸…ç†æ—§çš„åª’ä½“æ–‡ä»¶å¤¹ï¼Œç¡®ä¿æ¯æ¬¡è¿è¡Œéƒ½æ˜¯å…¨æ–°çš„åª’ä½“æ–‡ä»¶
    # è¿™é‡Œçš„æ¸…ç†åªé’ˆå¯¹å½“å‰è¿è¡Œåˆ›å»ºçš„åª’ä½“æ–‡ä»¶å¤¹ï¼Œé˜²æ­¢æäº¤æ—¶åŒ…å«ä¸Šä¸€æ¬¡è¿è¡Œçš„åª’ä½“æ–‡ä»¶
    if os.path.exists(MEDIA_DIR):
        shutil.rmtree(MEDIA_DIR)
    os.makedirs(MEDIA_DIR, exist_ok=True)

    print(f"æ•°æ®å°†ä¿å­˜åˆ°ç›®å½•: {BASE_DIR}")
    

def get_channel_content(username):
    """ä» Telegram Web é¢„è§ˆé¡µé¢æŠ“å–å†…å®¹"""
    url = f"https://t.me/s/{username}"
    all_messages = []
    downloaded_count = 0
    
    print(f"å¼€å§‹æŠ“å– Web é¢„è§ˆé¡µé¢: {url}...")
    
    try:
        # è®¾ç½® requests é‡è¯•æœºåˆ¶ï¼Œå¢å¼ºæŠ“å–ç¨³å®šæ€§
        session = requests.Session()
        retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
        session.mount('https://', HTTPAdapter(max_retries=retries))
        response = session.get(url, timeout=10)
        response.raise_for_status() 
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # å¯»æ‰¾æ‰€æœ‰çš„æ¶ˆæ¯å®¹å™¨
        messages = soup.find_all('div', class_='tgme_widget_message', limit=10)
        
        if not messages:
            return f"## é¢‘é“: @{username}ï¼ˆå…± 0 æ¡æ¶ˆæ¯ï¼‰\n\n**è­¦å‘Š:** æœªæ‰¾åˆ°ä»»ä½•æ¶ˆæ¯ï¼Œè¯¥é¢‘é“å¯èƒ½ä¸å­˜åœ¨æˆ–å¯ç”¨äº†å†…å®¹é™åˆ¶ã€‚\n"

        for message in messages:
            msg_text = ""
            message_id = 'N/A'
            clean_text = ""
            hashtags = []
            media_tag = None
            
            # 1. è·å–æ¶ˆæ¯IDå’Œæ—¶é—´æˆ³
            link_tag = message.find('a', class_='tgme_widget_message_date')
            if link_tag and 'href' in link_tag.attrs:
                parts = link_tag['href'].split('/')
                message_id = parts[-1] if parts[-1].isdigit() else 'N/A'
                
                time_tag = link_tag.find('time')
                if time_tag and 'datetime' in time_tag.attrs:
                    time_utc = datetime.fromisoformat(time_tag['datetime'].replace('Z', '+00:00'))
                    time_sh = time_utc.astimezone(SH_TZ).strftime('%Y-%m-%d %H:%M:%S')
                    
                    msg_text += f"---\n**æ—¶é—´ (ä¸Šæµ·):** {time_sh} **(ID:** `{message_id}` **)**\n"
            
            # 2. æå–å¹¶æ¸…ç†æ¶ˆæ¯æ–‡æœ¬å†…å®¹
            text_tag = message.find('div', class_='tgme_widget_message_text')
            if text_tag:
                # æ”¹è¿›æ–‡æœ¬æå–ï¼Œå°† <br> è§†ä¸ºæ¢è¡Œç¬¦ï¼Œå¹¶ç§»é™¤ç©ºæ ¼
                clean_text = text_tag.get_text(separator='\n', strip=True)
                
            # 3. æå–å¹¶æ¸…ç† Hashtag
            hashtags = re.findall(r'#\w+', clean_text)
            if hashtags:
                msg_text += "\n**æ ‡ç­¾**: " + ", ".join(hashtags) + "\n"
                # ä»æ–‡æœ¬ä¸­ç§»é™¤ hashtags
                clean_text = re.sub(r'#\w+', '', clean_text).strip()
            
            # 4. åª’ä½“ä¸‹è½½å’Œæ ‡è®°
            media_tag = message.find('a', class_='tgme_widget_message_photo_wrap') or \
                        message.find('a', class_='tgme_widget_message_document_wrap')
            
            if media_tag and 'style' in media_tag.attrs:
                url_match = re.search(r'url\(["\']?(.*?)["\']?\)', media_tag['style'])
                
                if url_match and message_id != 'N/A':
                    media_url = url_match.group(1)
                    # åŠ¨æ€æå–æ‰©å±•å
                    media_extension = os.path.splitext(media_url.split('?')[0])[1] or '.jpg'
                    
                    media_filename_relative = os.path.join('media', f"{username}_{message_id}{media_extension}")
                    media_filename_full = os.path.join(BASE_DIR, media_filename_relative)

                    try:
                        media_response = session.get(media_url, timeout=10)
                        if media_response.status_code == 200:
                            with open(media_filename_full, 'wb') as f:
                                f.write(media_response.content)
                            # Markdown é“¾æ¥ä½¿ç”¨ POSIX é£æ ¼è·¯å¾„ (/)
                            md_path = os.path.join(DATE_DIR, media_filename_relative).replace(os.path.sep, '/')
                            msg_text += f"\n![åª’ä½“æ–‡ä»¶]({md_path})\n"
                            downloaded_count += 1
                        else:
                            msg_text += f"\n*[åª’ä½“æ–‡ä»¶ä¸‹è½½å¤±è´¥: HTTP {media_response.status_code}]*\n"
                    except requests.exceptions.RequestException as download_err:
                        msg_text += f"\n*[åª’ä½“æ–‡ä»¶ä¸‹è½½å¤±è´¥: {download_err}]*\n"
                elif media_tag:
                    msg_text += f"\n*[åŒ…å«åª’ä½“/æ–‡ä»¶ï¼Œè¯·æŸ¥çœ‹åŸå§‹é“¾æ¥]({url})*\n"

            # 5. å¸‚åœºå½±å“åˆ†æ
            impact_summary = analyze_market_impact(clean_text, hashtags)
            
            # 6. è·³è¿‡ç©ºæ¶ˆæ¯ï¼ˆæ— æ–‡æœ¬ä¸”æ— åª’ä½“ï¼‰
            if not clean_text and not media_tag:
                continue

            # 7. æ·»åŠ æ¸…ç†åçš„æ–‡æœ¬å’Œåˆ†æç»“æœ
            if clean_text:
                msg_text += f"\n{clean_text}\n"

            # 8. æ·»åŠ åˆ†æç»“æœ
            msg_text += f"\n{impact_summary}\n"
            
            # 9. åŸå§‹æ¶ˆæ¯é“¾æ¥
            if message_id != 'N/A':
                msg_text += f"\n**[åŸå§‹é“¾æ¥](https://t.me/{username}/{message_id})**\n"
            
            all_messages.append(msg_text)
        
        print(f"é¢‘é“ @{username} æŠ“å–å®Œæˆï¼Œå…± {len(all_messages)} æ¡æ¶ˆæ¯ï¼Œä¸‹è½½åª’ä½“: {downloaded_count} ä¸ªã€‚")

    except requests.HTTPError as e:
        error_msg = f"HTTP é”™è¯¯ (å¯èƒ½æ˜¯ 404 æˆ– 403): {e}. URL: {url}"
        print(error_msg)
        return f"## é¢‘é“: @{username}ï¼ˆå…± 0 æ¡æ¶ˆæ¯ï¼‰\n\n**æŠ“å–å¤±è´¥ (HTTP é”™è¯¯):** {e}\n"
    except Exception as e:
        error_msg = f"æŠ“å– @{username} å¤±è´¥: {e}"
        print(error_msg)
        return f"## é¢‘é“: @{username}ï¼ˆå…± 0 æ¡æ¶ˆæ¯ï¼‰\n\n**æŠ“å–å¤±è´¥ (æœªçŸ¥é”™è¯¯):** {e}\n"

    # 10. æ·»åŠ æ¶ˆæ¯è®¡æ•°æ ‡é¢˜
    header = f"## é¢‘é“: @{username}ï¼ˆå…± {len(all_messages)} æ¡æ¶ˆæ¯ï¼‰\n\n"
    return header + "\n".join(all_messages)

def main():
    """ä¸»å‡½æ•°"""
    setup_directories() # åˆ›å»ºå¹¶æ¸…ç†ç›®å½•

    all_content = f"# Telegram é¢‘é“å†…å®¹æŠ“å– (Web é¢„è§ˆ)\n\n**æŠ“å–æ—¶é—´ (ä¸Šæµ·):** {now_shanghai.strftime('%Y-%m-%d %H:%M:%S')}\n\n---\n"
    
    for username in CHANNEL_USERNAMES:
        channel_content = get_channel_content(username)
        all_content += channel_content

    # å°†æ‰€æœ‰å†…å®¹å†™å…¥ Markdown æ–‡ä»¶
    with open(FULL_FILENAME_PATH, 'w', encoding='utf-8') as f:
        f.write(all_content)
        
    print(f"\nâœ… æ‰€æœ‰å†…å®¹å·²æˆåŠŸä¿å­˜åˆ° **{FULL_FILENAME_PATH}** æ–‡ä»¶ä¸­ã€‚")

if __name__ == '__main__':
    main()
