import os
import requests
import re
# import shutil  # ç§»é™¤ï¼šä¸å†éœ€è¦æ¸…ç†åª’ä½“ç›®å½•
from bs4 import BeautifulSoup
from datetime import datetime
import pytz
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry 
# import json # ç§»é™¤ï¼šä¸å†éœ€è¦ç”ŸæˆJSON
from collections import Counter 
CHANNEL_USERNAMES = ['FinanceNewsDaily','clsvip','ywcqdz']
# è®¾ç½®ä¸Šæµ·æ—¶åŒº
SH_TZ = pytz.timezone('Asia/Shanghai')
now_shanghai = datetime.now(SH_TZ)

# --- è·¯å¾„å’Œæ–‡ä»¶åç”Ÿæˆé€»è¾‘ ---
# 1. åˆ›å»ºæ—¥æœŸç›®å½•ç»“æ„ (ä¾‹å¦‚: 2025-10/09)
DATE_DIR = now_shanghai.strftime("%Y-%m/%d")

# 2. å®Œæ•´ä¿å­˜è·¯å¾„ (ä¾‹å¦‚: 2025-10/09/media)
BASE_DIR = os.path.join(os.getcwd(), DATE_DIR)
# MEDIA_DIR = os.path.join(BASE_DIR, 'media') # ç§»é™¤ï¼šä¸å†éœ€è¦åª’ä½“ç›®å½•

# 3. æ–‡ä»¶å (ä¾‹å¦‚: 15-20-00_telegram_web_content.md)
FILENAME_BASE = now_shanghai.strftime("%H-%M-%S_telegram_web_content.md")
FULL_FILENAME_PATH = os.path.join(BASE_DIR, FILENAME_BASE)
# --- è·¯å¾„å’Œæ–‡ä»¶åç”Ÿæˆé€»è¾‘ç»“æŸ ---

# --- å¸‚åœºå½±å“åˆ†æé…ç½® (å·²å¢å¼ºäº¤æ˜“ä¿¡å·å’Œç¾è‚¡å…³é”®è¯) ---
IMPACT_KEYWORDS = {
    # ç§¯æå…³é”®è¯ (åˆ†æ•° +2)
    'positive': ['æ¶¨', 'ä¸Šæ¶¨', 'å¤§æ¶¨', 'é£™æ¶¨', 'çªç ´', 'åˆ©å¥½', 'æ–°é«˜', 'çœ‹å¥½', 'å¢æŒ', 'èµ°å¼º', 'å¤è‹', 'æ‰©å¤§', 'åˆ©å¤š', 'é¢†å…ˆ', 
                 'rally', 'surge', 'breakout', 'ä¹°å…¥', 'åšå¤š', 'å…¥åœº', 'ç›®æ ‡ä»·', 'å¼ºåŠ›æ”¯æ’‘', 'long', 'buy'],
    # æ¶ˆæå…³é”®è¯ (åˆ†æ•° -2)
    'negative': ['è·Œ', 'ä¸‹è·Œ', 'å¤§è·Œ', 'èµ°ä½', 'åˆ©ç©º', 'ä¸‹è¡Œ', 'é£é™©', 'æ‹…å¿§', 'ç–²è½¯', 'æ”¶çª„', 'èµ°å¼±', 'ç¼©å‡', 'äºæŸ', 
                 'drop', 'decline', 'correction', 'å–å‡º', 'åšç©º', 'æ­¢æŸ', 'æ¸…ä»“', 'ç¦»åœº', 'è·Œç ´', 'é˜»åŠ›ä½', 'short', 'sell'],
    # ä¸­æ€§/å…³æ³¨å…³é”®è¯ (åˆ†æ•° +1 æˆ– -1)
    'neutral_positive': ['å›å‡', 'åå¼¹', 'æ¸©å’Œ', 'ä¼ç¨³', 'æ”¾é‡', 'å›è´­', 'rebound', 'stabilize'],
    'neutral_negative': ['å‹åŠ›', 'æ”¾ç¼“', 'éœ‡è¡', 'å›è°ƒ', 'ç›˜æ•´', 'é«˜ä½', 'volatility', 'pullback', 'è§‚æœ›', 'ç­‰å¾…'],
}

SECTOR_KEYWORDS = {
    'é»„é‡‘/è´µé‡‘å±': ['é»„é‡‘', 'æ²ªé‡‘', 'ç™½é“¶', 'é’¯é‡‘', 'é‡‘ä»·', 'è´µé‡‘å±', 'XAUUSD'],
    'Aè‚¡/å¤§ç›˜': ['Aè‚¡', 'æ²ªæŒ‡', 'æ·±æˆæŒ‡', 'åˆ›ä¸šæ¿', 'æ²ªæ·±', 'å¸‚åœº', 'äº¬ä¸‰å¸‚', 'åŒ—å‘èµ„é‡‘', 'Nifty'],
    'æœŸè´§/å¤§å®—å•†å“': ['æœŸè´§', 'æ£•æ¦ˆæ²¹', 'ç”ŸçŒª', 'é¸¡è›‹', 'LPG', 'é›†è¿', 'æ¶²åŒ–å¤©ç„¶æ°”', 'ç¢³é…¸é”‚', 'é“œä»·', 'åŸæ²¹', 'å¤§å®—å•†å“', 'å·¥ä¸šå“', 
                   'å¤©ç„¶æ°”', 'é“œ', 'é“', 'é•', 'é“çŸ¿çŸ³', 'å¸ƒä¼¦ç‰¹'], 
    'ç§‘æŠ€/åŠå¯¼ä½“': ['èŠ¯ç‰‡', 'ç§‘åˆ›50', 'ä¸­èŠ¯å›½é™…', 'åè™¹å…¬å¸', 'å…ˆè¿›å°è£…', 'å†…å­˜', 'SSD', 'AI', 'å¤§æ¨¡å‹', 'ç®—åŠ›', 'åŠå¯¼ä½“', 'TechNews'],
    'æ–°èƒ½æº/å‚¨èƒ½': ['ç¢³é…¸é”‚', 'å‚¨èƒ½', 'å…‰ä¼', 'ç”µæ± çº§', 'HVDC', 'æ–°èƒ½æºæ±½è½¦', 'é£ç”µ'],
    'å®è§‚/å¤®è¡Œ': ['ç¾è”å‚¨', 'å¤®è¡Œ', 'é™æ¯', 'åŠ æ¯', 'é€†å›è´­', 'SHIBOR', 'æ”¿åºœé¢„ç®—', 'ç¾å›½å›½å€º', 'å…³ç¨', 'é€šèƒ€', 'GDP', 'PMI', 'GlobalMacro'],
    'æ¸¯è‚¡/æ±‡ç‡': ['æ’ç”ŸæŒ‡æ•°', 'æ’æŒ‡', 'æ³°é“¢', 'ç¾å…ƒ', 'å¢æ¯”', 'æ–°åŠ å¡å…ƒ', 'æ±‡ç‡', 'æ¸¯è‚¡', 'ç¦»å²¸äººæ°‘å¸'],
    'ç¨€åœŸ': ['ç¨€åœŸ', 'å‡ºå£ç®¡åˆ¶'],
    'æ•°å­—è´§å¸': ['æ¯”ç‰¹å¸', 'ä»¥å¤ªåŠ', 'BTC', 'ETH', 'åŠ å¯†è´§å¸', 'åŒºå—é“¾', 'Solana', 'CryptoMarketUpdates'],
    'æŒ‡æ•°/é“¶è¡Œ': ['Bank Nifty', 'Nifty', 'æŒ‡æ•°', 'é“¶è¡Œè‚¡'],
    'å…¨çƒ/å¤–æ±‡': ['å¤–æ±‡', 'USD', 'EUR', 'GBP', 'å…¨çƒå¸‚åœº'],
    'åŸºé‡‘/ETF': ['ETF', 'åŸºé‡‘', 'é»„é‡‘ETF', 'æœ‰è‰²ETF', 'æŒ‡æ•°åŸºé‡‘', 'é¿é™©åŸºé‡‘'],
    'æœŸæƒ/äº¤æ˜“ä¿¡å·': ['æœŸæƒ', 'ä¿¡å·', 'åšå¤š', 'åšç©º', 'çœ‹æ¶¨', 'çœ‹è·Œ', 'åˆçº¦'],
    # ç¾è‚¡ä¸“å±å…³é”®è¯ (å·²ç»†åŒ–)
    'ç¾è‚¡': ['ç¾è‚¡', 'NASDAQ', 'S&P', 'Dow', 'US30', 'AMD', 'NVDA', 'AAPL', 'TSLA', 'GOOG', 'MSFT', 'META', 'AMZN', 
             'Dow Jones', 'S&P 500', 'è´¢æŠ¥', 'EPS', 'Guidance', 'è´¢æµ‹'],
    # !!! æ–°å¢æ¿å—ï¼šç”¨äºè¯†åˆ«æ—¥éŸ©/äºšå¤ªå¸‚åœº !!!
    'æ—¥éŸ©/äºšå¤ª': ['æ—¥ç»225', 'ä¸œè¯æŒ‡æ•°', 'KOSPI', 'äºšå¤ªå¸‚åœº', 'æ—¥å…ƒ', 'æ—¥æœ¬', 'éŸ©å›½', 'æ—¥ç»', 'æ—¥æœ¬å¸‚åœº'],
}

# ç”¨äºè¯†åˆ«å¹¶æå–è¢«æåŠçš„ä¸ªè‚¡æˆ–é‡è¦èµ„äº§åç§° (æ–°åŠŸèƒ½)
MENTIONED_ASSETS = [
    # Aè‚¡/æ¸¯è‚¡/å›½å†…èµ„äº§
    'ä¸­èŠ¯å›½é™…', 'åè™¹å…¬å¸', 'ç´«é‡‘çŸ¿ä¸š', 'æ´›é˜³é’¼ä¸š', 'åŒ—æ–¹ç¨€åœŸ', 'ä¸­å›½é“ä¸š', 'èµ¤å³°é»„é‡‘', 'å››å·é»„é‡‘', 'å±±é‡‘å›½é™…', 'è¥¿éƒ¨é»„é‡‘', 'å±±ä¸œé»„é‡‘', 'ä¸­é‡‘é»„é‡‘',
    # å›½é™…/ç¾è‚¡èµ„äº§
    'AAPL', 'NVDA', 'AMD', 'TSLA', 'GOOG', 'MSFT', 'META', 'AMZN', 'çŸ³æ²¹', 'å¤©ç„¶æ°”', 'æ¯”ç‰¹å¸'
]


def analyze_market_impact(text, hashtags):
    """
    åŸºäºå…³é”®è¯å’Œæ ‡ç­¾å¯¹æ–‡æœ¬è¿›è¡ŒåŸºæœ¬çš„å¸‚åœºå½±å“åˆ†æã€‚
    æ–°å¢äº†æåŠèµ„äº§çš„è¯†åˆ«ã€‚
    """
    score = 0
    impact_sectors = set()
    mentioned_assets = set()
    
    combined_content = text + " ".join(hashtags)
    
    # 1. è¯†åˆ«è¡Œä¸š/èµ„äº§
    for sector, keywords in SECTOR_KEYWORDS.items():
        for keyword in keywords:
            if keyword in combined_content:
                impact_sectors.add(sector)
                break
    
    # 2. è¯†åˆ«æåŠèµ„äº§ (æ–°åŠŸèƒ½)
    for asset in MENTIONED_ASSETS:
        if asset in combined_content:
            # ç¡®ä¿åªè®°å½•ä¸€æ¬¡
            mentioned_assets.add(asset) 

    # 3. è®¡ç®—æƒ…ç»ªåˆ†æ•°
    for word in IMPACT_KEYWORDS['positive']:
        score += combined_content.count(word) * 2
    for word in IMPACT_KEYWORDS['neutral_positive']:
        score += combined_content.count(word) * 1
        
    for word in IMPACT_KEYWORDS['negative']:
        score -= combined_content.count(word) * 2
    for word in IMPACT_KEYWORDS['neutral_negative']:
        score -= combined_content.count(word) * 1

    # 4. ç¡®å®šæœ€ç»ˆå½±å“æ ‡ç­¾
    if score >= 3:
        impact_label = "æ˜¾è‘—åˆ©å¥½ (Bullish)"
        impact_color = "ğŸŸ¢"
    elif score >= 1:
        impact_label = "æ½œåœ¨åˆ©å¥½ (Positive)"
        impact_color = "ğŸŸ¡"
    elif score <= -3:
        impact_label = "æ˜¾è‘—åˆ©ç©º (Bearish)"
        impact_color = "ğŸ”´"
    elif score <= -1:
        impact_label = "æ½œåœ¨åˆ©ç©º (Negative)"
        impact_color = "ğŸŸ "
    else:
        impact_label = "ä¸­æ€§/éœ€å…³æ³¨ (Neutral)"
        impact_color = "âšª"
        
    # 5. æ ¼å¼åŒ–è¾“å‡º
    sector_str = "ã€".join(list(impact_sectors)) if impact_sectors else "æœªè¯†åˆ«è¡Œä¸š"
    
    summary = f"**å¸‚åœºå½±å“** {impact_color} **{impact_label}** - å…³æ³¨æ¿å—ï¼š{sector_str}"
        
    return summary, list(mentioned_assets) # è¿”å›æ€»ç»“å’ŒæåŠèµ„äº§åˆ—è¡¨

# --- å®ç”¨å·¥å…·å‡½æ•° ---

def setup_directories():
    """è®¾ç½®ç›®å½•"""
    # ç¡®ä¿ä¸»ç›®å½•å­˜åœ¨ (ä¾‹å¦‚: 2025-10/09)
    os.makedirs(BASE_DIR, exist_ok=True)
    
    # ç§»é™¤åª’ä½“ç›®å½•æ¸…ç†é€»è¾‘
    # if os.path.exists(MEDIA_DIR):
    #     shutil.rmtree(MEDIA_DIR)
    # os.makedirs(MEDIA_DIR, exist_ok=True)

    print(f"æ•°æ®å°†ä¿å­˜åˆ°ç›®å½•: {BASE_DIR}")
    

def get_channel_content(username):
    """ä» Telegram Web é¢„è§ˆé¡µé¢æŠ“å–å†…å®¹"""
    url = f"https://t.me/s/{username}"
    all_messages = []
    # downloaded_count = 0 # ç§»é™¤ï¼šä¸å†è®¡æ•°åª’ä½“æ–‡ä»¶
    
    print(f"å¼€å§‹æŠ“å– Web é¢„è§ˆé¡µé¢: {url}...")
    
    try:
        # è®¾ç½® requests é‡è¯•æœºåˆ¶ï¼Œå¢å¼ºæŠ“å–ç¨³å®šæ€§
        session = requests.Session()
        retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
        session.mount('https://', HTTPAdapter(max_retries=retries))
        response = session.get(url, timeout=10)
        response.raise_for_status() 
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # å¯»æ‰¾æ‰€æœ‰çš„æ¶ˆæ¯å®¹å™¨ (ä¼˜åŒ–ï¼šæé«˜é™åˆ¶åˆ° 20 æ¡æ¶ˆæ¯)
        messages = soup.find_all('div', class_='tgme_widget_message', limit=20) 
        
        if not messages:
            return f"## é¢‘é“: @{username}ï¼ˆå…± 0 æ¡æ¶ˆæ¯ï¼‰\n\n**è­¦å‘Š:** æœªæ‰¾åˆ°ä»»ä½•æ¶ˆæ¯ï¼Œè¯¥é¢‘é“å¯èƒ½ä¸å­˜åœ¨æˆ–å¯ç”¨äº†å†…å®¹é™åˆ¶ã€‚\n"

        for message in messages:
            msg_text = ""
            message_id = 'N/A'
            clean_text = ""
            hashtags = []
            media_tag = None
            
            # 1. è·å–æ¶ˆæ¯IDå’Œæ—¶é—´æˆ³
            link_tag = message.find('a', class_='tgme_widget_message_date')
            if link_tag and 'href' in link_tag.attrs:
                parts = link_tag['href'].split('/')
                message_id = parts[-1] if parts[-1].isdigit() else 'N/A'
                
                time_tag = link_tag.find('time')
                if time_tag and 'datetime' in time_tag.attrs:
                    time_utc = datetime.fromisoformat(time_tag['datetime'].replace('Z', '+00:00'))
                    time_sh = time_utc.astimezone(SH_TZ).strftime('%Y-%m-%d %H:%M:%S')
                    
                    msg_text += f"---\n**æ—¶é—´ (ä¸Šæµ·):** {time_sh} **(ID:** `{message_id}` **)**\n"
            
            # 2. æå–å¹¶æ¸…ç†æ¶ˆæ¯æ–‡æœ¬å†…å®¹
            text_tag = message.find('div', class_='tgme_widget_message_text')
            if text_tag:
                # æ”¹è¿›æ–‡æœ¬æå–ï¼Œå°† <br> è§†ä¸ºæ¢è¡Œç¬¦
                clean_text = text_tag.get_text(separator='\n', strip=True)
                
            # 3. æå–å¹¶æ¸…ç† Hashtag
            hashtags = re.findall(r'#\w+', clean_text)
            if hashtags:
                msg_text += "\n**æ ‡ç­¾**: " + ", ".join(hashtags) + "\n"
                # ä»æ–‡æœ¬ä¸­ç§»é™¤ hashtags
                clean_text = re.sub(r'#\w+', '', clean_text).strip()
            
            # 4. åª’ä½“æ ‡è®° (ä¸å†ä¸‹è½½)
            media_tag = message.find('a', class_='tgme_widget_message_photo_wrap') or \
                        message.find('a', class_='tgme_widget_message_document_wrap')
            
            if media_tag:
                # ä»…æ ‡è®°å­˜åœ¨åª’ä½“ï¼Œä¸è¿›è¡Œä¸‹è½½
                msg_text += f"\n*[åŒ…å«åª’ä½“/æ–‡ä»¶ï¼Œè¯·æŸ¥çœ‹åŸå§‹é“¾æ¥]({url})*\n"
                
            # 5. å¸‚åœºå½±å“åˆ†æ
            impact_summary, mentioned_assets = analyze_market_impact(clean_text, hashtags)
            
            # 6. è·³è¿‡ç©ºæ¶ˆæ¯ï¼ˆæ— æ–‡æœ¬ä¸”æ— åª’ä½“ï¼‰
            if not clean_text and not media_tag:
                continue

            # 7. æ·»åŠ æ¸…ç†åçš„æ–‡æœ¬
            if clean_text:
                msg_text += f"\n{clean_text}\n"

            # 8. æ·»åŠ åˆ†æç»“æœ (åŒ…æ‹¬æ–°å¢çš„æåŠèµ„äº§)
            if mentioned_assets:
                msg_text += f"\n**æåŠèµ„äº§**ï¼š{', '.join(mentioned_assets)}\n"
                
            msg_text += f"\n{impact_summary}\n"
            
            # 9. åŸå§‹æ¶ˆæ¯é“¾æ¥
            if message_id != 'N/A':
                msg_text += f"\n**[åŸå§‹é“¾æ¥](https://t.me/{username}/{message_id})**\n"
            
            all_messages.append(msg_text)
        
        # è°ƒæ•´æ‰“å°ä¿¡æ¯ï¼Œç§»é™¤åª’ä½“è®¡æ•°
        print(f"é¢‘é“ @{username} æŠ“å–å®Œæˆï¼Œå…± {len(all_messages)} æ¡æ¶ˆæ¯ã€‚")

    except requests.HTTPError as e:
        error_msg = f"HTTP é”™è¯¯ (å¯èƒ½æ˜¯ 404 æˆ– 403): {e}. URL: {url}"
        print(error_msg)
        return f"## é¢‘é“: @{username}ï¼ˆå…± 0 æ¡æ¶ˆæ¯ï¼‰\n\n**æŠ“å–å¤±è´¥ (HTTP é”™è¯¯):** {e}\n"
    except Exception as e:
        error_msg = f"æŠ“å– @{username} å¤±è´¥: {e}"
        print(error_msg)
        return f"## é¢‘é“: @{username}ï¼ˆå…± 0 æ¡æ¶ˆæ¯ï¼‰\n\n**æŠ“å–å¤±è´¥ (æœªçŸ¥é”™è¯¯):** {e}\n"

    # 10. æ·»åŠ æ¶ˆæ¯è®¡æ•°æ ‡é¢˜
    header = f"## é¢‘é“: @{username}ï¼ˆå…± {len(all_messages)} æ¡æ¶ˆæ¯ï¼‰\n\n"
    return header + "\n".join(all_messages)

# ç§»é™¤ generate_overall_summary å‡½æ•°

def main():
    """ä¸»å‡½æ•°"""
    setup_directories() # åˆ›å»ºç›®å½•

    all_content = f"# Telegram é¢‘é“å†…å®¹æŠ“å– (Web é¢„è§ˆ)\n\n**æŠ“å–æ—¶é—´ (ä¸Šæµ·):** {now_shanghai.strftime('%Y-%m-%d %H:%M:%S')}\n\n---\n"
    
    for username in CHANNEL_USERNAMES:
        channel_content = get_channel_content(username)
        all_content += channel_content

    # å°†æ‰€æœ‰å†…å®¹å†™å…¥ Markdown æ–‡ä»¶
    with open(FULL_FILENAME_PATH, 'w', encoding='utf-8') as f:
        f.write(all_content)
        
    print(f"\nâœ… æ‰€æœ‰å†…å®¹å·²æˆåŠŸä¿å­˜åˆ° **{FULL_FILENAME_PATH}** æ–‡ä»¶ä¸­ã€‚")
    
    # ç§»é™¤ç”Ÿæˆæ•´ä½“æ€»ç»“ JSON çš„è°ƒç”¨
    # generate_overall_summary(all_content)

if __name__ == '__main__':
    main()
