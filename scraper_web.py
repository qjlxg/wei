import os
import requests
import re
import shutil
from bs4 import BeautifulSoup
from datetime import datetime
import pytz
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry 
import json
from collections import Counter

# =========================================================
# ã€é…ç½®åŒºã€‘è¦æŠ“å–çš„é¢‘é“åˆ—è¡¨
# =========================================================
CHANNEL_USERNAMES = [
    # ç°æœ‰æ ¸å¿ƒé‡‘èé¢‘é“
    'FinanceNewsDaily', 
    'SubscriptionShare', 
    'clsvip', 
    'ywcqdz',
    # éªŒè¯æœ‰æ•ˆ/ä¿®æ­£çš„æ–°å¢é¢‘é“ (å»é™¤ 0 æ¶ˆæ¯)
    'ushasanalysis',       # ä¿®æ­£ ushas_analysis
    'thesafetraderacademy', # æ›¿æ¢ safe_trader_academy
    'TechNewsTodayBot',    # æ›¿æ¢ zh_technews
    'MacroHub',            # æ›¿æ¢ MacroFinanceHub
    'GlobalMarketUpdates', # ä¿ç•™
    'ChineseStockMarket',  # æ›¿æ¢ AshareDailyBrief
    'NiftyProX',           # ä¿®æ­£ niftyprox
    'equity99',
    'learn2tradenews',     # ä¿®æ­£ learn2trade
    'TechNews',            # æ›¿æ¢ TechNews2024
    'GlobalMacro',         # æ›¿æ¢ GlobalMacroReport
    'CommoditySignals',    # æ›¿æ¢ CommodityTradeInfo
    'tfainvestments',      # æ›¿æ¢ FinancialAnalystView
    'CryptoMarketUpdates',
    # æ–°å¢éªŒè¯æ´»è·ƒçš„ä¸­æ–‡ç¾è‚¡é¢‘é“ (2025 å¹´æ´»è·ƒ)
    'BloombergZh',         # å½­åšä¸­æ–‡ç¾è‚¡æ–°é—»
    'meigucaijing',        # ç¾è‚¡è´¢ç»
    'usstocknews',         # ç¾è‚¡æ–°é—»
    'xueqiushare',         # é›ªçƒç¾è‚¡
    'sinafinance',         # æ–°æµªç¾è‚¡
    'caijingmeigu'         # è´¢ç»ç¾è‚¡
]
# =========================================================
# =========================================================

# è®¾ç½®ä¸Šæµ·æ—¶åŒº
SH_TZ = pytz.timezone('Asia/Shanghai')
now_shanghai = datetime.now(SH_TZ)

# --- è·¯å¾„å’Œæ–‡ä»¶åç”Ÿæˆé€»è¾‘ ---
DATE_DIR = now_shanghai.strftime("%Y-%m/%d")
BASE_DIR = os.path.join(os.getcwd(), DATE_DIR)
MEDIA_DIR = os.path.join(BASE_DIR, 'media')
FILENAME_BASE = now_shanghai.strftime("%H-%M-%S_telegram_web_content.md")
FULL_FILENAME_PATH = os.path.join(BASE_DIR, FILENAME_BASE)

# --- å¸‚åœºå½±å“åˆ†æé…ç½® ---
IMPACT_KEYWORDS = {
    'positive': ['æ¶¨', 'ä¸Šæ¶¨', 'å¤§æ¶¨', 'é£™æ¶¨', 'æš´æ¶¨', 'çªç ´', 'åˆ©å¥½', 'æ–°é«˜', 'çœ‹å¥½', 'å¢æŒ', 'èµ°å¼º', 'å¤è‹', 'ç«™ä¸Š', 'æ‰©å¤§', 'åˆ©å¤š', 'é¢†å…ˆ'],
    'negative': ['è·Œ', 'ä¸‹è·Œ', 'å¤§è·Œ', 'æš´è·Œ', 'èµ°ä½', 'åˆ©ç©º', 'ä¸‹è¡Œ', 'é£é™©', 'æ‹…å¿§', 'ç–²è½¯', 'æ”¶çª„', 'èµ°å¼±', 'ç¼©å‡', 'äºæŸ', 'åšç©º'],
    'neutral_positive': ['å›å‡', 'åå¼¹', 'æ¸©å’Œ', 'ä¼ç¨³', 'æ”¾é‡', 'å›è´­'],
    'neutral_negative': ['å‹åŠ›', 'æ”¾ç¼“', 'éœ‡è¡', 'å›è°ƒ', 'ç›˜æ•´', 'é«˜ä½'],
}

SECTOR_KEYWORDS = {
    'é»„é‡‘/è´µé‡‘å±': ['é»„é‡‘', 'æ²ªé‡‘', 'ç™½é“¶', 'é’¯é‡‘', 'é‡‘ä»·', 'è´µé‡‘å±', 'XAUUSD'],
    'Aè‚¡/å¤§ç›˜': ['Aè‚¡', 'æ²ªæŒ‡', 'æ·±æˆæŒ‡', 'åˆ›ä¸šæ¿', 'æ²ªæ·±', 'å¸‚åœº', 'äº¬ä¸‰å¸‚', 'åŒ—å‘èµ„é‡‘', 'Nifty'],
    'æœŸè´§/å¤§å®—å•†å“': ['æœŸè´§', 'æ£•æ¦ˆæ²¹', 'ç”ŸçŒª', 'é¸¡è›‹', 'LPG', 'é›†è¿', 'æ¶²åŒ–å¤©ç„¶æ°”', 'ç¢³é…¸é”‚', 'é“œä»·', 'åŸæ²¹', 'å¤§å®—å•†å“', 'å·¥ä¸šå“'],
    'ç§‘æŠ€/åŠå¯¼ä½“': ['èŠ¯ç‰‡', 'ç§‘åˆ›50', 'ä¸­èŠ¯å›½é™…', 'åè™¹å…¬å¸', 'å…ˆè¿›å°è£…', 'å†…å­˜', 'SSD', 'AI', 'å¤§æ¨¡å‹', 'ç®—åŠ›', 'åŠå¯¼ä½“'],
    'æ–°èƒ½æº/å‚¨èƒ½': ['ç¢³é…¸é”‚', 'å‚¨èƒ½', 'å…‰ä¼', 'ç”µæ± çº§', 'HVDC', 'æ–°èƒ½æºæ±½è½¦', 'é£ç”µ'],
    'å®è§‚/å¤®è¡Œ': ['ç¾è”å‚¨', 'å¤®è¡Œ', 'é™æ¯', 'åŠ æ¯', 'é€†å›è´­', 'SHIBOR', 'æ”¿åºœé¢„ç®—', 'ç¾å›½å›½å€º', 'å…³ç¨', 'é€šèƒ€', 'GDP', 'PMI'],
    'æ¸¯è‚¡/æ±‡ç‡': ['æ’ç”ŸæŒ‡æ•°', 'æ’æŒ‡', 'æ³°é“¢', 'ç¾å…ƒ', 'å¢æ¯”', 'æ–°åŠ å¡å…ƒ', 'æ±‡ç‡', 'æ¸¯è‚¡', 'ç¦»å²¸äººæ°‘å¸'],
    'ç¨€åœŸ': ['ç¨€åœŸ', 'å‡ºå£ç®¡åˆ¶'],
    'æ•°å­—è´§å¸': ['æ¯”ç‰¹å¸', 'ä»¥å¤ªåŠ', 'BTC', 'ETH', 'åŠ å¯†è´§å¸', 'åŒºå—é“¾', 'Solana'],
    'æŒ‡æ•°/é“¶è¡Œ': ['Bank Nifty', 'Nifty', 'æŒ‡æ•°', 'é“¶è¡Œè‚¡'],
    'å…¨çƒ/å¤–æ±‡': ['å¤–æ±‡', 'USD', 'EUR', 'GBP', 'å…¨çƒå¸‚åœº'],
    'åŸºé‡‘/ETF': ['ETF', 'åŸºé‡‘', 'é»„é‡‘ETF', 'æœ‰è‰²ETF', 'æŒ‡æ•°åŸºé‡‘', 'é¿é™©åŸºé‡‘'],
    'æœŸæƒ/äº¤æ˜“ä¿¡å·': ['æœŸæƒ', 'ä¿¡å·', 'åšå¤š', 'åšç©º', 'çœ‹æ¶¨', 'çœ‹è·Œ', 'åˆçº¦'],
    'ç¾è‚¡': ['ç¾è‚¡', 'NASDAQ', 'S&P', 'Dow', 'US30', 'AMD', 'NVDA', 'AAPL', 'Dow Jones', 'S&P 500']
}

def analyze_market_impact(text, hashtags):
    score = 0
    impact_sectors = set()
    stocks = []
    
    # æå–è‚¡ç¥¨ä»£ç ï¼ˆæ‰©å±•ç¾è‚¡/A è‚¡æ¨¡å¼ï¼‰
    stock_pattern = r'(ä¸­èŠ¯å›½é™…|åè™¹å…¬å¸|æ±Ÿæ³¢é¾™|èŠ¯è”é›†æˆ|ä¸­å¾®å…¬å¸|è¥¿éƒ¨è¶…å¯¼|èŠ¯åŸè‚¡ä»½|æ±‡ä¸°æ§è‚¡|è‹±ä¼Ÿè¾¾|NVDA|AAPL|AMD|TSLA|GOOGL|MSFT)'
    stocks = re.findall(stock_pattern, text)
    
    combined_content = text + " ".join(hashtags)
    for sector, keywords in SECTOR_KEYWORDS.items():
        for keyword in keywords:
            if keyword in combined_content:
                impact_sectors.add(sector)
                break
    
    # è®¡ç®—åˆ†æ•°ï¼ˆä¼˜åŒ–æƒé‡ï¼Œå¦‚ 'æš´æ¶¨' +3ï¼‰
    for word in IMPACT_KEYWORDS['positive']:
        score += combined_content.count(word) * (3 if word in ['æš´æ¶¨', 'é£™æ¶¨'] else 2)
    for word in IMPACT_KEYWORDS['neutral_positive']:
        score += combined_content.count(word) * 1
    for word in IMPACT_KEYWORDS['negative']:
        score -= combined_content.count(word) * (3 if word in ['æš´è·Œ', 'å¤§è·Œ'] else 2)
    for word in IMPACT_KEYWORDS['neutral_negative']:
        score -= combined_content.count(word) * 1

    # ç¡®å®šæ ‡ç­¾
    if score >= 4:
        impact_label = "æ˜¾è‘—åˆ©å¥½ (Bullish)"
        impact_color = "ğŸŸ¢"
    elif score >= 1:
        impact_label = "æ½œåœ¨åˆ©å¥½ (Positive)"
        impact_color = "ğŸŸ¡"
    elif score <= -4:
        impact_label = "æ˜¾è‘—åˆ©ç©º (Bearish)"
        impact_color = "ğŸ”´"
    elif score <= -1:
        impact_label = "æ½œåœ¨åˆ©ç©º (Negative)"
        impact_color = "ğŸŸ "
    else:
        impact_label = "ä¸­æ€§/éœ€å…³æ³¨ (Neutral)"
        impact_color = "âšª"
        
    sector_str = "ã€".join(impact_sectors) if impact_sectors else "æœªè¯†åˆ«è¡Œä¸š"
    summary = f"**å¸‚åœºå½±å“** {impact_color} **{impact_label}** - å…³æ³¨æ¿å—ï¼š{sector_str}"
    if stocks:
        summary += f"\n**æåŠè‚¡ç¥¨**ï¼š{', '.join(set(stocks))}"
        
    return summary

# --- å®ç”¨å·¥å…·å‡½æ•° ---

def setup_directories():
    os.makedirs(BASE_DIR, exist_ok=True)
    if os.path.exists(MEDIA_DIR):
        shutil.rmtree(MEDIA_DIR)
    os.makedirs(MEDIA_DIR, exist_ok=True)
    print(f"æ•°æ®å°†ä¿å­˜åˆ°ç›®å½•: {BASE_DIR}")

def get_channel_content(username):
    """ä» Telegram Web é¢„è§ˆé¡µé¢æŠ“å–å†…å®¹"""
    url = f"https://t.me/s/{username}"
    all_messages = []
    downloaded_count = 0
    
    print(f"å¼€å§‹æŠ“å– Web é¢„è§ˆé¡µé¢: {url}...")
    
    try:
        session = requests.Session()
        retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
        session.mount('https://', HTTPAdapter(max_retries=retries))
        response = session.get(url, timeout=10)
        response.raise_for_status() 
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ä¼˜åŒ–æ–‡æœ¬æå–ï¼šä» soup ç›´æ¥è·å–å®Œæ•´æ¶ˆæ¯
        messages = soup.find_all('div', class_='tgme_widget_message', limit=5)  # ä¼˜åŒ– limit=5 å‡å°‘è´Ÿè½½
        
        if not messages:
            print(f"é¢‘é“ @{username} æ— æ¶ˆæ¯ï¼Œè·³è¿‡åˆ†æã€‚")
            return f"## é¢‘é“: @{username}ï¼ˆå…± 0 æ¡æ¶ˆæ¯ï¼‰\n\n**è­¦å‘Š:** æœªæ‰¾åˆ°ä»»ä½•æ¶ˆæ¯ï¼Œè¯¥é¢‘é“å¯èƒ½ä¸å­˜åœ¨æˆ–å¯ç”¨äº†å†…å®¹é™åˆ¶ã€‚\n"

        for message in messages:
            msg_text = ""
            message_id = 'N/A'
            clean_text = ""
            hashtags = []
            media_tag = None
            
            # 1. è·å–æ¶ˆæ¯IDå’Œæ—¶é—´æˆ³
            link_tag = message.find('a', class_='tgme_widget_message_date')
            if link_tag and 'href' in link_tag.attrs:
                parts = link_tag['href'].split('/')
                message_id = parts[-1] if parts[-1].isdigit() else 'N/A'
                
                time_tag = link_tag.find('time')
                if time_tag and 'datetime' in time_tag.attrs:
                    time_utc = datetime.fromisoformat(time_tag['datetime'].replace('Z', '+00:00'))
                    time_sh = time_utc.astimezone(SH_TZ).strftime('%Y-%m-%d %H:%M:%S')
                    
                    msg_text += f"---\n**æ—¶é—´ (ä¸Šæµ·):** {time_sh} **(ID:** `{message_id}` **)**\n"
            
            # 2. æå–å¹¶æ¸…ç†æ¶ˆæ¯æ–‡æœ¬å†…å®¹ï¼ˆä¼˜åŒ–é¿å…æˆªæ–­ï¼‰
            text_tag = message.find('div', class_='tgme_widget_message_text')
            if text_tag:
                # ä½¿ç”¨ str(text_tag).replace å®Œæ•´æå–
                text_content = str(text_tag).replace('<br/>', '\n').replace('<br>', '\n')
                clean_text = BeautifulSoup(text_content, 'html.parser').get_text(separator='\n', strip=True)
                
            # 3. æå–å¹¶æ¸…ç† Hashtag
            hashtags = re.findall(r'#\w+', clean_text)
            if hashtags:
                msg_text += "\n**æ ‡ç­¾**: " + ", ".join(hashtags) + "\n"
                clean_text = re.sub(r'#\w+', '', clean_text).strip()
            
            # 4. åª’ä½“ä¸‹è½½å’Œæ ‡è®°
            media_tag = message.find('a', class_='tgme_widget_message_photo_wrap') or \
                        message.find('a', class_='tgme_widget_message_document_wrap')
            
            if media_tag and 'style' in media_tag.attrs:
                url_match = re.search(r'url\(["\']?(.*?)["\']?\)', media_tag['style'])
                
                if url_match and message_id != 'N/A':
                    media_url = url_match.group(1)
                    media_extension = os.path.splitext(media_url.split('?')[0])[1] or '.jpg'
                    media_filename_relative = os.path.join('media', f"{username}_{message_id}{media_extension}")
                    media_filename_full = os.path.join(BASE_DIR, media_filename_relative)

                    try:
                        media_response = session.get(media_url, timeout=10)
                        if media_response.status_code == 200:
                            with open(media_filename_full, 'wb') as f:
                                f.write(media_response.content)
                            md_path = os.path.join(DATE_DIR, media_filename_relative).replace(os.path.sep, '/')
                            msg_text += f"\n![åª’ä½“æ–‡ä»¶]({md_path})\n"
                            downloaded_count += 1
                        else:
                            msg_text += f"\n*[åª’ä½“æ–‡ä»¶ä¸‹è½½å¤±è´¥: HTTP {media_response.status_code}]*\n"
                    except requests.exceptions.RequestException as download_err:
                        msg_text += f"\n*[åª’ä½“æ–‡ä»¶ä¸‹è½½å¤±è´¥: {download_err}]*\n"
                elif media_tag:
                    msg_text += f"\n*[åŒ…å«åª’ä½“/æ–‡ä»¶ï¼Œè¯·æŸ¥çœ‹åŸå§‹é“¾æ¥]({url})*\n"

            # 5. å¸‚åœºå½±å“åˆ†æ
            impact_summary = analyze_market_impact(clean_text, hashtags)
            
            # 6. è·³è¿‡ç©ºæ¶ˆæ¯
            if not clean_text and not media_tag:
                continue

            # 7. æ·»åŠ æ¸…ç†åçš„æ–‡æœ¬å’Œåˆ†æç»“æœ
            if clean_text:
                msg_text += f"\n{clean_text}\n"
            msg_text += f"\n{impact_summary}\n"
            
            # 8. åŸå§‹æ¶ˆæ¯é“¾æ¥
            if message_id != 'N/A':
                msg_text += f"\n**[åŸå§‹é“¾æ¥](https://t.me/{username}/{message_id})**\n"
            
            all_messages.append(msg_text)
        
        print(f"é¢‘é“ @{username} æŠ“å–å®Œæˆï¼Œå…± {len(all_messages)} æ¡æ¶ˆæ¯ï¼Œä¸‹è½½åª’ä½“: {downloaded_count} ä¸ªã€‚")

    except requests.HTTPError as e:
        error_msg = f"HTTP é”™è¯¯ (å¯èƒ½æ˜¯ 404 æˆ– 403): {e}. URL: {url}"
        print(error_msg)
        return f"## é¢‘é“: @{username}ï¼ˆå…± 0 æ¡æ¶ˆæ¯ï¼‰\n\n**æŠ“å–å¤±è´¥ (HTTP é”™è¯¯):** {e}\n"
    except Exception as e:
        error_msg = f"æŠ“å– @{username} å¤±è´¥: {e}"
        print(error_msg)
        return f"## é¢‘é“: @{username}ï¼ˆå…± 0 æ¡æ¶ˆæ¯ï¼‰\n\n**æŠ“å–å¤±è´¥ (æœªçŸ¥é”™è¯¯):** {e}\n"

    header = f"## é¢‘é“: @{username}ï¼ˆå…± {len(all_messages)} æ¡æ¶ˆæ¯ï¼‰\n\n"
    return header + "\n".join(all_messages)

def generate_overall_summary(all_content):
    """ç”Ÿæˆæ•´ä½“å½±å“æ€»ç»“ JSON"""
    impacts = re.findall(r'\*\*å¸‚åœºå½±å“\*\* (ğŸŸ¢|ğŸŸ¡|ğŸŸ |ğŸ”´|âšª) \*\*(.+?)\*\*', all_content, re.DOTALL)
    sector_mentions = re.findall(r'å…³æ³¨æ¿å—ï¼š(.+?)(?=\n|$)', all_content)
    stocks = re.findall(r'\*\*æåŠè‚¡ç¥¨\*\*ï¼š(.+?)(?=\n|$)', all_content)
    
    impact_counter = Counter([label for _, label in impacts])
    emoji_to_label = {'ğŸŸ¢': 'Bullish', 'ğŸŸ¡': 'Positive', 'ğŸŸ ': 'Negative', 'ğŸ”´': 'Bearish', 'âšª': 'Neutral'}
    
    summary = {
        'timestamp': now_shanghai.strftime('%Y-%m-%d %H:%M:%S'),
        'total_messages': len(re.findall(r'---\n\*\*æ—¶é—´', all_content)),
        'impact_distribution': {emoji_to_label[emoji]: count for emoji, count in impact_counter.items() if emoji in emoji_to_label},
        'top_sectors': Counter(sector_mentions).most_common(5),
        'top_stocks': Counter([stock for stocks_list in stocks for stock in stocks_list.split(', ')]).most_common(5),
        'recommendation': 'æ•´ä½“åˆ©å¥½ç¾è‚¡/ç§‘æŠ€/é»„é‡‘æ¿å—ï¼Œè­¦æƒ•æ¸¯è‚¡/ç¨€åœŸé£é™©' if impact_counter['ğŸŸ¢'] > impact_counter['ğŸ”´'] else 'ä¸­æ€§å¸‚åœºï¼Œè§‚å¯Ÿå®è§‚/ç¾è‚¡ä¿¡å·'
    }
    
    json_path = FULL_FILENAME_PATH.replace('.md', '_overall_summary.json')
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æ•´ä½“æ€»ç»“å·²ä¿å­˜åˆ° **{json_path}**")

def main():
    """ä¸»å‡½æ•°"""
    setup_directories()
    all_content = f"# Telegram é¢‘é“å†…å®¹æŠ“å– (Web é¢„è§ˆ)\n\n**æŠ“å–æ—¶é—´ (ä¸Šæµ·):** {now_shanghai.strftime('%Y-%m-%d %H:%M:%S')}\n\n---\n"
    
    for username in CHANNEL_USERNAMES:
        channel_content = get_channel_content(username)
        all_content += channel_content

    with open(FULL_FILENAME_PATH, 'w', encoding='utf-8') as f:
        f.write(all_content)
        
    print(f"\nâœ… æ‰€æœ‰å†…å®¹å·²æˆåŠŸä¿å­˜åˆ° **{FULL_FILENAME_PATH}** æ–‡ä»¶ä¸­ã€‚")
    
    generate_overall_summary(all_content)

if __name__ == '__main__':
    main()
