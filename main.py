import os
import time
import random
import datetime

# å¾®åšæ•°æ®æŠ“å–åº“ (weibo-scraper)
# æ³¨æ„ï¼šè¯¥åº“çš„å…³é”®è¯æœç´¢APIæ–‡æ¡£ä¸æ˜ç¡®ï¼Œæ­¤å¤„çš„å¯¼å…¥ä¸ºå ä½ã€‚
# æ‚¨éœ€è¦åœ¨ search_weibo_keyword å‡½æ•°ä¸­é›†æˆçœŸå®çš„æœç´¢é€»è¾‘ã€‚
try:
    # å°è¯•å¯¼å…¥ weibo_scraper åº“ï¼Œç”¨äºæç¤ºä¾èµ–å·²å®‰è£…
    # from weibo_scraper import search_posts_by_keyword as actual_search_api
    pass 
except ImportError:
    print("Warning: The 'weibo-scraper' library is not installed or the import path is incorrect.")


def classify_tweet(content: str) -> str:
    """
    æ ¹æ®å¾®åšå†…å®¹è‡ªåŠ¨å°†å…¶åˆ†ç±»åˆ°ä¸‰ä¸ªä¸»é¢˜ä¹‹ä¸€ã€‚
    
    Args:
        content: å¾®åšçš„æ–‡æœ¬å†…å®¹ã€‚
        
    Returns:
        åˆ†ç±»çš„æ ‡é¢˜å­—ç¬¦ä¸²ã€‚
    """
    # å…³é”®è¯ç»Ÿä¸€è½¬ä¸ºå°å†™ï¼Œä»¥è¿›è¡Œä¸åŒºåˆ†å¤§å°å†™çš„åŒ¹é…
    content_lower = content.lower()

    # 1. å®ç›˜ã€ä¹°å–ä¸äº¤æ˜“è®°å½•
    # é‡ç‚¹å…³æ³¨æ“ä½œæ€§ã€æ•°å­—ã€è´¦æˆ·ç›¸å…³çš„è¯æ±‡
    trading_keywords = ["å®ç›˜", "ä¹°å–", "è®°å½•", "æŒä»“", "è°ƒä»“", "äº¤æ˜“", "æ¸…ä»“", "åŠ ä»“", "åšt", "æ”¶ç›Š", "è´¦æˆ·"]
    if any(k in content_lower for k in trading_keywords):
        return "ğŸ“Š ä¸ªäººå®ç›˜ã€ä¹°å–ä¸äº¤æ˜“è®°å½•"

    # 2. ä¸“ä¸šåˆ†æã€æ”¿ç­–ä¸å®è§‚å½±å“
    # é‡ç‚¹å…³æ³¨åˆ†ææ€§ã€å®˜æ–¹ã€ç»æµå­¦æœ¯è¯­
    analysis_keywords = ["æ”¿ç­–", "æ³•è§„", "å®è§‚", "å¾®è§‚", "å›½é™…", "å›½å†…", "å½±å“", "åˆ†æ", "å¤®è¡Œ", "æŠ¥å‘Š", "ç»æµ"]
    if any(k in content_lower for k in analysis_keywords):
        return "ğŸ“° ä¸“ä¸šåˆ†æã€æ”¿ç­–ä¸å®è§‚å½±å“"

    # 3. ç»éªŒã€å¿ƒå¾—ä¸ä½“ä¼šåˆ†äº«
    # é‡ç‚¹å…³æ³¨ä¸ªäººæ„Ÿå—ã€å»ºè®®ã€å“²å­¦ç±»çš„è¯æ±‡
    experience_keywords = ["å¿ƒå¾—", "ç»éªŒ", "ä½“ä¼š", "åˆ†äº«", "å»ºè®®", "æ€è€ƒ", "æ„Ÿå—", "ç†å¿µ", "æŠ•èµ„å“²å­¦"]
    if any(k in content_lower for k in experience_keywords):
        return "ğŸ§  ç»éªŒã€å¿ƒå¾—ä¸ä½“ä¼šåˆ†äº«"

    # é»˜è®¤åˆ†ç±»
    return "ğŸ¤” å…¶ä»–è®¨è®º/æœªåˆ†ç±»"


def mock_weibo_search(keyword: str, count: int) -> list:
    """
    æ¨¡æ‹Ÿå¾®åšæœç´¢ç»“æœçš„å‡½æ•°ã€‚
    """
    mock_data = []
    
    # æ„é€ ä¸€äº›å¸¦ç‰¹å®šå…³é”®è¯çš„æ¨¡æ‹Ÿå†…å®¹ï¼Œç”¨äºæµ‹è¯• classify_tweet
    content_templates = [
        # äº¤æ˜“ç±» (Trading)
        f"ä»Šæ—¥å¯¹ {keyword} è¿›è¡Œäº†ä¸€æ¬¡åšTæ“ä½œï¼ŒæˆåŠŸé™ä½æˆæœ¬0.5%ã€‚è¿™å°±æ˜¯æˆ‘çš„å®ç›˜è®°å½•ã€‚",
        f"æœ€æ–°è°ƒä»“è®°å½•ï¼šæ¸…ä»“äº†AåŸºé‡‘ï¼ŒåŠ ä»“äº†BåŸºé‡‘ã€‚åˆ†äº«æˆ‘çš„ä¹°å–ç»éªŒã€‚",
        f"åˆ†æäº†å®è§‚ç»æµï¼Œæˆ‘è®¤ä¸ºä¸‹å‘¨æ˜¯æœ€ä½³åŠ ä»“æ—¶æœºï¼Œè¿™åªæ˜¯ä¸ªäººè´¦æˆ·çš„äº¤æ˜“å¿ƒå¾—ã€‚",
        # åˆ†æç±» (Analysis)
        f"å›½å®¶æ”¿ç­–å¯¹ {keyword} å¸‚åœºçš„å½±å“å°†åœ¨Q4æ˜¾ç°ï¼Œè¿™æ˜¯ä¸€ä»½ä¸“ä¸šçš„æ·±åº¦åˆ†æã€‚",
        f"å›½é™…ç¯å¢ƒå¤æ‚ï¼Œå¾®è§‚æ•°æ®è¡¨æ˜ {keyword} ä»å°†æ‰¿å‹ï¼Œè¯·è°¨æ…æ“ä½œã€‚",
        f"è¯¦ç»†è§£è¯»æœ€æ–°æ³•è§„å¯¹ä¸ªäººæŠ•èµ„è€… {keyword} è´¦æˆ·çš„ä¿æŠ¤ã€‚",
        # ç»éªŒç±» (Experience)
        f"æˆ‘çš„ {keyword} æŠ•èµ„å¿ƒå¾—ï¼šé•¿æœŸä¸»ä¹‰æ‰èƒ½å¸¦æ¥çœŸæ­£çš„å¤åˆ©ä½“ä¼šå’Œç»éªŒã€‚",
        f"åˆ†äº«ä¸€ä¸ªæˆ‘çŠ¯è¿‡çš„ {keyword} æŠ•èµ„é”™è¯¯ï¼Œå¸Œæœ›å¤§å®¶å¼•ä»¥ä¸ºæˆ’ï¼Œå°‘èµ°å¼¯è·¯ã€‚",
    ]
    
    # ç¡®ä¿æ¯ä¸ªåˆ†ç±»éƒ½æœ‰æ•°æ®
    for i in range(count):
        tweet_id = str(10000 + i)
        
        # éšæœºé€‰æ‹©ä¸€ä¸ªæ¨¡æ¿ï¼Œå¹¶ç¡®ä¿åˆ†ç±»å…³é”®è¯è¢«åŒ…å«
        template = random.choice(content_templates)
        
        # æ¨¡æ‹Ÿç”Ÿæˆæ•°æ®
        mock_data.append({
            "content": template,
            "user": f"ç”¨æˆ·{i+1}",
            "time": (datetime.datetime.now() - datetime.timedelta(hours=random.randint(1, 48))).strftime("%Y-%m-%d %H:%M:%S"),
            "likes": random.randint(10, 500),
            "comments": random.randint(1, 100),
            "reposts": random.randint(0, 50),
            "search_keyword": keyword, # è®°å½•æ˜¯å“ªä¸ªå…³é”®è¯æœç´¢åˆ°çš„
        })
        
    return mock_data


def search_weibo_keyword(keyword: str) -> list:
    """
    ã€!!! å…³é”®å‡½æ•°ï¼šæ­¤å¤„éœ€è¦æ¥å…¥çœŸå®çš„ weibo-scraper æœç´¢é€»è¾‘ !!!ã€‘
    
    Args:
        keyword: è¦æœç´¢çš„å…³é”®è¯ã€‚
        
    Returns:
        åŒ…å«å¾®åšå†…å®¹çš„å­—å…¸åˆ—è¡¨ã€‚
    """
    print(f"-> æ­£åœ¨å°è¯•æœç´¢å…³é”®è¯: {keyword}")
    
    # =========================================================================
    # WARNING: å ä½ç¬¦ä»£ç 
    # æ‚¨å¿…é¡»ä½¿ç”¨ weibo-scraper åº“çš„ API æ›¿æ¢ä¸‹é¢çš„ mock_weibo_search(keyword, 30)
    #
    # ç¤ºä¾‹ï¼ˆå‡è®¾å­˜åœ¨ä¸€ä¸ª search_posts å‡½æ•°ï¼‰ï¼š
    # try:
    #     tweets = actual_search_api(keyword, pages=5, login_info=...)
    #     # æ‚¨å¯èƒ½è¿˜éœ€è¦åœ¨è¿™é‡Œç¼–å†™é€»è¾‘ï¼Œå°† weibo-scraper è¿”å›çš„ TweetMeta å¯¹è±¡
    #     # è½¬æ¢ä¸ºæœ¬è„šæœ¬éœ€è¦çš„å­—å…¸æ ¼å¼ (content, user, time, likes, comments, reposts)
    #     
    #     # return [your_parsed_data_dictionary for tweet in tweets]
    # except Exception as e:
    #     print(f"ERROR: å¾®åšæœç´¢å¤±è´¥ ({keyword}): {e}")
    #     return []
    #
    # é‡è¦çš„å­—æ®µæ˜ å°„:
    # - content: mblog.text
    # - user: user.screen_name
    # - likes: mblog.attitudes_count
    # - comments: mblog.comments_count
    # - reposts: mblog.reposts_count
    # - time: mblog.created_at
    # =========================================================================

    # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡ŒæŠ¥å‘Šç”Ÿæˆæµ‹è¯•
    return mock_weibo_search(keyword, 30)


def generate_report(all_tweets: list, keywords_used: list):
    """
    æ ¹æ®æŠ“å–åˆ°çš„æ‰€æœ‰å¾®åšå†…å®¹ï¼Œè¿›è¡Œåˆ†ç±»æ•´ç†å¹¶ç”Ÿæˆ Markdown æŠ¥å‘Šã€‚
    """
    if not all_tweets:
        print("æ— æŠ“å–ç»“æœï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆã€‚")
        return

    # 1. åˆå§‹åŒ–åˆ†ç±»å­—å…¸
    classified_tweets = {
        "ğŸ“Š ä¸ªäººå®ç›˜ã€ä¹°å–ä¸äº¤æ˜“è®°å½•": [],
        "ğŸ“° ä¸“ä¸šåˆ†æã€æ”¿ç­–ä¸å®è§‚å½±å“": [],
        "ğŸ§  ç»éªŒã€å¿ƒå¾—ä¸ä½“ä¼šåˆ†äº«": [],
        "ğŸ¤” å…¶ä»–è®¨è®º/æœªåˆ†ç±»": [],
    }

    # 2. è‡ªåŠ¨åˆ†ç±»
    for tweet in all_tweets:
        # ä½¿ç”¨ classify_tweet å‡½æ•°è¿›è¡Œè‡ªåŠ¨åˆ†ç±»
        category = classify_tweet(tweet['content'])
        
        # ç¡®ä¿åˆ†ç±»é”®å­˜åœ¨ï¼Œç„¶åæ·»åŠ 
        if category in classified_tweets:
            classified_tweets[category].append(tweet)
        else:
             # å¦‚æœåˆ†ç±»ç»“æœä¸åœ¨é¢„è®¾çš„å››ä¸ªç±»åˆ«ä¸­ï¼Œåˆ™æ”¾å…¥â€œå…¶ä»–â€
             classified_tweets["ğŸ¤” å…¶ä»–è®¨è®º/æœªåˆ†ç±»"].append(tweet)


    # 3. æŠ¥å‘Šå†…å®¹æ„å»º
    report_content = []
    
    # æŠ¥å‘Šå¤´éƒ¨
    report_content.append(f"# ğŸ“ˆ å¾®åšåŸºé‡‘è¯é¢˜è‡ªåŠ¨åˆ†ç±»æŠ¥å‘Š")
    report_content.append(f"\n**ç”Ÿæˆæ—¶é—´:** {datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
    report_content.append(f"**æœç´¢å…³é”®è¯:** {', '.join(keywords_used)}\n")
    report_content.append(f"**æŠ“å–æ€»æ¡æ•°:** {len(all_tweets)} æ¡\n")
    report_content.append("---\n")


    # 4. éå†åˆ†ç±»å¹¶ç”Ÿæˆå†…å®¹
    for category, tweets in classified_tweets.items():
        # æŒ‰ç‚¹èµæ•°é™åºæ’åºï¼Œçªæ˜¾çƒ­é—¨å†…å®¹
        tweets.sort(key=lambda x: x['likes'], reverse=True)
        
        report_content.append(f"## {category} ({len(tweets)} æ¡)")
        
        if not tweets:
            report_content.append("æš‚æ— ç›¸å…³çƒ­é—¨å†…å®¹ã€‚\n")
            continue
            
        # è¡¨æ ¼å¤´éƒ¨
        report_content.append("| çƒ­é—¨åº¦ (ç‚¹èµ) | ç”¨æˆ· | å¾®åšå†…å®¹ (å‰100å­—) | æ¥æºå…³é”®è¯ |")
        report_content.append("| :---: | :--- | :--- | :---: |")

        # éå†è¯¥åˆ†ç±»ä¸‹çš„çƒ­é—¨å¾®åšï¼ˆä»…å±•ç¤ºå‰10æ¡æœ€çƒ­é—¨çš„ï¼‰
        for tweet in tweets[:10]:
            content_preview = tweet['content'][:100].replace('\n', ' ').replace('|', '-') + ('...' if len(tweet['content']) > 100 else '')
            
            # ä½¿ç”¨ Markdown æ ¼å¼åŒ–è¡Œ
            row = (
                f"| {tweet['likes']} (ğŸ’¬{tweet['comments']}) "
                f"| @{tweet['user']} "
                f"| {content_preview} "
                f"| {tweet.get('search_keyword', 'N/A')} |"
            )
            report_content.append(row)
            
        report_content.append("\n---\n")

    # 5. ä¿å­˜æŠ¥å‘Š
    report_dir = 'reports'
    report_path = os.path.join(report_dir, 'report.md')
    
    # ç¡®ä¿ reports ç›®å½•å­˜åœ¨
    os.makedirs(report_dir, exist_ok=True)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_content))
        
    print(f"âœ… æŠ¥å‘Šå·²æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜åˆ° {report_path}")


def main():
    """ä¸»æ‰§è¡Œå‡½æ•°ï¼Œè´Ÿè´£è·å–å…³é”®è¯ã€æ‰§è¡Œæœç´¢å’ŒæŠ¥å‘Šç”Ÿæˆã€‚"""
    
    # ä»ç¯å¢ƒå˜é‡ KEYWORDS_ENV è·å–å…³é”®è¯åˆ—è¡¨
    keywords_env = os.environ.get('KEYWORDS_ENV')
    
    if keywords_env:
        # ç¯å¢ƒå˜é‡å­˜åœ¨ï¼ŒæŒ‰é€—å·åˆ†éš”å¤„ç†
        keywords_list = [k.strip() for k in keywords_env.split(',') if k.strip()]
        print(f"âš™ï¸ ä»ç¯å¢ƒå˜é‡ KEYWORDS_ENV è·å–å…³é”®è¯: {keywords_list}")
    else:
        # ç¯å¢ƒå˜é‡ä¸å­˜åœ¨ï¼Œä½¿ç”¨å†…éƒ¨å›é€€å…³é”®è¯
        keywords_list = ['åŸºé‡‘']
        print(f"âš ï¸ ç¯å¢ƒå˜é‡ KEYWORDS_ENV æœªè®¾ç½®ã€‚ä½¿ç”¨å†…éƒ¨å›é€€å…³é”®è¯: {keywords_list}")

    if not keywords_list:
        print("âŒ å…³é”®è¯åˆ—è¡¨ä¸ºç©ºï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    all_tweets = []
    
    # éå†æ‰€æœ‰å…³é”®è¯è¿›è¡Œæœç´¢
    for keyword in keywords_list:
        # åœ¨è¿™é‡Œæ‰§è¡ŒçœŸæ­£çš„å¾®åšæœç´¢ï¼ˆç›®å‰æ˜¯æ¨¡æ‹Ÿæ•°æ®ï¼‰
        search_results = search_weibo_keyword(keyword)
        all_tweets.extend(search_results)
        time.sleep(1) # æ¨¡æ‹Ÿç½‘ç»œè¯·æ±‚å»¶è¿Ÿï¼Œé¿å…è¢«å°ç¦

    # ç”ŸæˆæŠ¥å‘Š
    generate_report(all_tweets, keywords_list)

if __name__ == "__main__":
    main()
