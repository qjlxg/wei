import os
import datetime
import time
from pathlib import Path
from urllib.parse import quote
import re
# ä»¥ä¸‹å¯¼å…¥å·²è¢«ç§»é™¤ï¼šfrom selenium import webdriver, from selenium.webdriver.chrome.options import Options, from bs4 import BeautifulSoup, from selenium.webdriver.support.ui import WebDriverWait, from selenium.webdriver.support import expected_conditions as EC, from selenium.webdriver.common.by import By, import random
import json
import jieba
from snownlp import SnowNLP
import requests # æ–°å¢ requests åº“
import random # ç¡®ä¿ random åº“è¢«å¯¼å…¥

# MCP Server é…ç½®
MCP_SERVER_URL = "http://localhost:3000/api/tools/search_content"
DEFAULT_LIMIT = 50  # MCPæœåŠ¡å™¨é»˜è®¤è¿”å›çš„æ¡æ•°ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´

# --- è¾…åŠ©å‡½æ•°ï¼šæå–çƒ­é—¨åŸºé‡‘ç±»å‹ (åŠŸèƒ½ä¿ç•™) ---
def get_top_fund_types(tweets, n=2):
    """
    Counts fund types in a list of tweets and returns the top N most frequent ones.
    Excludes 'é€šç”¨åŸºé‡‘' from being the specific top type.
    """
    if not tweets:
        return []
        
    fund_counts = {}
    for tweet in tweets:
        fund_type = tweet['fund_type']
        fund_counts[fund_type] = fund_counts.get(fund_type, 0) + 1
    
    # ç§»é™¤æˆ–é™ä½â€œé€šç”¨åŸºé‡‘â€çš„ä¼˜å…ˆçº§ï¼Œå› ä¸ºå®ƒç¼ºä¹ç‰¹å¼‚æ€§
    if 'é€šç”¨åŸºé‡‘' in fund_counts:
        del fund_counts['é€šç”¨åŸºé‡‘']
        
    sorted_funds = sorted(fund_counts.items(), key=lambda item: item[1], reverse=True)
    return [f[0] for f in sorted_funds[:n]]


# --- è¾…åŠ©å‡½æ•°ï¼šæ—¶é—´è§£æ (åŠŸèƒ½ä¿ç•™) ---
def parse_absolute_time(time_str):
    """
    Parses relative Weibo time strings (e.g., '2å°æ—¶å‰', 'ä»Šå¤© 10:00') into absolute datetime objects.
    """
    now = datetime.datetime.now()
    time_str = time_str.strip()
    
    # åˆšåˆš/åˆ†é’Ÿå‰
    if 'åˆšåˆš' in time_str:
        return now - datetime.timedelta(seconds=random.randint(10, 59))
        
    elif 'åˆ†é’Ÿå‰' in time_str:
        try:
            minutes = int(re.search(r'(\d+)', time_str).group(1))
            return now - datetime.timedelta(minutes=minutes)
        except:
            return now
            
    # å°æ—¶å‰
    elif 'å°æ—¶å‰' in time_str:
        try:
            hours = int(re.search(r'(\d+)', time_str).group(1))
            return now - datetime.timedelta(hours=hours)
        except:
            return now

    # ä»Šå¤© XX:XX
    elif 'ä»Šå¤©' in time_str:
        try:
            time_part = time_str.split(' ')[-1]
            return datetime.datetime.strptime(f"{now.date()} {time_part}", '%Y-%m-%d %H:%M')
        except:
            return now.replace(minute=0, second=0, microsecond=0)
            
    # MM-DD
    elif re.match(r'\d{2}-\d{2}', time_str): 
        try:
            year = now.year
            # å¦‚æœè§£æåçš„æ—¥æœŸæ¯”å½“å‰æ—¥æœŸæ™šï¼Œåˆ™å¹´ä»½åº”ä¸ºå»å¹´
            parsed_date = datetime.datetime.strptime(f"{year}-{time_str}", '%Y-%m-%d')
            if parsed_date > now:
                parsed_date = datetime.datetime.strptime(f"{year-1}-{time_str}", '%Y-%m-%d')
            return parsed_date.replace(hour=12, minute=0, second=0, microsecond=0)
        except:
            return now.date()

    # YYYY-MM-DD
    try:
        return datetime.datetime.strptime(time_str, '%Y-%m-%d')
    except:
        # Fallback to current time if parsing fails
        return now.date()
        

# --- è¾…åŠ©å‡½æ•°ï¼šåŸºé‡‘ç±»å‹åˆ†ç±» (åŠŸèƒ½ä¿ç•™) ---
def get_fund_type(content):
    """
    Categorizes the tweet based on fund sector keywords.
    """
    content_lower = content.lower()
    patterns = {
        r'ä¸­è¯|cxo|åŒ»ç–—|åŒ»è¯|åˆ›æ–°è¯': 'åŒ»è¯/åŒ»ç–—åŸºé‡‘',
        r'èŠ¯ç‰‡|åŠå¯¼ä½“|é›†æˆç”µè·¯|ç§‘æŠ€|ä¿¡æ¯æŠ€æœ¯': 'åŠå¯¼ä½“/ç§‘æŠ€åŸºé‡‘',
        r'é”‚ç”µ|é£ç”µ|å…‰ä¼|æ–°èƒ½æº|ç”µåŠ¨è½¦|ç¢³ä¸­å’Œ': 'æ–°èƒ½æº/å…‰ä¼åŸºé‡‘',
        r'etf|æŒ‡æ•°|æ²ªæ·±300|ä¸­è¯500|çº³æ–¯è¾¾å…‹|æ ‡æ™®': 'ETF/æŒ‡æ•°åŸºé‡‘',
        r'fof': 'FOF',
        r'è¯åˆ¸|åˆ¸å•†|ä¿é™©|é“¶è¡Œ|é‡‘è': 'è¯åˆ¸/é‡‘èåŸºé‡‘',
        r'ç§å‹Ÿ|ç§å‹ŸåŸºé‡‘|ä¿¡æ‰˜': 'ç§å‹ŸåŸºé‡‘',
        r'æ¶ˆè´¹|ç™½é…’|é£Ÿå“|å®¶ç”µ|å…ç¨': 'æ¶ˆè´¹åŸºé‡‘',
        r'å†›å·¥|å›½é˜²|å¤§å®—å•†å“|èµ„æº|ç…¤ç‚­|æœ‰è‰²': 'å†›å·¥/èµ„æºåŸºé‡‘',
        r'æ¸¯è‚¡|æ’ç”Ÿ|ä¸­æ¦‚è‚¡': 'æ¸¯è‚¡/ä¸­æ¦‚è‚¡åŸºé‡‘'
    }
    for pattern, fund_type in patterns.items():
        if re.search(pattern, content_lower):
            return fund_type
    return 'é€šç”¨åŸºé‡‘'

# --- è¾…åŠ©å‡½æ•°ï¼šæ¨æ–‡å†…å®¹åˆ†ç±» (åŠŸèƒ½ä¿ç•™) ---
def categorize_tweet(content):
    """
    Categorizes the tweet based on the action/topic discussed.
    """
    content_lower = content.lower()
    categories = []
    
    if re.search(r'(åŠ ä»“|ä¹°å…¥|å–å‡º|æŒä»“|è°ƒä»“|äº¤æ˜“|å®ç›˜|åšt|å®šæŠ•|æ¸…ä»“|èµå›)', content_lower):
        categories.append('ä¸ªäººå®ç›˜/ä¹°å–è®°å½•')
    if re.search(r'(å¿ƒå¾—|ç»éªŒ|ä½“ä¼š|æ„Ÿæ‚Ÿ|æ€»ç»“|åˆ†äº«|æ•™è®­)', content_lower):
        categories.append('å¿ƒå¾—/ç»éªŒ/ä½“ä¼š')
    if re.search(r'(åˆ†æ|æ‹†è§£|è§£è¯»|è¯Šæ–­|æ”¶ç›Š|å‡€å€¼|ç­–ç•¥|ç›®æ ‡ä»·|ä¼°å€¼)', content_lower):
        categories.append('åˆ†æ')
    if re.search(r'(æ”¿ç­–|æ³•è§„|ç›‘ç®¡|å¤®è¡Œ|è¯ç›‘ä¼š|æŒæœ‰äººå¤§ä¼š|ç§å‹Ÿ|å…¬å‹Ÿ|é™å‡†|é™æ¯)', content_lower):
        categories.append('å›½å®¶æ”¿ç­–/æ³•è§„')
    if re.search(r'(å®è§‚|å¾®è§‚|å‘¨æœŸ|è¶‹åŠ¿|éœ‡è¡|å›è°ƒ|åº•éƒ¨|é£é™©|è§é¡¶|é«˜ä½)', content_lower):
        categories.append('å®è§‚/å¾®è§‚å½±å“')
    if re.search(r'(å›½å†…|å›½é™…|å…¨çƒ|å¸‚åœº|Aè‚¡|ç¾è‚¡|æ¬§è‚¡)', content_lower):
        categories.append('å›½å†…/å›½é™…å½±å“')
    
    return categories if categories else ['å…¶ä»–']

# --- è¾…åŠ©å‡½æ•°ï¼šå…³é”®è¯æå– (åŠŸèƒ½ä¿ç•™) ---
def extract_keywords(tweets, top_n=5):
    """
    Extracts and counts keywords from all tweets using jieba, returns top N.
    """
    all_text = " ".join(t['content'] for t in tweets)
    stopwords = set([
        'åŸºé‡‘', 'ä»Šå¤©', 'æ˜å¤©', 'æ“ä½œ', 'å¤§å®¶', 'å°±æ˜¯', 'è¿™ä¸ª', 'ä¸€ä¸ª', 'å·²ç»', 'è‡ªå·±', 
        'çš„', 'æ˜¯', 'äº†', 'å•Š', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'å’Œ', 'åœ¨', 'å¯¹', 'æœ‰', 
        'éƒ½', 'å¯ä»¥', 'éœ€è¦', 'å¦‚æœ', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'è¿™äº›', 'é‚£äº›', 'ä¸æ˜¯', 
        'æ²¡æœ‰', 'ä»€ä¹ˆ', 'æ‰€ä»¥', 'å»', 'æ¥', 'è·Ÿ', 'è¢«', 'æŠŠ', 'ä½†', 'ä¹Ÿ', 'è¿˜', 'ä¼š', 'èƒ½', 'è¦',
        'ä¸èƒ½', 'ä¸ä¼š', 'ä¸è¦', 'å¾ˆå¥½', 'éå¸¸', 'æ¯”è¾ƒ', 'å¯èƒ½', 'ä¸€å®š'
    ])
    
    words = jieba.cut(all_text)
    word_counts = {}
    
    for word in words:
        word = word.lower().strip()
        # ä»…ç»Ÿè®¡é•¿åº¦å¤§äº1ä¸”ä¸åœ¨åœç”¨è¯è¡¨ä¸­çš„è¯
        if len(word) > 1 and word not in stopwords:
            word_counts[word] = word_counts.get(word, 0) + 1
            
    sorted_keywords = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)
    return [kw[0] for kw in sorted_keywords[:top_n]]


# --- æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨ requests è°ƒç”¨ MCP Server API ---
def fetch_weibo_data(keyword, pages=2):
    """
    Fetch Weibo posts by calling the local mcp-server-weibo API.
    
    :param keyword: The search term.
    :param pages: Approximate number of pages to fetch (converted to a single limit for MCP).
    :return: List of processed tweets.
    """
    tweets = []
    
    # æˆ‘ä»¬å°† pages å‚æ•°è½¬åŒ–ä¸ºå•ä¸ªè¯·æ±‚çš„ limit å‚æ•°: total_limit = pages * DEFAULT_LIMIT
    total_limit = pages * DEFAULT_LIMIT
    
    # è¯·æ±‚å‚æ•°ä¸ mcp-server-weibo çš„ search_content(keyword, limit, page?) å·¥å…·å‡½æ•°å¯¹åº”
    payload = {
        "keyword": keyword,
        "limit": total_limit,
        "page": 1 # ç®€åŒ–ä¸ºä¸€æ¬¡æ€§è¯·æ±‚è¶³å¤Ÿå¤šçš„æ•°æ®
    }
    
    headers = {
        "Content-Type": "application/json"
    }
    
    print(f"Attempting to fetch data from MCP Server: {MCP_SERVER_URL} with keyword '{keyword}' and limit {total_limit}")
    
    try:
        # å°è¯•è¿æ¥ MCP Serverï¼Œå¢åŠ  timeout ä»¥é˜²æ­¢å¡ä½
        response = requests.post(MCP_SERVER_URL, headers=headers, json=payload, timeout=30)
        response.raise_for_status()  # æ£€æŸ¥ HTTP é”™è¯¯
        
        # è§£æ MCP Server è¿”å›çš„ JSON ç»“æ„
        response_json = response.json()
        
        # å¦‚æœ MCP Server è¿”å›äº†é”™è¯¯ä¿¡æ¯
        if 'error' in response_json or (response_json.get('status') == 'error'):
             error_msg = response_json.get('error', response_json.get('message', 'Unknown error'))
             print(f"MCP Server returned an error: {error_msg}")
             return []
             
        # æå–å¾®åšåˆ—è¡¨
        raw_weibo_list = response_json.get('data') or response_json.get('results') or []

        if not raw_weibo_list:
            print("MCP Server returned an empty list of results.")
            return []

        # å¤„ç†å’Œæ¸…æ´—æ•°æ®
        for item in raw_weibo_list:
            # å‡è®¾ MCP Server è¿”å›çš„ç»“æ„åŒ…å« text/content, user/username, created_at/time, url/link
            content = item.get('text', item.get('content', ''))
            
            # è¿‡æ»¤æ‰å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­çš„å¾®åš
            if not content or len(content.strip()) < 10:
                continue

            user = item.get('user', {}).get('screen_name', item.get('username', 'Unknown'))
            raw_time = item.get('created_at', item.get('time', datetime.datetime.now().strftime('%Y-%m-%d %H:%M')))
            link = item.get('url', MCP_SERVER_URL) # å¦‚æœæ²¡æœ‰é“¾æ¥ï¼Œä½¿ç”¨æœåŠ¡å™¨URLä½œä¸ºå›é€€
            
            absolute_time = parse_absolute_time(raw_time)
            sentiment_score = SnowNLP(content).sentiments
            
            tweet = {
                'user': user,
                'time': raw_time,
                'absolute_time': absolute_time.strftime('%Y-%m-%d %H:%M:%S'),
                'content': content.strip(),
                'link': link,
                'fund_type': get_fund_type(content),
                'categories': categorize_tweet(content),
                'sentiment': round(sentiment_score, 4)
            }
            tweets.append(tweet)
            
        print(f"Successfully fetched {len(tweets)} tweets via MCP Server.")
        
    except requests.exceptions.HTTPError as errh:
        print(f"HTTP Error: {errh}")
    except requests.exceptions.ConnectionError as errc:
        print(f"Connection Error (Is MCP Server running?): {errc}")
    except requests.exceptions.Timeout as errt:
        print(f"Timeout Error: {errt}")
    except requests.exceptions.RequestException as err:
        print(f"An unexpected Request Error occurred: {err}")
    except Exception as e:
        print(f"An unexpected error occurred during processing: {e}")
        
    return tweets


# --- æŠ¥å‘Šç”Ÿæˆå’Œä¿å­˜å‡½æ•° (åŠŸèƒ½ä¿ç•™) ---
def generate_report(keyword, tweets):
    """
    Generates a detailed markdown report based on the processed tweets.
    """
    timestamp = datetime.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")
    
    if not tweets:
        return f"# åŸºé‡‘ç›¸å…³å¾®åšå†…å®¹æŠ¥å‘Š\n\n**ç”Ÿæˆæ—¶é—´**ï¼š{timestamp}\n**å…³é”®è¯**ï¼š{keyword}\n**æ•°æ®æ¥æº**ï¼šæœªæŠ“å–åˆ°ä»»ä½•æ•°æ®ã€‚\n\n## å…³é”®æ´å¯Ÿä¸è¶‹åŠ¿\n- **å»ºè®®**ï¼šè¯·æ£€æŸ¥ mcp-server-weibo æœåŠ¡çš„è¿è¡ŒçŠ¶æ€ã€‚\n"
        
    avg_sentiment = sum(t['sentiment'] for t in tweets) / len(tweets)
    sentiment_label = "ç§¯æ" if avg_sentiment > 0.6 else ("æ¶ˆæ" if avg_sentiment < 0.4 else "ä¸­æ€§")
    top_keywords = extract_keywords(tweets)
        
    report_lines = [
        f"# åŸºé‡‘ç›¸å…³å¾®åšå†…å®¹æŠ¥å‘Š\n\n**ç”Ÿæˆæ—¶é—´**ï¼š{timestamp}  \n**å…³é”®è¯**ï¼š{keyword}  \n**æ•°æ®æ¥æº**ï¼šåŸºäºå¾®åšæœç´¢ç»“æœï¼Œæå–{len(tweets)}æ¡å†…å®¹ (é€šè¿‡ MCP Server æ¥å£)ã€‚  \n\n- **æƒ…æ„ŸæŒ‡æ•°**ï¼šåŸºäºSnowNLPåˆ†æï¼Œæ•´ä½“æƒ…ç»ª{sentiment_label} (å‡å€¼ï¼š{avg_sentiment:.4f})  \n- **çƒ­é—¨å…³é”®è¯**ï¼š{', '.join(top_keywords)}  \n\n## 1. åŸå§‹å¸–å­/æ–‡ç« åˆ—è¡¨ï¼ˆTop {len(tweets)} ç²¾é€‰ï¼‰\nä»¥ä¸‹æŒ‰æ—¶é—´å€’åºã€‚",
        "| # | ç”¨æˆ·/æ¥æº | æ—¶é—´ï¼ˆä¼°ç®—ï¼‰ | å½’ä¸€åŒ–æ—¶é—´ | å†…å®¹è¦ç‚¹ | æƒ…æ„Ÿ | é“¾æ¥/æ¥æº |\n|---|-----------|-------------|------------|----------|------|-----------|\n"
    ]
    
    def parse_sort_time(t):
        try:
            return datetime.datetime.strptime(t['absolute_time'], '%Y-%m-%d %H:%M:%S')
        except:
            return datetime.datetime.now()
    
    sorted_tweets = sorted(tweets, key=parse_sort_time, reverse=True)
    
    for i, tweet in enumerate(sorted_tweets, 1):
        content_snippet = tweet['content'][:100] + '...' if len(tweet['content']) > 100 else tweet['content']
        sentiment_display = f"{'ç§¯æ' if tweet['sentiment'] > 0.6 else ('æ¶ˆæ' if tweet['sentiment'] < 0.4 else 'ä¸­æ€§')}({tweet['sentiment']:.2f})"
        report_lines.append(f"| {i} | {tweet['user']} | {tweet['time']} | {tweet['absolute_time'][:16]} | {content_snippet.replace('|', '/')} | {sentiment_display} | [ğŸ”—]({tweet['link']}) |\n")
    
    
    categories = {
        'ä¸ªäººå®ç›˜/ä¹°å–è®°å½•': [], 'å¿ƒå¾—/ç»éªŒ/ä½“ä¼š': [], 'åˆ†æ': [], 'å›½å®¶æ”¿ç­–/æ³•è§„': [],
        'å®è§‚/å¾®è§‚å½±å“': [], 'å›½å†…/å›½é™…å½±å“': [], 'å…¶ä»–': []
    }
    fund_groups = {}
    buy_sell = {'buy': [], 'sell': [], 'hold': [], 'adjust': []}
    
    for tweet in tweets:
        fund_type = tweet['fund_type']
        if fund_type not in fund_groups:
            fund_groups[fund_type] = []
        fund_groups[fund_type].append(tweet)
        
        for cat in tweet['categories']:
            if cat in categories:
                categories[cat].append(tweet)
        
        content_lower = tweet['content'].lower()
        if re.search(r'(åŠ ä»“|ä¹°å…¥)', content_lower): buy_sell['buy'].append(tweet)
        if re.search(r'(å–å‡º|æ¸…ä»“|èµå›)', content_lower): buy_sell['sell'].append(tweet)
        if re.search(r'(æŒä»“|æŒæœ‰|å®šæŠ•)', content_lower): buy_sell['hold'].append(tweet)
        if re.search(r'(è°ƒä»“|è°ƒæ•´|æš‚åœ)', content_lower): buy_sell['adjust'].append(tweet)
    
    
    report_lines.append("\n## 2. åˆ†ç±»æ•´ç†\næŒ‰ç±»åˆ«åˆ†ç»„ï¼Œç›¸åŒåŸºé‡‘/ä¸»é¢˜æ±‡æ€»ã€‚")
    for category, cat_tweets in categories.items():
        if cat_tweets:
            cat_sentiment = sum(t['sentiment'] for t in cat_tweets) / len(cat_tweets)
            cat_label = "ç§¯æ" if cat_sentiment > 0.6 else ("æ¶ˆæ" if cat_sentiment < 0.4 else "ä¸­æ€§")
            report_lines.append(f"\n### {category} ({len(cat_tweets)} posts) - æƒ…ç»ª: {cat_label} (å‡å€¼: {cat_sentiment:.2f})")
            for tweet in cat_tweets[:5]: # ä»…æ˜¾ç¤ºå‰5æ¡
                content_snippet = tweet['content'][:80] + '...' if len(tweet['content']) > 80 else tweet['content']
                sentiment_status = 'ç§¯æ' if tweet['sentiment'] > 0.6 else ('æ¶ˆæ' if tweet['sentiment'] < 0.4 else 'ä¸­æ€§')
                report_lines.append(f"- **{tweet['fund_type']}** ({sentiment_status}): {tweet['user']} ({tweet['time']}): {content_snippet} [ğŸ”—]({tweet['link']})\n")
    
    
    report_lines.append("\n## 3. æŒ‰åŸºé‡‘ç±»å‹åˆ†ç»„")
    for fund_type, group_tweets in sorted(fund_groups.items(), key=lambda item: len(item[1]), reverse=True):
        if len(group_tweets) < 3 and fund_type == 'é€šç”¨åŸºé‡‘': # è¿‡æ»¤æ‰é€šç”¨åŸºé‡‘ä¸­æ•°é‡è¿‡å°‘çš„
             continue
        group_sentiment = sum(t['sentiment'] for t in group_tweets) / len(group_tweets)
        group_label = "ç§¯æ" if group_sentiment > 0.6 else ("æ¶ˆæ" if group_sentiment < 0.4 else "ä¸­æ€§")
        report_lines.append(f"\n### {fund_type} ({len(group_tweets)} posts) - æƒ…ç»ª: {group_label} (å‡å€¼: {group_sentiment:.2f})")
        for tweet in group_tweets[:5]: # ä»…æ˜¾ç¤ºå‰5æ¡
            content_snippet = tweet['content'][:80] + '...' if len(tweet['content']) > 80 else tweet['content']
            report_lines.append(f"- {tweet['user']} ({tweet['time']}): {content_snippet} [ğŸ”—]({tweet['link']})\n")
    
    
    report_lines.append("\n## 4. ä¹°å–/æŒä»“/è°ƒä»“å¯¹æ¯”")
    for action, action_tweets in buy_sell.items():
        if action_tweets:
            action_name = action.capitalize()
            action_sentiment = sum(t['sentiment'] for t in action_tweets) / len(action_tweets)
            action_label = "ç§¯æ" if action_sentiment > 0.6 else ("æ¶ˆæ" if action_sentiment < 0.4 else "ä¸­æ€§")
            report_lines.append(f"\n### {action_name} ({len(action_tweets)} posts) - æƒ…ç»ª: {action_label} (å‡å€¼: {action_sentiment:.2f})")
            top_funds = get_top_fund_types(action_tweets, n=2)
            funds_str = 'ã€'.join(top_funds) if top_funds else 'æ— æ˜æ˜¾çƒ­ç‚¹'
            report_lines.append(f"**çƒ­ç‚¹åŸºé‡‘**ï¼š{funds_str}\n")
            
            for tweet in action_tweets[:3]: # ä»…æ˜¾ç¤ºå‰3æ¡
                content_snippet = tweet['content'][:60] + '...' if len(tweet['content']) > 60 else tweet['content']
                report_lines.append(f"- **{tweet['fund_type']}**: {tweet['user']}: {content_snippet} [ğŸ”—]({tweet['link']})\n")
    
    
    trends = []
    if buy_sell['buy']:
        top_buy_funds = get_top_fund_types(buy_sell['buy'], n=2)
        if top_buy_funds:
            funds_str = 'ã€'.join(top_buy_funds)
            trends.append(f"**åŠ ä»“çƒ­ç‚¹**ï¼šæ•°æ®æ˜¾ç¤ºï¼Œç”¨æˆ·å¯¹ **{funds_str}** åŸºé‡‘æ“ä½œæœ€ç§¯æï¼Œè¡¨ç°å‡ºä¹°å…¥/åŠ ä»“å€¾å‘ã€‚")
        else:
            trends.append("ä¹°å…¥/åŠ ä»“æ“ä½œè¾ƒåˆ†æ•£ï¼Œæš‚æ— æ˜æ˜¾çƒ­ç‚¹åŸºé‡‘ã€‚")

    if buy_sell['sell'] or buy_sell['adjust']:
        all_sell_adjust = buy_sell['sell'] + buy_sell['adjust']
        top_sell_funds = get_top_fund_types(all_sell_adjust, n=2)
        if top_sell_funds:
            funds_str = 'ã€'.join(top_sell_funds)
            trends.append(f"**è°ƒä»“é£é™©**ï¼š**{funds_str}** åŸºé‡‘çš„å–å‡º/è°ƒä»“è¡Œä¸ºæœ€ä¸ºé›†ä¸­ï¼Œéœ€å…³æ³¨æ½œåœ¨å›è°ƒé£é™©ã€‚")
        else:
            trends.append("å–å‡º/è°ƒä»“æ“ä½œè¾ƒåˆ†æ•£ï¼Œä½†ç”¨æˆ·æœ‰è¿›è¡Œé£é™©è°ƒæ•´çš„è¡Œä¸ºã€‚")

    if buy_sell['hold']:
        top_hold_funds = get_top_fund_types(buy_sell['hold'], n=1)
        funds_str = 'ã€'.join(top_hold_funds) if top_hold_funds else 'å¤šåªåŸºé‡‘'
        trends.append(f"**æŒä»“é£æ ¼**ï¼šç”¨æˆ·å¼ºè°ƒå¯¹ **{funds_str}** çš„é•¿æœŸå®šæŠ•æˆ–æŒæœ‰ç­–ç•¥ã€‚")

    policy_macro_tweets = categories['å›½å®¶æ”¿ç­–/æ³•è§„'] + categories['å®è§‚/å¾®è§‚å½±å“']
    if policy_macro_tweets:
        policy_keywords = extract_keywords(policy_macro_tweets, top_n=3)
        keywords_str = 'ã€'.join(policy_keywords) if policy_keywords else 'æ”¿ç­–æˆ–å®è§‚ç»æµ'
        trends.append(f"**æ”¿ç­–å½±å“**ï¼šè®¨è®ºé›†ä¸­åœ¨ **{keywords_str}** ç­‰å…³é”®è¯ï¼Œæ˜¾ç¤ºæ”¿ç­–é©±åŠ¨æˆ–å®è§‚å˜åŒ–æ˜¯é‡è¦å…³æ³¨ç‚¹ã€‚")
    
    
    enhanced_trends = [
        f"**æƒ…æ„Ÿæ¦‚è§ˆ**: æ•´ä½“æƒ…ç»ªå€¾å‘äº{sentiment_label} (å‡å€¼: {avg_sentiment:.2f})ã€‚",
        f"**çƒ­é—¨å…³é”®è¯**: æœ¬æœŸè®¨è®ºä¸­ï¼Œ**{', '.join(top_keywords)}** è¯é¢‘æœ€é«˜ï¼Œæ˜¾ç¤ºå¸‚åœºç„¦ç‚¹é›†ä¸­äºæ­¤ã€‚",
        f"**äº¤æ˜“è¡Œä¸º**: {' '.join(trends) if trends else 'æš‚æ— æ˜æ˜¾äº¤æ˜“è¶‹åŠ¿'}"
    ]
    
    trends_content = '\n'.join([f'- {t}' for t in enhanced_trends])
    report_lines.append(f"\n## 5. å…³é”®æ´å¯Ÿä¸è¶‹åŠ¿\n{trends_content}\n- **å»ºè®®**ï¼šæŠ¥å‘ŠåŸºäºå…¬å¼€ç‰‡æ®µçš„è‡ªåŠ¨åˆ†æï¼Œè¯·åŠ¡å¿…æ£€æŸ¥ mcp-server-weibo çš„è¿è¡Œæƒ…å†µã€‚\n")
    
    return ''.join(report_lines)

def save_json_data(tweets, output_path="reports"):
    """
    ä¿å­˜åŸå§‹æ•°æ®åˆ° JSON æ–‡ä»¶ã€‚
    """
    Path(output_path).mkdir(exist_ok=True)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{output_path}/weibo_fund_data_{timestamp}.json"
    
    serializable_tweets = [
        {k: v for k, v in t.items()} for t in tweets
    ]
    
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(serializable_tweets, f, ensure_ascii=False, indent=4)
    print(f"Raw data saved to {filename}")
    return filename


def save_report(report_content, output_path="reports"):
    """
    Save the report to a Markdown file.
    """
    Path(output_path).mkdir(exist_ok=True)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{output_path}/weibo_fund_report_{timestamp}.md"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(report_content)
    print(f"Report saved to {filename}")
    return filename

def main():
    keyword = os.getenv("WEIBO_KEYWORD", "åŸºé‡‘")
    tweets = fetch_weibo_data(keyword)
    
    if not tweets:
        print("Scraper returned 0 tweets. Exiting script as there is no data to report.")
        return 

    save_json_data(tweets) 
    
    report_content = generate_report(keyword, tweets)
    save_report(report_content)

if __name__ == "__main__":
    main()
