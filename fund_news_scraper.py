import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from typing import List, Dict
import re
import xml.etree.ElementTree as ET
from dateutil import parser
import pytz
import matplotlib.pyplot as plt
from collections import Counter
import sqlite3
import json
import logging
import time
from wordcloud import WordCloud
import jieba
from retry import retry
# å¢åŠ  os å’Œ json æ¨¡å—ç”¨äºé…ç½®åˆ†ç¦»
import os 
import json 

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('fund_news_scraper.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# --- ã€æ ¸å¿ƒé…ç½®ã€‘åˆ†æè§„åˆ™åº“ï¼šé€šç”¨ã€å¯æ‰©å±•çš„åˆ†æé€»è¾‘ ---

# 1. æŠ•èµ„çº¿ç´¢ (æ ‡çš„/ç­–ç•¥ -> æ€»ç»“)
CLUES_MAP = {
    r'çœ‹å¥½|å»ºè®®é…ç½®|ç­–ç•¥ä¸»çº¿|èšç„¦|å¸ƒå±€|æ¨è|é‡‘è‚¡|å®ç›˜': {'desc': 'ã€é€šç”¨ç­–ç•¥ä¿¡å·ã€‘è¯†åˆ«åˆ°æ˜ç¡®çš„é…ç½®å»ºè®®æˆ–ç­–ç•¥ä¸»çº¿', 'weight': 1.0},
    r'å®è§‚|ç­–ç•¥æŠ¥å‘Š|å‘¨æœŸ|å››ä¸­å…¨ä¼š|åäº”äº”': {'desc': 'ã€å®è§‚ç­–ç•¥ä¿¡å·ã€‘å®è§‚æˆ–å‘¨æœŸæ€§ä¸»é¢˜æŠ¥å‘Š', 'weight': 0.8},
    r'æè““|åŠå¤|ä¸­è¯500|IC': {'desc': 'ç§å‹Ÿè§‚ç‚¹ï¼šä¸­è¯500/ç§‘æŠ€æˆé•¿ç­–ç•¥', 'weight': 0.9},
    r'åå®‰è¯åˆ¸|æˆé•¿äº§ä¸š|AI|å†›å·¥': {'desc': 'åˆ¸å•†è§‚ç‚¹ï¼šAI/å†›å·¥/æ–°æˆé•¿äº§ä¸šé“¾é…ç½®', 'weight': 0.9},
    r'å¼€æºè¯åˆ¸|é‡‘è‚¡ç­–ç•¥|ç§‘æŠ€|æ¸¯è‚¡': {'desc': 'åˆ¸å•†è§‚ç‚¹ï¼šAI+è‡ªä¸»å¯æ§ç§‘æŠ€ä¸»çº¿', 'weight': 0.9},
    r'ETF|è‚¡ç¥¨ETF|ç™¾äº¿ä¿±ä¹éƒ¨|å¸é‡‘': {'desc': 'èµ„é‡‘æµå‘/è‚¡ç¥¨ETF/å¸é‡‘èµ›é“', 'weight': 0.8},
    r'è´µé‡‘å±|é»„é‡‘|é¿é™©': {'desc': 'èµ„äº§å¯¹å†²/é¿é™©é…ç½® (è´µé‡‘å±)', 'weight': 0.7},
    r'å‡è¡¡é…ç½®|å…‰ä¼|åŒ–å·¥|å†œä¸š|æœ‰è‰²|é“¶è¡Œ': {'desc': 'ä½ä½/å‡è¡¡æ¿å—é…ç½®å»ºè®®', 'weight': 0.7},
    r'ç§‘æŠ€è‚¡|AIç®—åŠ›|äººå½¢æœºå™¨äºº': {'desc': 'ç§‘æŠ€åˆ›æ–°/AIé©±åŠ¨äº§ä¸šé“¾æœºä¼š', 'weight': 1.0},
    r'æ¶ˆè´¹|ETF|ç§‘æŠ€èåˆ': {'desc': 'æ¶ˆè´¹ç§‘æŠ€èåˆé…ç½®', 'weight': 0.8},
    r'å…¬å‹Ÿè§„æ¨¡|çªç ´|å¢é•¿': {'desc': 'å…¬å‹Ÿè¡Œä¸šè§„æ¨¡æ‰©å¼ ä¿¡å·', 'weight': 0.7},
    # æ–°å¢è§„åˆ™ï¼šåŠ å¼ºæ–°èƒ½æºå’Œæµ·å¤–å¸‚åœºåˆ†æ
    r'æ–°èƒ½æº|ç”µæ± |å…‰ä¼|é£ç”µ': {'desc': 'æ–°èƒ½æºé…ç½®æœºä¼š', 'weight': 0.8},
    r'æµ·å¤–|æ¸¯è‚¡|ç¾è‚¡|QDII': {'desc': 'æµ·å¤–èµ„äº§é…ç½®ä¿¡å·', 'weight': 0.7},
}

# 2. ç»éªŒæ•™è®­ (è¡Œä¸º/ç»“æœ -> é£é™©/æ•™è®­)
LESSONS_MAP = {
    r'è­¦æƒ•|é£é™©|æ•™è®­|æ¶‰èµŒ|è·‘è¾“|å†…æ§': {'desc': 'ã€é€šç”¨é£é™©ä¿¡å·ã€‘è¯†åˆ«åˆ°è¡Œä¸šé£é™©æˆ–è´Ÿé¢ç»éªŒæ•™è®­', 'weight': 1.0},
    r'è·‘è¾“å¤§ç›˜|æœªèƒ½æ»¡ä»“|çº¢åˆ©æ¿å—': {'desc': 'æ–°åŸºé‡‘å»ºä»“ç­–ç•¥ä¸å¸‚åœºé”™é…é£é™©', 'weight': 0.9},
    r'åŸºé‡‘ç»ç†|æ¶‰èµŒ|å…èŒ': {'desc': 'åŸºé‡‘ç»ç†é“å¾·é£é™©ä¸å…¬å¸å†…æ§è­¦ç¤º', 'weight': 1.0},
    r'æœºæ„å¤§ä¸¾å¢æŒ|ä¸»åŠ¨æƒç›ŠåŸºé‡‘': {'desc': 'æœºæ„è¡Œä¸ºï¼šä¸»åŠ¨æƒç›ŠåŸºé‡‘ä»æ˜¯é…ç½®é‡ç‚¹', 'weight': 0.8},
    r'ä¼ªæˆé•¿|æ‹¥æŒ¤|é™·é˜±': {'desc': 'æˆé•¿èµ›é“æ‹¥æŒ¤ä¸ä¼ªæˆé•¿é£é™©', 'weight': 0.9},
    r'å‡æŒ|é«˜ä½': {'desc': 'è‚¡ä¸œå‡æŒä¸é«˜ä½å›è°ƒé£é™©', 'weight': 0.9},
    # æ–°å¢è§„åˆ™ï¼šåŠ å¼ºå¸‚åœºæ³¢åŠ¨å’Œæ”¿ç­–é£é™©
    r'æ³¢åŠ¨|å›è°ƒ|æ³¡æ²«': {'desc': 'å¸‚åœºæ³¢åŠ¨ä¸ä¼°å€¼å›è°ƒé£é™©', 'weight': 0.9},
    r'æ”¿ç­–|ç›‘ç®¡|åˆè§„': {'desc': 'æ”¿ç­–å˜åŠ¨ä¸ç›‘ç®¡åˆè§„é£é™©', 'weight': 0.8},
}

# 3. è¡Œä¸šè¶‹åŠ¿ (ç»“æ„å˜åŒ– -> è¡Œä¸šæ´å¯Ÿ)
TRENDS_MAP = {
    r'AI|æŠ•ç ”|å·¥ä¸šåŒ–|èš‚èšè´¢å¯Œ': {'desc': 'è¡Œä¸šè¶‹åŠ¿ï¼šæŠ•ç ”å·¥ä¸šåŒ–å’ŒAIèµ‹èƒ½', 'weight': 0.9},
    r'è´¹ç‡|ä¸‹è°ƒ|æ‰˜ç®¡è´¹|ä½™é¢å®': {'desc': 'è¡Œä¸šè¶‹åŠ¿ï¼šå…³æ³¨è´¹ç‡æˆæœ¬çš„é•¿æœŸä¸‹è¡Œ', 'weight': 0.8},
    r'ç§å‹Ÿè‚¡æƒ|å­å…¬å¸|å¹¿å‘åŸºé‡‘': {'desc': 'è¡Œä¸šè¶‹åŠ¿ï¼šå¤´éƒ¨å…¬å‹Ÿçš„ä¸šåŠ¡å¤šå…ƒåŒ–', 'weight': 0.8},
    r'é‡åŒ–åŸºé‡‘ç»ç†|ä¸»åŠ¨åŸºé‡‘|ä¸€æ‹–å¤š': {'desc': 'è¡Œä¸šè¶‹åŠ¿ï¼šé‡åŒ–ä¸ä¸»åŠ¨æŠ•èµ„è¾¹ç•Œæ¨¡ç³Š', 'weight': 0.8},
    r'REITs|è·æ‰¹|åŸºç¡€è®¾æ–½': {'desc': 'REITså¸‚åœºæ‰©å¼ ä¸åŸºç¡€è®¾æ–½æŠ•èµ„è¶‹åŠ¿', 'weight': 0.7},
    r'ESG|å‡æ’|ç»¿è‰²é‡‘è': {'desc': 'ESGä¸ç»¿è‰²æŠ•èµ„è¶‹åŠ¿', 'weight': 0.7},
    r'å…»è€|ç¬¬ä¸‰æ”¯æŸ±': {'desc': 'å…»è€åŸºé‡‘ä¸é•¿æœŸæŠ•èµ„ä½“ç³»å»ºè®¾', 'weight': 0.7},
    # æ–°å¢è§„åˆ™ï¼šåŠ å¼ºæ•°å­—åŒ–å’Œè·¨å¢ƒè¶‹åŠ¿
    r'æ•°å­—åŒ–|FinTech|åŒºå—é“¾': {'desc': 'æ•°å­—åŒ–è½¬å‹ä¸FinTechè¶‹åŠ¿', 'weight': 0.8},
    r'è·¨å¢ƒ|REITs|æµ·å¤–åŸºé‡‘': {'desc': 'è·¨å¢ƒæŠ•èµ„ä¸REITså›½é™…åŒ–è¶‹åŠ¿', 'weight': 0.7},
}

# èšåˆæ‰€æœ‰ä¸»é¢˜ï¼Œç”¨äºé•¿æœŸè¶‹åŠ¿åˆ†æ
ALL_TOPICS_MAP = {**CLUES_MAP, **LESSONS_MAP, **TRENDS_MAP}

# æ–°å¢ï¼šæ½œåœ¨å½±å“æ¨¡æ¿åº“ (åŸºäºå…³é”®è¯ç”Ÿæˆæ–°é—»å½±å“æ€»ç»“)
IMPACT_TEMPLATES = {
    r'AI|ç®—åŠ›|ç§‘æŠ€é¾™å¤´|äº§ä¸šé“¾': 'æ½œåœ¨å½±å“ï¼šå¯èƒ½æ¨åŠ¨ç§‘æŠ€æ¿å—çŸ­æœŸä¸Šæ¶¨ï¼Œä½†éœ€è­¦æƒ•ä¼°å€¼æ³¡æ²«é£é™©ï¼Œå»ºè®®å…³æ³¨äº§ä¸šé“¾ä¸­ä½ä¼°å€¼æ ‡çš„ã€‚',
    r'è´¹ç‡|é™è´¹|è´§å¸åŸºé‡‘': 'æ½œåœ¨å½±å“ï¼šé™ä½æŠ•èµ„è€…æˆæœ¬ï¼Œæå‡åŸºé‡‘å¸å¼•åŠ›ï¼Œé•¿æœŸåˆ©å¥½å…¬å‹Ÿè¡Œä¸šè§„æ¨¡æ‰©å¼ ï¼Œä½†çŸ­æœŸå¯èƒ½æŒ¤å‹åŸºé‡‘å…¬å¸åˆ©æ¶¦ã€‚',
    r'é£é™©|è­¦æƒ•|è·‘è¾“|é“å¾·é£é™©': 'æ½œåœ¨å½±å“ï¼šå¢åŠ å¸‚åœºæ³¢åŠ¨æ€§ï¼ŒæŠ•èµ„è€…åº”åŠ å¼ºé£é™©ç®¡ç†ï¼Œé¿å…è¿½é«˜çƒ­é—¨èµ›é“ã€‚',
    r'ETF|å¸é‡‘|èµ„é‡‘æµå‘': 'æ½œåœ¨å½±å“ï¼šåŠ é€Ÿèµ„é‡‘å‘çƒ­é—¨ä¸»é¢˜å€¾æ–œï¼Œå¢å¼ºå¸‚åœºæµåŠ¨æ€§ï¼Œä½†å¯èƒ½æ”¾å¤§æ¿å—è½®åŠ¨æ•ˆåº”ã€‚',
    r'å®è§‚|ç­–ç•¥æŠ¥å‘Š|éœ‡è¡ä¸Šè¡Œ': 'æ½œåœ¨å½±å“ï¼šå››å­£åº¦å¸‚åœºæˆ–å‘ˆNå‹èµ°åŠ¿ï¼Œç§‘æŠ€ä¸åå†…å·æ¿å—å—ç›Šï¼Œå»ºè®®å‡è¡¡é…ç½®ã€‚',
    r'ç§å‹Ÿè§‚ç‚¹|ä¸­è¯500': 'æ½œåœ¨å½±å“ï¼šä¸­è¯500æŒ‡æ•°å¯èƒ½å¸å¼•æ›´å¤šèµ„é‡‘æµå…¥ç§‘æŠ€æˆé•¿è‚¡ï¼Œæå‡æŒ‡æ•°è¡¨ç°ã€‚',
    r'è´µé‡‘å±|é¿é™©': 'æ½œåœ¨å½±å“ï¼šåœ¨åœ°ç¼˜é£é™©ä¸‹ï¼Œè´µé‡‘å±ä½œä¸ºå¯¹å†²å·¥å…·éœ€æ±‚ä¸Šå‡ï¼Œé…ç½®ä»·å€¼æå‡ã€‚',
    r'ä¸šåŠ¡å¤šå…ƒåŒ–|å­å…¬å¸': 'æ½œåœ¨å½±å“ï¼šå…¬å‹Ÿæ‰©å±•ç§å‹Ÿè‚¡æƒç­‰é¢†åŸŸï¼Œå¢å¼ºç»¼åˆç«äº‰åŠ›ï¼Œåˆ©å¥½é•¿æœŸæŠ•èµ„è€…ã€‚',
    r'REITs|è·æ‰¹': 'æ½œåœ¨å½±å“ï¼šREITsè·æ‰¹å°†æ³¨å…¥æ–°æ´»åŠ›ï¼Œä¿ƒè¿›åŸºç¡€è®¾æ–½æŠ•èµ„ï¼Œå¸å¼•æ›´å¤šèµ„é‡‘è¿›å…¥ç›¸å…³é¢†åŸŸã€‚',
    r'ESG|å‡æ’': 'æ½œåœ¨å½±å“ï¼šESGæ”¿ç­–å¼ºåŒ–å°†æ¨åŠ¨ç»¿è‰²è½¬å‹ï¼Œåˆ©å¥½å¯æŒç»­æŠ•èµ„ä¸»é¢˜åŸºé‡‘ã€‚',
    r'å…»è€|ç¬¬ä¸‰æ”¯æŸ±': 'æ½œåœ¨å½±å“ï¼šå…»è€ä½“ç³»å®Œå–„å°†å¢åŠ é•¿æœŸèµ„é‡‘ä¾›ç»™ï¼Œç¨³å®šèµ„æœ¬å¸‚åœºã€‚',
    # æ–°å¢æ¨¡æ¿ï¼šåŠ å¼ºæ–°èƒ½æºå’Œæµ·å¤–å½±å“
    r'æ–°èƒ½æº|ç”µæ± |å…‰ä¼|é£ç”µ': 'æ½œåœ¨å½±å“ï¼šæ”¿ç­–æ”¯æŒä¸‹æ–°èƒ½æºæ¿å—æˆ–æŒç»­åå¼¹ï¼Œå»ºè®®é…ç½®é¾™å¤´ä¼ä¸šï¼Œä½†å…³æ³¨ä¾›åº”é“¾é£é™©ã€‚',
    r'æµ·å¤–|æ¸¯è‚¡|ç¾è‚¡|QDII': 'æ½œåœ¨å½±å“ï¼šå…¨çƒæµåŠ¨æ€§å®½æ¾åˆ©å¥½æµ·å¤–èµ„äº§ï¼ŒQDIIåŸºé‡‘é…ç½®ä»·å€¼æå‡ï¼Œä½†éœ€è­¦æƒ•æ±‡ç‡æ³¢åŠ¨ã€‚',
    r'æ³¢åŠ¨|å›è°ƒ|æ³¡æ²«': 'æ½œåœ¨å½±å“ï¼šçŸ­æœŸå¸‚åœºè°ƒæ•´å¯èƒ½åŠ å‰§ï¼ŒæŠ•èµ„è€…åº”åˆ†æ•£æŒä»“ï¼Œç­‰å¾…ä½ä½å¸ƒå±€æœºä¼šã€‚',
    r'æ”¿ç­–|ç›‘ç®¡|åˆè§„': 'æ½œåœ¨å½±å“ï¼šæ–°æ”¿è½åœ°æˆ–é‡å¡‘è¡Œä¸šæ ¼å±€ï¼Œåˆ©å¥½åˆè§„å¤´éƒ¨æœºæ„ï¼Œä½†ä¸­å°ç©å®¶é¢ä¸´æ•´åˆå‹åŠ›ã€‚',
    r'æ•°å­—åŒ–|FinTech|åŒºå—é“¾': 'æ½œåœ¨å½±å“ï¼šFinTechåˆ›æ–°åŠ é€Ÿé‡‘èæ•ˆç‡æå‡ï¼Œç›¸å…³åŸºé‡‘æˆ–è¿æ¥å¢é•¿å‘¨æœŸã€‚',
    r'è·¨å¢ƒ|REITs': 'æ½œåœ¨å½±å“ï¼šè·¨å¢ƒREITsæ‰©å¼ å°†å¤šå…ƒåŒ–æŠ•èµ„æ¸ é“ï¼Œå¸å¼•æµ·å¤–èµ„é‡‘æµå…¥åŸºç¡€è®¾æ–½é¢†åŸŸã€‚',
    r'.*': 'æ½œåœ¨å½±å“ï¼šè¯¥æ–°é—»å¯èƒ½å¯¹ç›¸å…³æ¿å—äº§ç”Ÿä¸­æ€§å½±å“ï¼Œå»ºè®®ç»“åˆå¸‚åœºåŠ¨æ€è¿›ä¸€æ­¥è¯„ä¼°ã€‚'
}

# æ–°å¢ï¼šç®€å•æƒ…æ„Ÿåˆ†æå…³é”®è¯
POSITIVE_WORDS = ['çœ‹å¥½', 'ä¸Šæ¶¨', 'å¢é•¿', 'æœºä¼š', 'å¸ƒå±€', 'æ¨è', 'æ½œåŠ›', 'çªç ´']
NEGATIVE_WORDS = ['é£é™©', 'è­¦æƒ•', 'è·‘è¾“', 'å‡æŒ', 'æ•™è®­', 'é™·é˜±', 'æ³¢åŠ¨', 'ä¸‹è·Œ']

# --- æ–°å¢ï¼šåŠ¨æ€å…³é”®è¯æ‰©å±• ---
def extract_dynamic_keywords(text: str, min_freq: int = 2) -> List[str]:
    """åŸºäº jieba åˆ†è¯åŠ¨æ€æå–é«˜é¢‘å…³é”®è¯ï¼Œæ’é™¤å·²æœ‰è§„åˆ™ä¸­çš„å…³é”®è¯ã€‚"""
    # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œæ•°å­—ï¼Œåªä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€ç©ºæ ¼
    clean_text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z\s]', '', text)
    words = jieba.cut(clean_text)
    word_freq = Counter(words)
    
    # æ’é™¤é€šç”¨åœç”¨è¯å’Œè§„åˆ™ä¸­çš„è¯
    stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'å’Œ', 'ä¹Ÿ', 'ç­‰', 'åŸºé‡‘', 'æŠ•èµ„', 'åˆ†æ', 'æŠ¥å‘Š', 'é‡‘è', 'è¯åˆ¸', 'å…¬å¸', 'å…¬å¸ƒ', 'æ•°æ®'}
    existing_keywords = set()
    for pattern in ALL_TOPICS_MAP.keys():
        existing_keywords.update(pattern.split('|'))
    
    dynamic_keywords = [
        word for word, freq in word_freq.items() 
        if freq >= min_freq and 
           word not in existing_keywords and 
           word not in stopwords and
           len(word) > 1
    ]
    return dynamic_keywords[:5] # è¿”å›å‰5ä¸ªé«˜é¢‘è¯

# --- æ•°æ®åº“ç®¡ç†ç±» ---
class DatabaseManager:
    def __init__(self, db_name='fund_news_analysis.db'):
        self.db_name = db_name
        self.conn = None
        self._connect()
        self._create_table()

    def _connect(self):
        self.conn = sqlite3.connect(self.db_name)
        self.conn.row_factory = sqlite3.Row
        # ä¼˜åŒ–ï¼šå¯ç”¨ WAL æ¨¡å¼ï¼Œæå‡å¹¶å‘æ€§èƒ½
        self.conn.execute('PRAGMA journal_mode=WAL')
        
    def _create_table(self):
        cursor = self.conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS analyzed_news (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                link TEXT UNIQUE,
                pubDate TEXT,
                source TEXT,
                crawl_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                sentiment TEXT,
                key_topics_json TEXT,
                dynamic_keywords_json TEXT
            )
        ''')
        # ä¼˜åŒ–ï¼šä¸ºå¸¸ç”¨æŸ¥è¯¢å­—æ®µæ·»åŠ ç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_pubDate ON analyzed_news(pubDate)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_source ON analyzed_news(source)')
        self.conn.commit()

    def get_existing_links(self) -> set:
        cursor = self.conn.cursor()
        cursor.execute("SELECT link FROM analyzed_news WHERE link IS NOT NULL AND link != 'N/A'")
        return {row['link'] for row in cursor.fetchall()}

    def store_news_and_analysis(self, news_item: Dict, analysis_result: Dict):
        title = news_item['title']
        link = news_item.get('link', 'N/A')
        pubDate = news_item.get('pubDate', datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d %H:%M:%S'))
        source = news_item['source']
        sentiment = analysis_result.get('sentiment', 'ä¸­æ€§ (Neutral)')
        key_topics_json = json.dumps(analysis_result.get('key_topics', []))
        dynamic_keywords_json = json.dumps(analysis_result.get('dynamic_keywords', []))
        
        cursor = self.conn.cursor()
        try:
            cursor.execute('''
                INSERT INTO analyzed_news (title, link, pubDate, source, sentiment, key_topics_json, dynamic_keywords_json)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (title, link, pubDate, source, sentiment, key_topics_json, dynamic_keywords_json))
            self.conn.commit()
        except sqlite3.IntegrityError:
            logger.warning(f"Duplicate link found, skipping: {link}")
        except Exception as e:
            logger.error(f"Error storing news to DB: {e}")

    def get_topics_by_time_range(self, days: int) -> Dict[str, int]:
        since_date = (datetime.now(pytz.timezone('Asia/Shanghai')) - timedelta(days=days)).strftime('%Y-%m-%d %H:%M:%S')
        cursor = self.conn.cursor()
        cursor.execute("SELECT key_topics_json FROM analyzed_news WHERE pubDate > ?", (since_date,))
        
        topic_counter = Counter()
        for row in cursor.fetchall():
            try:
                topics = json.loads(row['key_topics_json'])
                topic_counter.update(topics)
            except (json.JSONDecodeError, TypeError):
                continue
        return dict(topic_counter)

    def get_dynamic_keywords_by_time_range(self, days: int) -> Dict[str, int]:
        since_date = (datetime.now(pytz.timezone('Asia/Shanghai')) - timedelta(days=days)).strftime('%Y-%m-%d %H:%M:%S')
        cursor = self.conn.cursor()
        cursor.execute("SELECT dynamic_keywords_json FROM analyzed_news WHERE pubDate > ?", (since_date,))
        
        keyword_counter = Counter()
        for row in cursor.fetchall():
            try:
                keywords = json.loads(row['dynamic_keywords_json'])
                keyword_counter.update(keywords)
            except (json.JSONDecodeError, TypeError):
                continue
        return dict(keyword_counter)

    def close(self):
        if self.conn:
            self.conn.close()
            logger.info("Database connection closed.")

# --- è¾…åŠ©å‡½æ•°ï¼šæ—¶é—´è§£æå’Œæ ¼å¼åŒ– ---
def parse_and_format_time(pub_date: str) -> str:
    if pub_date == 'N/A' or not pub_date:
        return 'N/A'
    try:
        dt_utc = parser.parse(pub_date).replace(tzinfo=pytz.utc)
        dt_local = dt_utc.astimezone(pytz.timezone('Asia/Shanghai'))
        return dt_local.strftime('%Y-%m-%d %H:%M:%S')
    except Exception:
        logger.warning(f"Failed to parse date: {pub_date}")
        return pub_date

# --- è¾…åŠ©å‡½æ•°ï¼šHTMLæ¸…ç†å’Œæ‘˜è¦å¤„ç† ---
def clean_html_summary(summary: str, max_len: int = 400) -> str:
    if not summary:
        return 'æ— æ‘˜è¦'
    clean_soup = BeautifulSoup(summary, 'html.parser')
    clean_text = clean_soup.get_text(strip=True)
    clean_text = re.sub(r'\s+', ' ', clean_text)
    if len(clean_text) > max_len:
        return clean_text[:max_len] + '...'
    return clean_text

# --- æ–°å¢ï¼šä¼˜åŒ–æƒ…æ„Ÿåˆ†æ ---
def weighted_sentiment_analysis(text: str) -> tuple[str, float]:
    pos_score = 0
    neg_score = 0
    for word in POSITIVE_WORDS:
        if re.search(word, text, re.IGNORECASE):
            pos_score += text.lower().count(word.lower()) * 1.0
    for word in NEGATIVE_WORDS:
        if re.search(word, text, re.IGNORECASE):
            neg_score += text.lower().count(word.lower()) * 1.0
    
    total_score = pos_score - neg_score
    if total_score > 0:
        sentiment = 'æ­£é¢ (Positive)'
    elif total_score < 0:
        sentiment = 'è´Ÿé¢ (Negative)'
    else:
        sentiment = 'ä¸­æ€§ (Neutral)'
    return sentiment, total_score

# --- è¯¦ç»†æ–°é—»åˆ†æå‡½æ•° ---
def detailed_analyze_news(item: Dict) -> Dict:
    text = item['title'] + ' ' + item['summary']
    analysis = {
        'title': item['title'],
        'detailed_summary': f"æ ‡é¢˜ï¼š{item['title']}\næ‘˜è¦ï¼š{item['summary']}",
        'key_topics': [],
        'potential_impact': '',
        'sentiment': 'ä¸­æ€§ (Neutral)',
        'sentiment_score': 0.0,
        'dynamic_keywords': extract_dynamic_keywords(text)
    }
    
    # æå–å…³é”®ä¸»é¢˜ï¼ˆè€ƒè™‘æƒé‡ï¼‰
    for map_dict in [CLUES_MAP, LESSONS_MAP, TRENDS_MAP]:
        for pattern, info in map_dict.items():
            if re.search(pattern, text, re.IGNORECASE):
                analysis['key_topics'].append(info['desc'])
    
    # ç”Ÿæˆæ½œåœ¨å½±å“
    impact_found = False
    for pattern, impact in IMPACT_TEMPLATES.items():
        if re.search(pattern, text, re.IGNORECASE):
            analysis['potential_impact'] = impact
            impact_found = True
            break
    if not impact_found:
        analysis['potential_impact'] = IMPACT_TEMPLATES['.*']
    
    # ä¼˜åŒ–æƒ…æ„Ÿåˆ†æ
    analysis['sentiment'], analysis['sentiment_score'] = weighted_sentiment_analysis(text)
    
    return analysis

# --- æ ¸å¿ƒæŠ“å–å‡½æ•°ï¼šRSS ---
@retry(tries=3, delay=2, backoff=2, logger=logger)
def fetch_rss_feed(url: str, source_name: str, limit: int = 20) -> List[Dict]:
    filtered_items = []
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    response = requests.get(url, timeout=10, headers=headers)
    response.raise_for_status()
    
    try:
        # å°è¯•ç›´æ¥è§£æ
        root = ET.fromstring(response.content)
    except ET.ParseError:
        logger.warning(f"[{source_name}] Error parsing XML. Trying content decoding...")
        # å°è¯•ç”¨ utf-8 è§£ç åè§£æï¼Œä»¥å¤„ç†ç¼–ç é—®é¢˜
        root = ET.fromstring(response.text.encode('utf-8'))

    items = root.findall('.//item') or root.findall('.//entry')
    for item in items[:limit]:
        # ä¿®å¤ï¼šä½¿ç”¨æ˜¾å¼çš„ 'is not None' æ£€æŸ¥
        title_element = item.find('title')
        link_element = item.find('link')
        pub_date_element = item.find('pubDate') or item.find('{http://purl.org/dc/elements/1.1/}date') or item.find('published')
        summary_element = item.find('description') or item.find('summary') or item.find('content')

        title = title_element.text.strip() if title_element is not None and title_element.text else ''
        
        link = 'N/A'
        if link_element is not None:
            if link_element.text:
                link = link_element.text.strip()
            elif link_element.attrib.get('href'):
                link = link_element.attrib['href'].strip()

        pub_date_raw = pub_date_element.text.strip() if pub_date_element is not None and pub_date_element.text else 'N/A'
        summary_raw = summary_element.text.strip() if summary_element is not None and summary_element.text else ''
        
        summary = clean_html_summary(summary_raw, max_len=400)
        pub_date = parse_and_format_time(pub_date_raw)
        
        if re.search(r'åŸºé‡‘|å®ç›˜|è§‚ç‚¹|ç»éªŒ|æ¨è|ç­–ç•¥|æŠ•èµ„|è‚¡ç¥¨|å®è§‚|é‡‘è', title + summary, re.IGNORECASE):
            filtered_items.append({
                'title': title,
                'link': link,
                'pubDate': pub_date,
                'summary': summary,
                'source': source_name
            })
    return filtered_items

# --- æ ¸å¿ƒæŠ“å–å‡½æ•°ï¼šWeb ---
@retry(tries=3, delay=2, backoff=2, logger=logger)
def fetch_web_page(url: str, source_name: str, selector: str, limit: int = 15) -> List[Dict]:
    filtered_items = []
    
    # æå–åŸºç¡€åŸŸåç”¨äºæ„å»º Referer
    base_domain = url.split('/')[2]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/555.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/555.36',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Referer': f'https://{base_domain}/' 
    }
    response = requests.get(url, timeout=10, headers=headers)
    response.raise_for_status()
    soup = BeautifulSoup(response.content, 'html.parser')
    
    items = soup.select(selector)
    for item in items[:limit]:
        title_tag = item
        if not title_tag:
            continue
        
        title = title_tag.get_text(strip=True)
        link = title_tag.get('href', '')
        
        # ä¿®å¤é“¾æ¥æ‹¼æ¥ï¼šç¡®ä¿ Web é“¾æ¥æ˜¯å®Œæ•´çš„
        if link and not link.startswith('http'):
            # å…¼å®¹éœ€è¦å®Œæ•´é“¾æ¥çš„æƒ…å†µ
            link = requests.compat.urljoin(url, link)
        
        # ä¼˜åŒ–ï¼šå°è¯•ä»çˆ¶çº§æˆ–ç›´æ¥å…„å¼ŸèŠ‚ç‚¹å¯»æ‰¾æ‘˜è¦ï¼Œæé«˜ Web æŠ“å–çš„é€šç”¨æ€§
        parent = item.find_parent() if item.find_parent() else soup
        summary_tag = parent.select_one('.summary, .search-summary, .search-snippet, .search-content, .content, p')
        summary_raw = summary_tag.get_text(strip=True) if summary_tag else title
        summary = clean_html_summary(summary_raw, max_len=400)
        
        if re.search(r'åŸºé‡‘|å®ç›˜|è§‚ç‚¹|ç»éªŒ|æ¨è|ç­–ç•¥|æŠ•èµ„|è‚¡ç¥¨|å®è§‚|é‡‘è', title + summary, re.IGNORECASE):
            filtered_items.append({
                'title': title,
                'link': link if link else 'N/A',
                'pubDate': 'N/A',
                'summary': summary,
                'source': source_name
            })
    return filtered_items

# --- æ ¸å¿ƒæ–°é—»åˆ†æå‡½æ•° ---
def analyze_news(news_items: List[Dict]) -> Dict:
    analysis = {
        'investment_clues': [],
        'experience_lessons': [],
        'industry_trends': [],
        'detailed_analyses': []
    }
    seen_clues = set()
    seen_lessons = set()
    seen_trends = set()

    for item in news_items:
        text = item['title'] + ' ' + item['summary']
        
        # 1. æŠ•èµ„çº¿ç´¢
        for pattern, info in CLUES_MAP.items():
            if re.search(pattern, text, re.IGNORECASE) and info['desc'] not in seen_clues:
                analysis['investment_clues'].append({
                    'focus': info['desc'],
                    'title': item['title'],
                    'link': item['link'],
                    'weight': info['weight']
                })
                seen_clues.add(info['desc'])
        
        # 2. ç»éªŒæ•™è®­
        for pattern, info in LESSONS_MAP.items():
            if re.search(pattern, text, re.IGNORECASE) and info['desc'] not in seen_lessons:
                analysis['experience_lessons'].append({
                    'lesson': info['desc'],
                    'title': item['title'],
                    'link': item['link'],
                    'weight': info['weight']
                })
                seen_lessons.add(info['desc'])
        
        # 3. è¡Œä¸šè¶‹åŠ¿
        for pattern, info in TRENDS_MAP.items():
            if re.search(pattern, text, re.IGNORECASE) and info['desc'] not in seen_trends:
                analysis['industry_trends'].append({
                    'trend': info['desc'],
                    'title': item['title'],
                    'link': item['link'],
                    'weight': info['weight']
                })
                seen_trends.add(info['desc'])
        
        detailed = detailed_analyze_news(item)
        analysis['detailed_analyses'].append(detailed)
        
    # æŒ‰æƒé‡æ’åº
    analysis['investment_clues'].sort(key=lambda x: x['weight'], reverse=True)
    analysis['experience_lessons'].sort(key=lambda x: x['weight'], reverse=True)
    analysis['industry_trends'].sort(key=lambda x: x['weight'], reverse=True)
    
    return analysis

# --- è¯äº‘å­—ä½“è·¯å¾„æŸ¥æ‰¾è¾…åŠ©å‡½æ•° ---
def get_chinese_font_path() -> str | None:
    """å°è¯•æŸ¥æ‰¾å¸¸è§çš„ç³»ç»Ÿ/é»˜è®¤ä¸­æ–‡å­—ä½“è·¯å¾„"""
    # å¸¸è§ Windows è·¯å¾„
    if os.name == 'nt' and os.path.exists('C:/Windows/Fonts/simhei.ttf'):
        return 'C:/Windows/Fonts/simhei.ttf'
    
    # å¸¸è§ Linux/macOS/CI è·¯å¾„
    for path in [
        'SimHei.ttf', # è„šæœ¬åŒç›®å½•
        '/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc',
        '/System/Library/Fonts/Supplemental/Songti.ttc',
        '/Library/Fonts/Arial Unicode.ttf',
        '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc'
    ]:
        if os.path.exists(path):
            return path
            
    return None

# --- æ–°å¢ï¼šç”Ÿæˆè¯äº‘ (å·²ä¿®å¤ OSEerror) ---
def generate_wordcloud(keywords: Dict[str, int], output_file: str):
    if not keywords:
        logger.info("No keywords for wordcloud generation.")
        return
    
    font_path = get_chinese_font_path()
    
    if font_path is None:
        logger.error("WordCloud generation failed (OSError: cannot open resource): No valid Chinese font found. Skipping wordcloud generation. Please install SimHei.ttf or a CJK font in the execution environment.")
        return # ä¿®å¤ï¼šæ‰¾ä¸åˆ°å­—ä½“æ—¶ç›´æ¥è¿”å›ï¼Œé¿å… OSError

    try:
        wordcloud = WordCloud(
            font_path=font_path,
            width=800, height=400, background_color='white', max_words=50
        ).generate_from_frequencies(keywords)
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.savefig(f'{output_file}_wordcloud.png')
        plt.close()
        logger.info(f"Generated wordcloud: {output_file}_wordcloud.png")
    except Exception as e:
        logger.error(f"WordCloud generation failed unexpectedly: {e}")

# --- é•¿æœŸè¶‹åŠ¿åˆ†æå‡½æ•° ---
def generate_trend_analysis(db_manager: DatabaseManager) -> str:
    recent_topics = db_manager.get_topics_by_time_range(days=7)
    previous_topics = db_manager.get_topics_by_time_range(days=14)
    recent_keywords = db_manager.get_dynamic_keywords_by_time_range(days=7)
    
    # è®¡ç®—å‰ 7 å¤©çš„ç‹¬æœ‰è®¡æ•° (P0)
    p2_only_topics = {
        topic: count - recent_topics.get(topic, 0)
        for topic, count in previous_topics.items()
        if count > recent_topics.get(topic, 0) # æ’é™¤åœ¨ P1 ä¸­è®¡æ•°æ›´é«˜çš„ä¸»é¢˜
    }
    
    trend_report = "\n### ğŸ“ˆ ä¸»é¢˜ä¸å…³é”®è¯è¶‹åŠ¿åˆ†æ (è¿‘ 7 å¤© vs å‰ 7 å¤©)\n"
    trend_report += "å¯¹æ¯”æ˜¾ç¤ºä¸»é¢˜å’ŒåŠ¨æ€å…³é”®è¯çš„å…³æ³¨åº¦å˜åŒ–ï¼Œå˜åŒ–ç‡ > 50% çš„ä¸»é¢˜é«˜äº®ã€‚\n\n"
    
    # ä¸»é¢˜è¶‹åŠ¿
    trend_report += "#### ä¸»é¢˜çƒ­åº¦å˜åŒ–\n"
    trend_report += "| ä¸»é¢˜ | è¿‘ 7 å¤© (P1) | å‰ 7 å¤© (P0) | å˜åŒ–ç‡ | è¶‹åŠ¿ |\n"
    trend_report += "| :--- | :---: | :---: | :---: | :---: |\n"
    
    all_topics = set(recent_topics.keys()) | set(p2_only_topics.keys())
    sorted_topics = sorted(list(all_topics), key=lambda x: recent_topics.get(x, 0), reverse=True)

    for topic in sorted_topics:
        count_p1 = recent_topics.get(topic, 0)
        count_p0 = p2_only_topics.get(topic, 0) # è¿™é‡Œä½¿ç”¨è®¡ç®—å‡ºçš„ P0 è®¡æ•°
        
        if count_p1 == 0 and count_p0 == 0:
            continue

        if count_p0 > 0:
            change_rate = (count_p1 - count_p0) / count_p0
            trend_icon = "â¬†ï¸" if change_rate > 0.1 else ("â¬‡ï¸" if change_rate < -0.1 else "â†”ï¸")
            trend_str = f"{change_rate:.0%}"
        elif count_p1 > 0:
            # P0 ä¸º 0ï¼ŒP1 > 0ï¼Œè§†ä¸ºæ–°çƒ­ç‚¹
            change_rate = float('inf')
            trend_icon = "ğŸ”¥"
            trend_str = "NEW"
        else:
            change_rate = 0
            trend_icon = "â–"
            trend_str = "0%"

        if abs(change_rate) > 0.5 and change_rate != float('inf'):
            trend_str = f"**{trend_str}**"
        
        trend_report += f"| {topic} | {count_p1} | {count_p0} | {trend_str} | {trend_icon} |\n"
    
    # åŠ¨æ€å…³é”®è¯è¶‹åŠ¿
    trend_report += "\n#### åŠ¨æ€å…³é”®è¯ Top 5\n"
    top_keywords = sorted(recent_keywords.items(), key=lambda x: x[1], reverse=True)[:5]
    if top_keywords:
        trend_report += "| å…³é”®è¯ | å‡ºç°æ¬¡æ•° |\n"
        trend_report += "| :--- | :---: |\n"
        for keyword, count in top_keywords:
            trend_report += f"| {keyword} | {count} |\n"
    else:
        trend_report += "æš‚æ— åŠ¨æ€å…³é”®è¯ã€‚\n"
    
    return trend_report

# --- ç”Ÿæˆç»Ÿè®¡å›¾è¡¨ ---
def generate_stats_chart(analysis: Dict, output_file: str):
    clue_count = len(analysis['investment_clues'])
    lesson_count = len(analysis['experience_lessons'])
    trend_count = len(analysis['industry_trends'])
    
    categories = ['Investment Clues', 'Experience Lessons', 'Industry Trends']
    counts = [clue_count, lesson_count, trend_count]
    
    plt.figure(figsize=(8, 5))
    bars = plt.bar(categories, counts, color=['#1f77b4', '#ff7f0e', '#2ca02c'])
    plt.title('News Analysis Categories Count (Current Run)')
    plt.ylabel('Count')
    for bar in bars:
        yval = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.1, int(yval), ha='center', va='bottom')
    plt.savefig(f'{output_file}_stats.png')
    plt.close()
    logger.info(f"Generated stats chart: {output_file}_stats.png")

# --- ç”Ÿæˆåˆ†ææŠ¥å‘Š ---
def generate_analysis_report(analysis: Dict, total_count: int, trend_report: str, output_file: str) -> str:
    md_report = "\n---\n"
    md_report += "# ğŸ“° åŸºé‡‘æŠ•èµ„ç­–ç•¥åˆ†ææŠ¥å‘Š\n\n"
    md_report += f"æœ¬æŠ¥å‘Šæ ¹æ®ä» {total_count} æ¡æ–°é—»ä¸­æå–çš„é«˜ä»·å€¼ä¿¡æ¯ç”Ÿæˆï¼Œæ—¨åœ¨ä¸ºæ‚¨æä¾› **ä¹°å…¥æŒ‡å¼•ã€é£é™©è§„é¿å’Œè¡Œä¸šæ´å¯Ÿ**ã€‚\n\n"

    md_report += "## ğŸ“Š ç»Ÿè®¡æ¦‚è¿°\n"
    md_report += f"- æœ¬æ¬¡æŠ“å–æŠ•èµ„çº¿ç´¢æ•°é‡: {len(analysis['investment_clues'])}\n"
    md_report += f"- æœ¬æ¬¡æŠ“å–ç»éªŒæ•™è®­æ•°é‡: {len(analysis['experience_lessons'])}\n"
    md_report += f"- æœ¬æ¬¡æŠ“å–è¡Œä¸šè¶‹åŠ¿æ•°é‡: {len(analysis['industry_trends'])}\n"
    md_report += f"- æ€»æ–°é—»æ¡ç›®: {total_count}\n"
    # ä½¿ç”¨ä¼ å…¥çš„ output_file å‚æ•°
    md_report += f"- ç”Ÿæˆå›¾è¡¨: {output_file}_stats.png, {output_file}_wordcloud.png\n\n" 
    
    md_report += "## é•¿æœŸè¶‹åŠ¿åˆ†æ\n"
    md_report += trend_report

    md_report += "\n## ğŸ’° æŠ•èµ„çº¿ç´¢ä¸å¸‚åœºç„¦ç‚¹ (ä¹°å…¥æŒ‡å¼•)\n"
    if analysis['investment_clues']:
        md_report += "| ç„¦ç‚¹æ ‡çš„/ç­–ç•¥ | åŸå§‹æ ‡é¢˜ (ç‚¹å‡»æŸ¥çœ‹) | æƒé‡ |\n"
        md_report += "| :--- | :--- | :---: |\n"
        for clue in analysis['investment_clues']:
            # ä¿®å¤ Markdown é“¾æ¥è¯­æ³•
            md_report += f"| **{clue['focus']}** | [{clue['title']}](<{clue['link']}>) | {clue['weight']:.1f} |\n"
    else:
        md_report += "æš‚æ— æ˜ç¡®çš„æŠ•èµ„çº¿ç´¢æˆ–æœºæ„è§‚ç‚¹è¢«è¯†åˆ«ã€‚\n"
        
    md_report += "\n## âš ï¸ æŠ•èµ„ç»éªŒä¸é£é™©è§„é¿ (é¿å…è¸©å‘)\n"
    if analysis['experience_lessons']:
        md_report += "| æ•™è®­/ç»éªŒ | åŸå§‹æ ‡é¢˜ (ç‚¹å‡»æŸ¥çœ‹) | æƒé‡ |\n"
        md_report += "| :--- | :--- | :---: |\n"
        for lesson in analysis['experience_lessons']:
            # ä¿®å¤ Markdown é“¾æ¥è¯­æ³•
            md_report += f"| **{lesson['lesson']}** | [{lesson['title']}](<{lesson['link']}>) | {lesson['weight']:.1f} |\n"
    else:
        md_report += "æš‚æ— æ˜ç¡®çš„ç»éªŒæ•™è®­æˆ–é£é™©æç¤ºè¢«è¯†åˆ«ã€‚\n"

    md_report += "\n## âœ¨ è¡Œä¸šç»“æ„ä¸æœªæ¥è¶‹åŠ¿ (é•¿æœŸæ´å¯Ÿ)\n"
    if analysis['industry_trends']:
        md_report += "| è¡Œä¸šè¶‹åŠ¿ | åŸå§‹æ ‡é¢˜ (ç‚¹å‡»æŸ¥çœ‹) | æƒé‡ |\n"
        md_report += "| :--- | :--- | :---: |\n"
        for trend in analysis['industry_trends']:
            # ä¿®å¤ Markdown é“¾æ¥è¯­æ³•
            md_report += f"| **{trend['trend']}** | [{trend['title']}](<{trend['link']}>) | {trend['weight']:.1f} |\n"
    else:
        md_report += "æš‚æ— æ˜ç¡®çš„è¡Œä¸šè¶‹åŠ¿æˆ–ç»“æ„å˜åŒ–è¢«è¯†åˆ«ã€‚\n"

    md_report += "\n## ğŸ” æ‰€æœ‰æ–°é—»è¯¦ç»†åˆ†æä¸æ½œåœ¨å½±å“\n"
    if analysis['detailed_analyses']:
        md_report += "| æ–°é—»æ ‡é¢˜ | å…³é”®ä¸»é¢˜ | æƒ…æ„Ÿåˆ†æ (å¾—åˆ†) | åŠ¨æ€å…³é”®è¯ | æ½œåœ¨å½±å“ |\n"
        md_report += "| :--- | :--- | :--- | :--- | :--- |\n"
        for det in analysis['detailed_analyses']:
            topics_str = '; '.join(det['key_topics']) if det['key_topics'] else 'æ— ç‰¹å®šä¸»é¢˜'
            keywords_str = ', '.join(det['dynamic_keywords']) if det['dynamic_keywords'] else 'æ— '
            md_report += f"| {det['title']} | {topics_str} | {det['sentiment']} ({det['sentiment_score']:.1f}) | {keywords_str} | **{det['potential_impact']}** |\n"
    else:
        md_report += "æš‚æ— è¯¦ç»†åˆ†æã€‚\n"

    return md_report

# --- æ–°å¢ï¼šåŠ è½½å¤–éƒ¨é…ç½®å‡½æ•° ---
def load_sources_from_json(file_path: str = 'sources.json') -> List[Dict]:
    """ä»æŒ‡å®šçš„ JSON æ–‡ä»¶ä¸­åŠ è½½æ•°æ®æºé…ç½®ã€‚"""
    if not os.path.exists(file_path):
        logger.error(f"Configuration file not found: {file_path}. Using empty list.")
        return []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            sources = json.load(f)
            logger.info(f"Loaded {len(sources)} sources from {file_path}.")
            return sources
    except json.JSONDecodeError as e:
        logger.error(f"Error decoding JSON from {file_path}: {e}")
        return []
    except Exception as e:
        logger.error(f"An unexpected error occurred while loading sources: {e}")
        return []

# âš ï¸ æ³¨æ„ï¼šæ—§çš„ç¡¬ç¼–ç  sources åˆ—è¡¨å·²ä»æ­¤å¤„ç§»é™¤ï¼Œä»¥å®ç°é…ç½®å¤–ç½®åŒ–ã€‚

def generate_markdown(news_items: List[Dict], analysis_report: str, timestamp_str: str, configured_sources: List[Dict]) -> str:
    md_content = f"# åŸºé‡‘æ–°é—»èšåˆ ({timestamp_str})\n\n"
    # ä»ä¼ å…¥çš„é…ç½®åˆ—è¡¨ä¸­æå–åç§°
    source_names = "ã€".join([s['name'].split('(')[0].strip() for s in configured_sources])
    md_content += f"æ¥æºï¼š{source_names}ï¼ˆå…³é”®è¯ï¼šåŸºé‡‘/å®ç›˜/è§‚ç‚¹/ç»éªŒ/æ¨è/ç­–ç•¥/æŠ•èµ„/å®è§‚/é‡‘èï¼‰ã€‚æ€»è®¡ {len(news_items)} æ¡ã€‚\n"
    md_content += analysis_report
    md_content += "\n---\n# åŸå§‹æ–°é—»åˆ—è¡¨\n\n"
    for i, item in enumerate(news_items, 1):
        md_content += f"## {i}. {item['title']} ({item['source']})\n"
        md_content += f"- **é“¾æ¥**: [{item['link']}]({item['link']})\n"
        md_content += f"- **æ—¶é—´**: {item['pubDate']}\n"
        md_content += f"- **æ‘˜è¦**: {item['summary']}\n\n"
    return md_content

def main():
    db_manager = DatabaseManager()
    tz = pytz.timezone('Asia/Shanghai')
    now = datetime.now(tz)
    timestamp_str = now.strftime('%Y-%m-%d %H:%M:%S')
    
    output_file_base = f'fund_news_{now.strftime("%Y%m%d")}' 
    
    # ğŸ’¥ å…³é”®æ”¹åŠ¨ï¼šåŠ¨æ€åŠ è½½ sources åˆ—è¡¨
    sources = load_sources_from_json() 
    
    all_news = []
    logger.info(f"[{timestamp_str}] å¼€å§‹æŠ“å–åŸºé‡‘æ–°é—» (å…± {len(sources)} ä¸ªæ¥æº)...")
    
    existing_links = db_manager.get_existing_links()
    
    for source in sources:
        logger.info(f"å¤„ç†æ¥æº: {source['name']} ({source['url']})")
        try:
            if source['type'] == 'rss':
                items = fetch_rss_feed(source['url'], source['name'], limit=20)
            else:
                # ç¡®ä¿ Web æºæœ‰ selector
                selector = source.get('selector')
                if not selector:
                    logger.error(f"Web source {source['name']} is missing a 'selector' key in sources.json. Skipping.")
                    continue
                items = fetch_web_page(source['url'], source['name'], selector, limit=15)
            all_news.extend(items)
        except Exception as e:
            logger.error(f"Failed to process source {source['name']}: {e}")
            
    unique_news = []
    batch_seen = set()
    for news in all_news:
        link = news.get('link', 'N/A')
        
        # è·¨æ¬¡å»é‡
        if link != 'N/A' and link in existing_links:
            continue
            
        # æ‰¹æ¬¡å†…å»é‡ (æ ‡é¢˜+æ¥æº)
        if (news['title'], news['source']) not in batch_seen:
            unique_news.append(news)
            batch_seen.add((news['title'], news['source']))
            if link != 'N/A':
                existing_links.add(link) # æå‰åŠ å…¥ï¼Œé¿å…æœ¬æ‰¹æ¬¡å†…é‡å¤

    # æ’åº
    unique_news.sort(key=lambda x: datetime.strptime(x['pubDate'], '%Y-%m-%d %H:%M:%S') if x['pubDate'] != 'N/A' else datetime(1900, 1, 1), reverse=True)
    
    analysis_results = analyze_news(unique_news)
    
    # æ‰¹é‡å­˜å‚¨æ–°é—»
    for item, detailed_analysis in zip(unique_news, analysis_results['detailed_analyses']):
        db_manager.store_news_and_analysis(item, detailed_analysis)
    
    # ç”Ÿæˆè¯äº‘ (å·²ä¿®å¤ OSEerror)
    recent_keywords = db_manager.get_dynamic_keywords_by_time_range(days=7)
    generate_wordcloud(recent_keywords, output_file_base)
    
    # ç”ŸæˆæŠ¥å‘Š
    trend_report_md = generate_trend_analysis(db_manager)
    analysis_report_md = generate_analysis_report(analysis_results, len(unique_news), trend_report_md, output_file_base) 
    generate_stats_chart(analysis_results, output_file_base)
    
    # ä¿®å¤ï¼šå°† sources åˆ—è¡¨ä½œä¸ºå‚æ•°ä¼ å…¥ generate_markdown
    md_content = generate_markdown(unique_news, analysis_report_md, timestamp_str, sources)
    
    with open(f'{output_file_base}.md', 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    logger.info(f"æ”¶é›†åˆ° {len(unique_news)} æ¡ç‹¬ç‰¹åŸºé‡‘æ–°é—»ã€‚åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆå¹¶ä¿å­˜è‡³ {output_file_base}.md")
    logger.info("\n--- åˆ†ææŠ¥å‘Šæ‘˜è¦ ---")
    logger.info(analysis_report_md.split('## ğŸ’°')[0])
    
    db_manager.close()

if __name__ == "__main__":
    main()
