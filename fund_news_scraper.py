import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict
import re
import xml.etree.ElementTree as ET
from dateutil import parser
import pytz

# --- ã€æ ¸å¿ƒé…ç½®ã€‘åˆ†æè§„åˆ™åº“ï¼Œå¯æ ¹æ®æ–°çš„æ–°é—»ä¸»é¢˜æ‰©å±• ---
# 1. æŠ•èµ„çº¿ç´¢ (äººç‰©/æœºæ„ -> æ ‡çš„/ç­–ç•¥)
CLUES_MAP = {
    # è§„åˆ™: æ­£åˆ™è¡¨è¾¾å¼ç”¨äºåŒ¹é…æ–°é—»å†…å®¹ï¼ˆæ ‡é¢˜+æ‘˜è¦ï¼‰
    r'æè““|åŠå¤|ä¸­è¯500|IC': 'åŠå¤æè““/ä¸­è¯500/ç§‘æŠ€æˆé•¿ç­–ç•¥',
    r'å›½é‡‘è¯åˆ¸|å››ä¸­å…¨ä¼š|ç­–ç•¥æœˆæŠ¥': 'å›½é‡‘è¯åˆ¸/å››ä¸­å…¨ä¼šä¸»é¢˜ç­–ç•¥',
    r'åå®‰è¯åˆ¸|æˆé•¿äº§ä¸š|AI|å†›å·¥': 'åå®‰è¯åˆ¸/AI/å†›å·¥/æ–°æˆé•¿é…ç½®',
    r'å¼€æºè¯åˆ¸|é‡‘è‚¡ç­–ç•¥|ç§‘æŠ€|æ¸¯è‚¡': 'å¼€æºè¯åˆ¸/AI+è‡ªä¸»å¯æ§ç§‘æŠ€ä¸»çº¿',
    r'ETF|è‚¡ç¥¨ETF|ç™¾äº¿ä¿±ä¹éƒ¨|å¸é‡‘': 'èµ„é‡‘æµå‘/è‚¡ç¥¨ETF/å¸é‡‘èµ›é“',
    r'è´µé‡‘å±|é»„é‡‘|é¿é™©': 'èµ„äº§å¯¹å†²/é¿é™©é…ç½®',
    r'å‡è¡¡é…ç½®|å…‰ä¼|åŒ–å·¥|å†œä¸š|æœ‰è‰²|é“¶è¡Œ': 'å‡è¡¡ç­–ç•¥/ä½ä¼°å€¼è½®åŠ¨é…ç½®',
}

# 2. ç»éªŒæ•™è®­ (è¡Œä¸º/ç»“æœ -> é£é™©/æ•™è®­)
LESSONS_MAP = {
    r'è·‘è¾“å¤§ç›˜|æœªèƒ½æ»¡ä»“|çº¢åˆ©æ¿å—': 'ç»éªŒæ•™è®­ï¼šæ–°åŸºé‡‘å»ºä»“ç­–ç•¥ä¸å¸‚åœºé”™é…é£é™©',
    r'åŸºé‡‘ç»ç†|æ¶‰èµŒ|å…èŒ|å†…æ§': 'é£é™©æç¤ºï¼šåŸºé‡‘å…¬å¸å†…æ§å’Œç»ç†é“å¾·é£é™©',
    r'æœºæ„å¤§ä¸¾å¢æŒ|ä¸»åŠ¨æƒç›ŠåŸºé‡‘': 'æœºæ„è¡Œä¸ºï¼šä¸»åŠ¨æƒç›ŠåŸºé‡‘ä»æ˜¯é…ç½®é‡ç‚¹',
}

# 3. è¡Œä¸šè¶‹åŠ¿ (ç»“æ„å˜åŒ– -> è¡Œä¸šæ´å¯Ÿ)
TRENDS_MAP = {
    r'AI|æŠ•ç ”|å·¥ä¸šåŒ–|èš‚èšè´¢å¯Œ': 'è¡Œä¸šè¶‹åŠ¿ï¼šæŠ•ç ”å·¥ä¸šåŒ–å’ŒAIèµ‹èƒ½',
    r'è´¹ç‡|ä¸‹è°ƒ|æ‰˜ç®¡è´¹|ä½™é¢å®': 'è¡Œä¸šè¶‹åŠ¿ï¼šå…³æ³¨è´¹ç‡æˆæœ¬çš„é•¿æœŸä¸‹è¡Œ',
    r'ç§å‹Ÿè‚¡æƒ|å­å…¬å¸|å¹¿å‘åŸºé‡‘': 'è¡Œä¸šè¶‹åŠ¿ï¼šå¤´éƒ¨å…¬å‹Ÿçš„ä¸šåŠ¡å¤šå…ƒåŒ–',
    r'é‡åŒ–åŸºé‡‘ç»ç†|ä¸»åŠ¨åŸºé‡‘|ä¸€æ‹–å¤š': 'è¡Œä¸šè¶‹åŠ¿ï¼šé‡åŒ–ä¸ä¸»åŠ¨æŠ•èµ„è¾¹ç•Œæ¨¡ç³Š',
}
# -----------------------------------------------------------------


# --- è¾…åŠ©å‡½æ•°ï¼šæ—¶é—´è§£æå’Œæ ¼å¼åŒ– ---
def parse_and_format_time(pub_date: str) -> str:
    """è§£ææ—¶é—´å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´å¹¶æ ¼å¼åŒ–ã€‚"""
    if pub_date == 'N/A' or not pub_date:
        return 'N/A'
    try:
        dt_utc = parser.parse(pub_date).replace(tzinfo=pytz.utc)
        dt_local = dt_utc.astimezone(pytz.timezone('Asia/Shanghai'))
        return dt_local.strftime('%Y-%m-%d %H:%M:%S')
    except Exception:
        return pub_date

# --- è¾…åŠ©å‡½æ•°ï¼šHTMLæ¸…ç†å’Œæ‘˜è¦å¤„ç† ---
def clean_html_summary(summary: str, max_len: int = 400) -> str:
    """æ¸…ç†æ‘˜è¦ä¸­çš„HTMLæ ‡ç­¾å’Œå¤šä½™ç©ºæ ¼ï¼Œå¹¶è¿›è¡Œæˆªæ–­ã€‚"""
    if not summary:
        return 'æ— æ‘˜è¦'
    
    clean_soup = BeautifulSoup(summary, 'html.parser')
    clean_text = clean_soup.get_text(strip=True)
    clean_text = re.sub(r'\s+', ' ', clean_text)
    
    if len(clean_text) > max_len:
        return clean_text[:max_len] + '...'
    return clean_text

# --- æ ¸å¿ƒæŠ“å–å‡½æ•°ï¼šRSS ---
def fetch_rss_feed(url: str, source_name: str, limit: int = 15) -> List[Dict]:
    """è·å–å¹¶è§£æRSS feedï¼Œè¿‡æ»¤åŒ…å«'åŸºé‡‘'ã€'å®ç›˜'ã€'è§‚ç‚¹'ç­‰å…³é”®è¯çš„æ¡ç›®ã€‚"""
    filtered_items = []
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, timeout=10, headers=headers)
        response.raise_for_status()
        
        try:
            root = ET.fromstring(response.content)
        except ET.ParseError:
            print(f"[{source_name}] Error parsing XML. Trying content decoding...")
            root = ET.fromstring(response.text.encode('utf-8'))

        items = root.findall('.//item')
        
        for item in items[:limit]:
            title = item.find('title').text if item.find('title') is not None and item.find('title').text else ''
            link = item.find('link').text if item.find('link') is not None and item.find('link').text else 'N/A'
            pub_date_raw = item.find('pubDate').text if item.find('pubDate') is not None and item.find('pubDate').text else 'N/A'
            summary_raw = item.find('description').text if item.find('description') is not None and item.find('description').text else ''
            
            summary = clean_html_summary(summary_raw, max_len=400)
            pub_date = parse_and_format_time(pub_date_raw)
            
            if re.search(r'åŸºé‡‘|å®ç›˜|è§‚ç‚¹|ç»éªŒ|æ¨è|ç­–ç•¥', title + summary, re.IGNORECASE):
                filtered_items.append({
                    'title': title.strip(),
                    'link': link.strip(),
                    'pubDate': pub_date,
                    'summary': summary,
                    'source': source_name
                })
        return filtered_items
        
    except requests.exceptions.Timeout:
        print(f"[{source_name}] Error fetching RSS {url}: Request timed out.")
    except requests.exceptions.RequestException as e:
        print(f"[{source_name}] Error fetching RSS {url}: Network or HTTP error: {e}")
    except Exception as e:
        print(f"[{source_name}] Error fetching RSS {url}: An unexpected error occurred: {e}")
    return []

# --- æ ¸å¿ƒæŠ“å–å‡½æ•°ï¼šWeb (é›ªçƒ) ---
def fetch_web_page(url: str, source_name: str, selector: str, limit: int = 15) -> List[Dict]:
    """æŠ“å–ç½‘é¡µï¼ˆä¸“ç”¨äºé›ªçƒï¼‰ï¼Œè¿‡æ»¤'åŸºé‡‘'ã€'å®ç›˜'ç­‰å…³é”®è¯ã€‚"""
    filtered_items = []
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://xueqiu.com/'
        }
        response = requests.get(url, timeout=10, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        items = soup.select(selector)
        
        for item in items[:limit]:
            title_tag = item
            if not title_tag:
                continue
            
            title = title_tag.get_text(strip=True)
            link = title_tag.get('href', '')
            
            if link and not link.startswith('http'):
                link = f"https://xueqiu.com{link}"
            
            parent = item.parent.parent
            summary_tag = parent.select_one('.search-summary, .search-snippet, .search-content')
            
            summary_raw = summary_tag.get_text(strip=True) if summary_tag else title
            summary = clean_html_summary(summary_raw, max_len=400)
            
            if re.search(r'åŸºé‡‘|å®ç›˜|è§‚ç‚¹|ç»éªŒ|æ¨è|ç­–ç•¥', title + summary, re.IGNORECASE):
                filtered_items.append({
                    'title': title,
                    'link': link if link else 'N/A',
                    'pubDate': 'N/A', 
                    'summary': summary,
                    'source': source_name
                })
        return filtered_items
        
    except requests.exceptions.Timeout:
        print(f"[{source_name}] Error fetching web {url}: Request timed out.")
    except requests.exceptions.RequestException as e:
        print(f"[{source_name}] Error fetching web {url}: Network or HTTP error: {e}")
    except Exception as e:
        print(f"[{source_name}] Error fetching web {url}: An unexpected error occurred: {e}")
    return []


# --- ã€æ ¸å¿ƒã€‘æ–°é—»åˆ†æå‡½æ•°ï¼šåŸºäºè§„åˆ™åŒ¹é… ---
def analyze_news(news_items: List[Dict]) -> Dict:
    """
    åŸºäºå…³é”®è¯å’Œè§„åˆ™ï¼Œä»æ–°é—»åˆ—è¡¨ä¸­æå–æŠ•èµ„çº¿ç´¢å’Œç»éªŒæ•™è®­ã€‚
    ä»£ç çš„æ ¸å¿ƒæ˜¯éå†æ¯æ¡æ–°é—»ï¼Œå°è¯•åŒ¹é…é¢„å®šä¹‰çš„æ­£åˆ™æ¨¡å¼ï¼ˆCLUES_MAP, LESSONS_MAP, TRENDS_MAPï¼‰ã€‚
    """
    analysis = {
        'investment_clues': [],
        'experience_lessons': [],
        'industry_trends': []
    }

    # è®°å½•å·²åŒ¹é…åˆ°çš„åˆ†æç‚¹ï¼Œé¿å…é‡å¤
    seen_clues = set()
    seen_lessons = set()
    seen_trends = set()

    for item in news_items:
        # å°†æ ‡é¢˜å’Œæ‘˜è¦åˆå¹¶æˆä¸€ä¸ªé•¿å­—ç¬¦ä¸²è¿›è¡ŒåŒ¹é…
        text = item['title'] + item['summary']
        
        # 1. åŒ¹é…æŠ•èµ„çº¿ç´¢
        for pattern, focus in CLUES_MAP.items():
            if re.search(pattern, text, re.IGNORECASE) and focus not in seen_clues:
                analysis['investment_clues'].append({
                    'focus': focus,
                    'title': item['title'],
                    'link': item['link'],
                })
                seen_clues.add(focus)
                
        # 2. åŒ¹é…ç»éªŒæ•™è®­
        for pattern, lesson in LESSONS_MAP.items():
            if re.search(pattern, text, re.IGNORECASE) and lesson not in seen_lessons:
                analysis['experience_lessons'].append({
                    'lesson': lesson,
                    'title': item['title'],
                    'link': item['link'],
                })
                seen_lessons.add(lesson)

        # 3. åŒ¹é…è¡Œä¸šè¶‹åŠ¿
        for pattern, trend in TRENDS_MAP.items():
            if re.search(pattern, text, re.IGNORECASE) and trend not in seen_trends:
                analysis['industry_trends'].append({
                    'trend': trend,
                    'title': item['title'],
                    'link': item['link'],
                })
                seen_trends.add(trend)
                
    return analysis

# --- ç”Ÿæˆåˆ†ææŠ¥å‘Š ---
def generate_analysis_report(analysis: Dict, total_count: int) -> str:
    """æ ¹æ®åˆ†æç»“æœç”Ÿæˆç»“æ„åŒ– Markdown æŠ¥å‘Šã€‚"""
    md_report = "\n---\n"
    md_report += "# ğŸ“° åŸºé‡‘æŠ•èµ„ç­–ç•¥åˆ†ææŠ¥å‘Š\n\n"
    md_report += f"æœ¬æŠ¥å‘Šæ ¹æ®ä» {total_count} æ¡æ–°é—»ä¸­æå–çš„é«˜ä»·å€¼ä¿¡æ¯ç”Ÿæˆï¼Œæ—¨åœ¨ä¸ºæ‚¨æä¾› **ä¹°å…¥æŒ‡å¼•ã€é£é™©è§„é¿å’Œè¡Œä¸šæ´å¯Ÿ**ã€‚\n\n"

    # 1. æŠ•èµ„çº¿ç´¢
    md_report += "## ğŸ’° æŠ•èµ„çº¿ç´¢ä¸å¸‚åœºç„¦ç‚¹ (ä¹°å…¥æŒ‡å¼•)\n"
    if analysis['investment_clues']:
        md_report += "| ç„¦ç‚¹æ ‡çš„/ç­–ç•¥ | åŸå§‹æ ‡é¢˜ (ç‚¹å‡»æŸ¥çœ‹) |\n"
        md_report += "| :--- | :--- |\n"
        
        for clue in analysis['investment_clues']:
            md_report += f"| **{clue['focus']}** | [{clue['title']}](<{clue['link']}>) |\n"
    else:
        md_report += "æš‚æ— æ˜ç¡®çš„æŠ•èµ„çº¿ç´¢æˆ–æœºæ„è§‚ç‚¹è¢«è¯†åˆ«ã€‚\n"
        
    # 2. ç»éªŒä¸æ•™è®­
    md_report += "\n## âš ï¸ æŠ•èµ„ç»éªŒä¸é£é™©è§„é¿ (é¿å…è¸©å‘)\n"
    if analysis['experience_lessons']:
        md_report += "| æ•™è®­/ç»éªŒ | åŸå§‹æ ‡é¢˜ (ç‚¹å‡»æŸ¥çœ‹) |\n"
        md_report += "| :--- | :--- |\n"
        
        for lesson in analysis['experience_lessons']:
            md_report += f"| **{lesson['lesson']}** | [{lesson['title']}](<{lesson['link']}>) |\n"
    else:
        md_report += "æš‚æ— æ˜ç¡®çš„ç»éªŒæ•™è®­æˆ–é£é™©æç¤ºè¢«è¯†åˆ«ã€‚\n"

    # 3. è¡Œä¸šç»“æ„ä¸è¶‹åŠ¿
    md_report += "\n## âœ¨ è¡Œä¸šç»“æ„ä¸æœªæ¥è¶‹åŠ¿ (é•¿æœŸæ´å¯Ÿ)\n"
    if analysis['industry_trends']:
        md_report += "| è¡Œä¸šè¶‹åŠ¿ | åŸå§‹æ ‡é¢˜ (ç‚¹å‡»æŸ¥çœ‹) |\n"
        md_report += "| :--- | :--- |\n"
        
        for trend in analysis['industry_trends']:
            md_report += f"| **{trend['trend']}** | [{trend['title']}](<{trend['link']}>) |\n"
    else:
        md_report += "æš‚æ— æ˜ç¡®çš„è¡Œä¸šè¶‹åŠ¿æˆ–ç»“æ„å˜åŒ–è¢«è¯†åˆ«ã€‚\n"

    return md_report


# --- æ•°æ®æºé…ç½®å¤–éƒ¨åŒ– (ä¿æŒä¸å˜) ---
proxy_base = 'https://rsshub.rss.zgdnz.cc'
sources = [
    {'url': f'{proxy_base}/cls/telegraph/fund', 'name': 'è´¢è”ç¤¾-åŸºé‡‘ç”µæŠ¥', 'type': 'rss'},
    {'url': f'{proxy_base}/eastmoney/report/strategyreport', 'name': 'ä¸œæ–¹è´¢å¯Œ-ç­–ç•¥æŠ¥å‘Š', 'type': 'rss'},
    {'url': f'{proxy_base}/gelonghui/home/fund', 'name': 'æ ¼éš†æ±‡-åŸºé‡‘', 'type': 'rss'},
    {'url': f'{proxy_base}/stcn/article/list/fund', 'name': 'è¯åˆ¸æ—¶æŠ¥-åŸºé‡‘åˆ—è¡¨', 'type': 'rss'},
    {'url': f'{proxy_base}/21caijing/channel/%E8%AF%81%E5%88%B8/%E8%B5%A2%E5%9F%BA%E9%87%91', 'name': '21è´¢ç»-èµ¢åŸºé‡‘', 'type': 'rss'},
    {
        'url': 'https://xueqiu.com/k?q=%E5%9F%BA%E9%87%91',
        'name': 'é›ªçƒ-åŸºé‡‘æœç´¢',
        'type': 'web',
        'selector': '.search__list .search-result-item .search-title a' 
    }
]

def generate_markdown(news_items: List[Dict], analysis_report: str) -> str:
    """
    ç”ŸæˆMarkdownã€‚åœ¨æ–°é—»åˆ—è¡¨å‰æ’å…¥åˆ†ææŠ¥å‘Šã€‚
    """
    md_content = f"# åŸºé‡‘æ–°é—»èšåˆ ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})\n\n"
    configured_sources = list(set([s['name'].split('-')[0] for s in globals().get('sources', [])]))
    source_names = "ã€".join(configured_sources)
    md_content += f"æ¥æºï¼š{source_names}ï¼ˆå…³é”®è¯ï¼šåŸºé‡‘/å®ç›˜/è§‚ç‚¹/ç»éªŒ/æ¨è/ç­–ç•¥ï¼‰ã€‚æ€»è®¡ {len(news_items)} æ¡ã€‚\n"
    
    # æ’å…¥åˆ†ææŠ¥å‘Š
    md_content += analysis_report
    
    # æ’å…¥åŸå§‹æ–°é—»åˆ—è¡¨
    md_content += "\n---\n# åŸå§‹æ–°é—»åˆ—è¡¨\n\n"
    for i, item in enumerate(news_items, 1):
        md_content += f"## {i}. {item['title']} ({item['source']})\n"
        md_content += f"- **é“¾æ¥**: [{item['link']}]({item['link']})\n"
        md_content += f"- **æ—¶é—´**: {item['pubDate']}\n"
        md_content += f"- **æ‘˜è¦**: {item['summary']}\n\n"
    return md_content

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°ï¼Œåè°ƒæŠ“å–ã€åˆ†æã€å»é‡å’Œæ–‡ä»¶ç”Ÿæˆã€‚"""
    all_news = []
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] å¼€å§‹æŠ“å–åŸºé‡‘æ–°é—» (å·²æ‰©å±•å…³é”®è¯å’Œæ·±åº¦)...")
    
    for source in sources:
        print(f"å¤„ç†æ¥æº: {source['name']} ({source['url']})")
        if source['type'] == 'rss':
            items = fetch_rss_feed(source['url'], source['name'], limit=15)
        else:
            items = fetch_web_page(source['url'], source['name'], source.get('selector'), limit=15)
        all_news.extend(items)
    
    # å»é‡
    unique_news = []
    seen_links = set()
    for news in all_news:
        if news['link'] and news['link'] != 'N/A' and news['link'] not in seen_links:
            seen_links.add(news['link'])
            unique_news.append(news)
        elif news['link'] == 'N/A' and (news['title'], news['source']) not in seen_links:
             seen_links.add((news['title'], news['source']))
             unique_news.append(news)

    # æ’åºï¼šæŒ‰æ—¶é—´å€’åºæ’åˆ—
    def sort_key(item):
        time_str = item['pubDate']
        if time_str and time_str != 'N/A':
            try:
                # å°è¯•è§£æä¸º datetime å¯¹è±¡
                return datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
            except ValueError:
                pass
        # æ— æ³•è§£æçš„æ’åœ¨æœ€å
        return datetime(1900, 1, 1)

    unique_news.sort(key=sort_key, reverse=True)
    
    # ã€æ ¸å¿ƒã€‘è¿è¡Œåˆ†æ
    analysis_results = analyze_news(unique_news)
    analysis_report_md = generate_analysis_report(analysis_results, len(unique_news))
    
    # ç”ŸæˆMD
    md_content = generate_markdown(unique_news, analysis_report_md)
    output_file = 'fund_news.md'
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    print(f"æ”¶é›†åˆ° {len(unique_news)} æ¡ç‹¬ç‰¹åŸºé‡‘æ–°é—»ã€‚åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆå¹¶ä¿å­˜è‡³ {output_file}")
    
    print("\n--- åˆ†ææŠ¥å‘Šæ‘˜è¦ ---")
    print(analysis_report_md.split('## ğŸ’°')[0])

if __name__ == "__main__":
    main()
