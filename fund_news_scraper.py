import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from typing import List, Dict
import re
import xml.etree.ElementTree as ET
from dateutil import parser
import pytz
import matplotlib.pyplot as plt
from collections import Counter
import sqlite3
import json
import logging
import time
from wordcloud import WordCloud
import jieba
from retry import retry

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('fund_news_scraper.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# --- ã€æ ¸å¿ƒé…ç½®ã€‘åˆ†æè§„åˆ™åº“ï¼šé€šç”¨ã€å¯æ‰©å±•çš„åˆ†æé€»è¾‘ ---

# 1. æŠ•èµ„çº¿ç´¢ (æ ‡çš„/ç­–ç•¥ -> æ€»ç»“)
CLUES_MAP = {
    r'çœ‹å¥½|å»ºè®®é…ç½®|ç­–ç•¥ä¸»çº¿|èšç„¦|å¸ƒå±€|æ¨è|é‡‘è‚¡|å®ç›˜': {'desc': 'ã€é€šç”¨ç­–ç•¥ä¿¡å·ã€‘è¯†åˆ«åˆ°æ˜ç¡®çš„é…ç½®å»ºè®®æˆ–ç­–ç•¥ä¸»çº¿', 'weight': 1.0},
    r'å®è§‚|ç­–ç•¥æŠ¥å‘Š|å‘¨æœŸ|å››ä¸­å…¨ä¼š|åäº”äº”': {'desc': 'ã€å®è§‚ç­–ç•¥ä¿¡å·ã€‘å®è§‚æˆ–å‘¨æœŸæ€§ä¸»é¢˜æŠ¥å‘Š', 'weight': 0.8},
    r'æè““|åŠå¤|ä¸­è¯500|IC': {'desc': 'ç§å‹Ÿè§‚ç‚¹ï¼šä¸­è¯500/ç§‘æŠ€æˆé•¿ç­–ç•¥', 'weight': 0.9},
    r'åå®‰è¯åˆ¸|æˆé•¿äº§ä¸š|AI|å†›å·¥': {'desc': 'åˆ¸å•†è§‚ç‚¹ï¼šAI/å†›å·¥/æ–°æˆé•¿äº§ä¸šé“¾é…ç½®', 'weight': 0.9},
    r'å¼€æºè¯åˆ¸|é‡‘è‚¡ç­–ç•¥|ç§‘æŠ€|æ¸¯è‚¡': {'desc': 'åˆ¸å•†è§‚ç‚¹ï¼šAI+è‡ªä¸»å¯æ§ç§‘æŠ€ä¸»çº¿', 'weight': 0.9},
    r'ETF|è‚¡ç¥¨ETF|ç™¾äº¿ä¿±ä¹éƒ¨|å¸é‡‘': {'desc': 'èµ„é‡‘æµå‘/è‚¡ç¥¨ETF/å¸é‡‘èµ›é“', 'weight': 0.8},
    r'è´µé‡‘å±|é»„é‡‘|é¿é™©': {'desc': 'èµ„äº§å¯¹å†²/é¿é™©é…ç½® (è´µé‡‘å±)', 'weight': 0.7},
    r'å‡è¡¡é…ç½®|å…‰ä¼|åŒ–å·¥|å†œä¸š|æœ‰è‰²|é“¶è¡Œ': {'desc': 'ä½ä½/å‡è¡¡æ¿å—é…ç½®å»ºè®®', 'weight': 0.7},
    r'ç§‘æŠ€è‚¡|AIç®—åŠ›|äººå½¢æœºå™¨äºº': {'desc': 'ç§‘æŠ€åˆ›æ–°/AIé©±åŠ¨äº§ä¸šé“¾æœºä¼š', 'weight': 1.0},
    r'æ¶ˆè´¹|ETF|ç§‘æŠ€èåˆ': {'desc': 'æ¶ˆè´¹ç§‘æŠ€èåˆé…ç½®', 'weight': 0.8},
    r'å…¬å‹Ÿè§„æ¨¡|çªç ´|å¢é•¿': {'desc': 'å…¬å‹Ÿè¡Œä¸šè§„æ¨¡æ‰©å¼ ä¿¡å·', 'weight': 0.7},
}

# 2. ç»éªŒæ•™è®­ (è¡Œä¸º/ç»“æœ -> é£é™©/æ•™è®­)
LESSONS_MAP = {
    r'è­¦æƒ•|é£é™©|æ•™è®­|æ¶‰èµŒ|è·‘è¾“|å†…æ§': {'desc': 'ã€é€šç”¨é£é™©ä¿¡å·ã€‘è¯†åˆ«åˆ°è¡Œä¸šé£é™©æˆ–è´Ÿé¢ç»éªŒæ•™è®­', 'weight': 1.0},
    r'è·‘è¾“å¤§ç›˜|æœªèƒ½æ»¡ä»“|çº¢åˆ©æ¿å—': {'desc': 'æ–°åŸºé‡‘å»ºä»“ç­–ç•¥ä¸å¸‚åœºé”™é…é£é™©', 'weight': 0.9},
    r'åŸºé‡‘ç»ç†|æ¶‰èµŒ|å…èŒ': {'desc': 'åŸºé‡‘ç»ç†é“å¾·é£é™©ä¸å…¬å¸å†…æ§è­¦ç¤º', 'weight': 1.0},
    r'æœºæ„å¤§ä¸¾å¢æŒ|ä¸»åŠ¨æƒç›ŠåŸºé‡‘': {'desc': 'æœºæ„è¡Œä¸ºï¼šä¸»åŠ¨æƒç›ŠåŸºé‡‘ä»æ˜¯é…ç½®é‡ç‚¹', 'weight': 0.8},
    r'ä¼ªæˆé•¿|æ‹¥æŒ¤|é™·é˜±': {'desc': 'æˆé•¿èµ›é“æ‹¥æŒ¤ä¸ä¼ªæˆé•¿é£é™©', 'weight': 0.9},
    r'å‡æŒ|é«˜ä½': {'desc': 'è‚¡ä¸œå‡æŒä¸é«˜ä½å›è°ƒé£é™©', 'weight': 0.9},
}

# 3. è¡Œä¸šè¶‹åŠ¿ (ç»“æ„å˜åŒ– -> è¡Œä¸šæ´å¯Ÿ)
TRENDS_MAP = {
    r'AI|æŠ•ç ”|å·¥ä¸šåŒ–|èš‚èšè´¢å¯Œ': {'desc': 'è¡Œä¸šè¶‹åŠ¿ï¼šæŠ•ç ”å·¥ä¸šåŒ–å’ŒAIèµ‹èƒ½', 'weight': 0.9},
    r'è´¹ç‡|ä¸‹è°ƒ|æ‰˜ç®¡è´¹|ä½™é¢å®': {'desc': 'è¡Œä¸šè¶‹åŠ¿ï¼šå…³æ³¨è´¹ç‡æˆæœ¬çš„é•¿æœŸä¸‹è¡Œ', 'weight': 0.8},
    r'ç§å‹Ÿè‚¡æƒ|å­å…¬å¸|å¹¿å‘åŸºé‡‘': {'desc': 'è¡Œä¸šè¶‹åŠ¿ï¼šå¤´éƒ¨å…¬å‹Ÿçš„ä¸šåŠ¡å¤šå…ƒåŒ–', 'weight': 0.8},
    r'é‡åŒ–åŸºé‡‘ç»ç†|ä¸»åŠ¨åŸºé‡‘|ä¸€æ‹–å¤š': {'desc': 'è¡Œä¸šè¶‹åŠ¿ï¼šé‡åŒ–ä¸ä¸»åŠ¨æŠ•èµ„è¾¹ç•Œæ¨¡ç³Š', 'weight': 0.8},
    r'REITs|è·æ‰¹|åŸºç¡€è®¾æ–½': {'desc': 'REITså¸‚åœºæ‰©å¼ ä¸åŸºç¡€è®¾æ–½æŠ•èµ„è¶‹åŠ¿', 'weight': 0.7},
    r'ESG|å‡æ’|ç»¿è‰²é‡‘è': {'desc': 'ESGä¸ç»¿è‰²æŠ•èµ„è¶‹åŠ¿', 'weight': 0.7},
    r'å…»è€|ç¬¬ä¸‰æ”¯æŸ±': {'desc': 'å…»è€åŸºé‡‘ä¸é•¿æœŸæŠ•èµ„ä½“ç³»å»ºè®¾', 'weight': 0.7},
}

# èšåˆæ‰€æœ‰ä¸»é¢˜ï¼Œç”¨äºé•¿æœŸè¶‹åŠ¿åˆ†æ
ALL_TOPICS_MAP = {**CLUES_MAP, **LESSONS_MAP, **TRENDS_MAP}

# æ–°å¢ï¼šæ½œåœ¨å½±å“æ¨¡æ¿åº“ (åŸºäºå…³é”®è¯ç”Ÿæˆæ–°é—»å½±å“æ€»ç»“)
IMPACT_TEMPLATES = {
    r'AI|ç®—åŠ›|ç§‘æŠ€é¾™å¤´|äº§ä¸šé“¾': 'æ½œåœ¨å½±å“ï¼šå¯èƒ½æ¨åŠ¨ç§‘æŠ€æ¿å—çŸ­æœŸä¸Šæ¶¨ï¼Œä½†éœ€è­¦æƒ•ä¼°å€¼æ³¡æ²«é£é™©ï¼Œå»ºè®®å…³æ³¨äº§ä¸šé“¾ä¸­ä½ä¼°å€¼æ ‡çš„ã€‚',
    r'è´¹ç‡|é™è´¹|è´§å¸åŸºé‡‘': 'æ½œåœ¨å½±å“ï¼šé™ä½æŠ•èµ„è€…æˆæœ¬ï¼Œæå‡åŸºé‡‘å¸å¼•åŠ›ï¼Œé•¿æœŸåˆ©å¥½å…¬å‹Ÿè¡Œä¸šè§„æ¨¡æ‰©å¼ ï¼Œä½†çŸ­æœŸå¯èƒ½æŒ¤å‹åŸºé‡‘å…¬å¸åˆ©æ¶¦ã€‚',
    r'é£é™©|è­¦æƒ•|è·‘è¾“|é“å¾·é£é™©': 'æ½œåœ¨å½±å“ï¼šå¢åŠ å¸‚åœºæ³¢åŠ¨æ€§ï¼ŒæŠ•èµ„è€…åº”åŠ å¼ºé£é™©ç®¡ç†ï¼Œé¿å…è¿½é«˜çƒ­é—¨èµ›é“ã€‚',
    r'ETF|å¸é‡‘|èµ„é‡‘æµå‘': 'æ½œåœ¨å½±å“ï¼šåŠ é€Ÿèµ„é‡‘å‘çƒ­é—¨ä¸»é¢˜å€¾æ–œï¼Œå¢å¼ºå¸‚åœºæµåŠ¨æ€§ï¼Œä½†å¯èƒ½æ”¾å¤§æ¿å—è½®åŠ¨æ•ˆåº”ã€‚',
    r'å®è§‚|ç­–ç•¥æŠ¥å‘Š|éœ‡è¡ä¸Šè¡Œ': 'æ½œåœ¨å½±å“ï¼šå››å­£åº¦å¸‚åœºæˆ–å‘ˆNå‹èµ°åŠ¿ï¼Œç§‘æŠ€ä¸åå†…å·æ¿å—å—ç›Šï¼Œå»ºè®®å‡è¡¡é…ç½®ã€‚',
    r'ç§å‹Ÿè§‚ç‚¹|ä¸­è¯500': 'æ½œåœ¨å½±å“ï¼šä¸­è¯500æŒ‡æ•°å¯èƒ½å¸å¼•æ›´å¤šèµ„é‡‘æµå…¥ç§‘æŠ€æˆé•¿è‚¡ï¼Œæå‡æŒ‡æ•°è¡¨ç°ã€‚',
    r'è´µé‡‘å±|é¿é™©': 'æ½œåœ¨å½±å“ï¼šåœ¨åœ°ç¼˜é£é™©ä¸‹ï¼Œè´µé‡‘å±ä½œä¸ºå¯¹å†²å·¥å…·éœ€æ±‚ä¸Šå‡ï¼Œé…ç½®ä»·å€¼æå‡ã€‚',
    r'ä¸šåŠ¡å¤šå…ƒåŒ–|å­å…¬å¸': 'æ½œåœ¨å½±å“ï¼šå…¬å‹Ÿæ‰©å±•ç§å‹Ÿè‚¡æƒç­‰é¢†åŸŸï¼Œå¢å¼ºç»¼åˆç«äº‰åŠ›ï¼Œåˆ©å¥½é•¿æœŸæŠ•èµ„è€…ã€‚',
    r'REITs|è·æ‰¹': 'æ½œåœ¨å½±å“ï¼šREITsè·æ‰¹å°†æ³¨å…¥æ–°æ´»åŠ›ï¼Œä¿ƒè¿›åŸºç¡€è®¾æ–½æŠ•èµ„ï¼Œå¸å¼•æ›´å¤šèµ„é‡‘è¿›å…¥ç›¸å…³é¢†åŸŸã€‚',
    r'ESG|å‡æ’': 'æ½œåœ¨å½±å“ï¼šESGæ”¿ç­–å¼ºåŒ–å°†æ¨åŠ¨ç»¿è‰²è½¬å‹ï¼Œåˆ©å¥½å¯æŒç»­æŠ•èµ„ä¸»é¢˜åŸºé‡‘ã€‚',
    r'å…»è€|ç¬¬ä¸‰æ”¯æŸ±': 'æ½œåœ¨å½±å“ï¼šå…»è€ä½“ç³»å®Œå–„å°†å¢åŠ é•¿æœŸèµ„é‡‘ä¾›ç»™ï¼Œç¨³å®šèµ„æœ¬å¸‚åœºã€‚',
    r'.*': 'æ½œåœ¨å½±å“ï¼šè¯¥æ–°é—»å¯èƒ½å¯¹ç›¸å…³æ¿å—äº§ç”Ÿä¸­æ€§å½±å“ï¼Œå»ºè®®ç»“åˆå¸‚åœºåŠ¨æ€è¿›ä¸€æ­¥è¯„ä¼°ã€‚'
}

# æ–°å¢ï¼šç®€å•æƒ…æ„Ÿåˆ†æå…³é”®è¯
POSITIVE_WORDS = ['çœ‹å¥½', 'ä¸Šæ¶¨', 'å¢é•¿', 'æœºä¼š', 'å¸ƒå±€', 'æ¨è', 'æ½œåŠ›', 'çªç ´']
NEGATIVE_WORDS = ['é£é™©', 'è­¦æƒ•', 'è·‘è¾“', 'å‡æŒ', 'æ•™è®­', 'é™·é˜±', 'æ³¢åŠ¨', 'ä¸‹è·Œ']

# --- æ–°å¢ï¼šåŠ¨æ€å…³é”®è¯æ‰©å±• ---
def extract_dynamic_keywords(text: str, min_freq: int = 2) -> List[str]:
    """åŸºäº jieba åˆ†è¯åŠ¨æ€æå–é«˜é¢‘å…³é”®è¯ï¼Œæ’é™¤å·²æœ‰è§„åˆ™ä¸­çš„å…³é”®è¯ã€‚"""
    words = jieba.cut(text)
    word_freq = Counter(words)
    existing_keywords = set()
    for pattern in ALL_TOPICS_MAP.keys():
        existing_keywords.update(pattern.split('|'))
    
    dynamic_keywords = [word for word, freq in word_freq.items() if freq >= min_freq and word not in existing_keywords and len(word) > 1]
    return dynamic_keywords[:5]  # è¿”å›å‰5ä¸ªé«˜é¢‘è¯

# --- æ•°æ®åº“ç®¡ç†ç±» ---
class DatabaseManager:
    def __init__(self, db_name='fund_news_analysis.db'):
        self.db_name = db_name
        self.conn = None
        self._connect()
        self._create_table()

    def _connect(self):
        self.conn = sqlite3.connect(self.db_name)
        self.conn.row_factory = sqlite3.Row
        # ä¼˜åŒ–ï¼šå¯ç”¨ WAL æ¨¡å¼ï¼Œæå‡å¹¶å‘æ€§èƒ½
        self.conn.execute('PRAGMA journal_mode=WAL')
    
    def _create_table(self):
        cursor = self.conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS analyzed_news (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                link TEXT UNIQUE,
                pubDate TEXT,
                source TEXT,
                crawl_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                sentiment TEXT,
                key_topics_json TEXT,
                dynamic_keywords_json TEXT
            )
        ''')
        # ä¼˜åŒ–ï¼šä¸ºå¸¸ç”¨æŸ¥è¯¢å­—æ®µæ·»åŠ ç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_pubDate ON analyzed_news(pubDate)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_source ON analyzed_news(source)')
        self.conn.commit()

    def get_existing_links(self) -> set:
        cursor = self.conn.cursor()
        cursor.execute("SELECT link FROM analyzed_news WHERE link IS NOT NULL AND link != 'N/A'")
        return {row['link'] for row in cursor.fetchall()}

    def store_news_and_analysis(self, news_item: Dict, analysis_result: Dict):
        title = news_item['title']
        link = news_item.get('link', 'N/A')
        pubDate = news_item.get('pubDate', datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d %H:%M:%S'))
        source = news_item['source']
        sentiment = analysis_result.get('sentiment', 'ä¸­æ€§ (Neutral)')
        key_topics_json = json.dumps(analysis_result.get('key_topics', []))
        dynamic_keywords_json = json.dumps(analysis_result.get('dynamic_keywords', []))
        
        cursor = self.conn.cursor()
        try:
            cursor.execute('''
                INSERT INTO analyzed_news (title, link, pubDate, source, sentiment, key_topics_json, dynamic_keywords_json)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (title, link, pubDate, source, sentiment, key_topics_json, dynamic_keywords_json))
            self.conn.commit()
        except sqlite3.IntegrityError:
            logger.warning(f"Duplicate link found, skipping: {link}")
        except Exception as e:
            logger.error(f"Error storing news to DB: {e}")

    def get_topics_by_time_range(self, days: int) -> Dict[str, int]:
        since_date = (datetime.now(pytz.timezone('Asia/Shanghai')) - timedelta(days=days)).strftime('%Y-%m-%d %H:%M:%S')
        cursor = self.conn.cursor()
        cursor.execute("SELECT key_topics_json FROM analyzed_news WHERE pubDate > ?", (since_date,))
        
        topic_counter = Counter()
        for row in cursor.fetchall():
            try:
                topics = json.loads(row['key_topics_json'])
                topic_counter.update(topics)
            except (json.JSONDecodeError, TypeError):
                continue
        return dict(topic_counter)

    def get_dynamic_keywords_by_time_range(self, days: int) -> Dict[str, int]:
        since_date = (datetime.now(pytz.timezone('Asia/Shanghai')) - timedelta(days=days)).strftime('%Y-%m-%d %H:%M:%S')
        cursor = self.conn.cursor()
        cursor.execute("SELECT dynamic_keywords_json FROM analyzed_news WHERE pubDate > ?", (since_date,))
        
        keyword_counter = Counter()
        for row in cursor.fetchall():
            try:
                keywords = json.loads(row['dynamic_keywords_json'])
                keyword_counter.update(keywords)
            except (json.JSONDecodeError, TypeError):
                continue
        return dict(keyword_counter)

    def close(self):
        if self.conn:
            self.conn.close()
            logger.info("Database connection closed.")

# --- è¾…åŠ©å‡½æ•°ï¼šæ—¶é—´è§£æå’Œæ ¼å¼åŒ– ---
def parse_and_format_time(pub_date: str) -> str:
    if pub_date == 'N/A' or not pub_date:
        return 'N/A'
    try:
        dt_utc = parser.parse(pub_date).replace(tzinfo=pytz.utc)
        dt_local = dt_utc.astimezone(pytz.timezone('Asia/Shanghai'))
        return dt_local.strftime('%Y-%m-%d %H:%M:%S')
    except Exception:
        logger.warning(f"Failed to parse date: {pub_date}")
        return pub_date

# --- è¾…åŠ©å‡½æ•°ï¼šHTMLæ¸…ç†å’Œæ‘˜è¦å¤„ç† ---
def clean_html_summary(summary: str, max_len: int = 400) -> str:
    if not summary:
        return 'æ— æ‘˜è¦'
    clean_soup = BeautifulSoup(summary, 'html.parser')
    clean_text = clean_soup.get_text(strip=True)
    clean_text = re.sub(r'\s+', ' ', clean_text)
    if len(clean_text) > max_len:
        return clean_text[:max_len] + '...'
    return clean_text

# --- æ–°å¢ï¼šä¼˜åŒ–æƒ…æ„Ÿåˆ†æ ---
def weighted_sentiment_analysis(text: str) -> tuple[str, float]:
    pos_score = 0
    neg_score = 0
    for word in POSITIVE_WORDS:
        if re.search(word, text, re.IGNORECASE):
            pos_score += text.lower().count(word.lower()) * 1.0
    for word in NEGATIVE_WORDS:
        if re.search(word, text, re.IGNORECASE):
            neg_score += text.lower().count(word.lower()) * 1.0
    
    total_score = pos_score - neg_score
    if total_score > 0:
        sentiment = 'æ­£é¢ (Positive)'
    elif total_score < 0:
        sentiment = 'è´Ÿé¢ (Negative)'
    else:
        sentiment = 'ä¸­æ€§ (Neutral)'
    return sentiment, total_score

# --- è¯¦ç»†æ–°é—»åˆ†æå‡½æ•° ---
def detailed_analyze_news(item: Dict) -> Dict:
    text = item['title'] + ' ' + item['summary']
    analysis = {
        'title': item['title'],
        'detailed_summary': f"æ ‡é¢˜ï¼š{item['title']}\næ‘˜è¦ï¼š{item['summary']}",
        'key_topics': [],
        'potential_impact': '',
        'sentiment': 'ä¸­æ€§ (Neutral)',
        'sentiment_score': 0.0,
        'dynamic_keywords': extract_dynamic_keywords(text)
    }
    
    # æå–å…³é”®ä¸»é¢˜ï¼ˆè€ƒè™‘æƒé‡ï¼‰
    for map_dict in [CLUES_MAP, LESSONS_MAP, TRENDS_MAP]:
        for pattern, info in map_dict.items():
            if re.search(pattern, text, re.IGNORECASE):
                analysis['key_topics'].append(info['desc'])
    
    # ç”Ÿæˆæ½œåœ¨å½±å“
    impact_found = False
    for pattern, impact in IMPACT_TEMPLATES.items():
        if re.search(pattern, text, re.IGNORECASE):
            analysis['potential_impact'] = impact
            impact_found = True
            break
    if not impact_found:
        analysis['potential_impact'] = IMPACT_TEMPLATES['.*']
    
    # ä¼˜åŒ–æƒ…æ„Ÿåˆ†æ
    analysis['sentiment'], analysis['sentiment_score'] = weighted_sentiment_analysis(text)
    
    return analysis

# --- æ ¸å¿ƒæŠ“å–å‡½æ•°ï¼šRSS ---
@retry(tries=3, delay=2, backoff=2, logger=logger)
def fetch_rss_feed(url: str, source_name: str, limit: int = 20) -> List[Dict]:
    filtered_items = []
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    response = requests.get(url, timeout=10, headers=headers)
    response.raise_for_status()
    
    try:
        root = ET.fromstring(response.content)
    except ET.ParseError:
        logger.warning(f"[{source_name}] Error parsing XML. Trying content decoding...")
        root = ET.fromstring(response.text.encode('utf-8'))

    items = root.findall('.//item') or root.findall('.//entry')
    for item in items[:limit]:
        title_element = item.find('title')
        link_element = item.find('link')
        pub_date_element = item.find('pubDate') or item.find('{http://purl.org/dc/elements/1.1/}date') or item.find('published')
        summary_element = item.find('description') or item.find('summary') or item.find('content')

        title = title_element.text.strip() if title_element is not None and title_element.text else ''
        link = 'N/A'
        if link_element is not None:
            if link_element.text:
                link = link_element.text.strip()
            elif link_element.attrib.get('href'):
                link = link_element.attrib['href'].strip()

        pub_date_raw = pub_date_element.text.strip() if pub_date_element is not None and pub_date_element.text else 'N/A'
        summary_raw = summary_element.text.strip() if summary_element is not None and summary_element.text else ''
        
        summary = clean_html_summary(summary_raw, max_len=400)
        pub_date = parse_and_format_time(pub_date_raw)
        
        if re.search(r'åŸºé‡‘|å®ç›˜|è§‚ç‚¹|ç»éªŒ|æ¨è|ç­–ç•¥|æŠ•èµ„|è‚¡ç¥¨|å®è§‚|é‡‘è', title + summary, re.IGNORECASE):
            filtered_items.append({
                'title': title,
                'link': link,
                'pubDate': pub_date,
                'summary': summary,
                'source': source_name
            })
    return filtered_items

# --- æ ¸å¿ƒæŠ“å–å‡½æ•°ï¼šWeb ---
@retry(tries=3, delay=2, backoff=2, logger=logger)
def fetch_web_page(url: str, source_name: str, selector: str, limit: int = 15) -> List[Dict]:
    filtered_items = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Referer': url.split('/')[2]
    }
    response = requests.get(url, timeout=10, headers=headers)
    response.raise_for_status()
    soup = BeautifulSoup(response.content, 'html.parser')
    
    items = soup.select(selector)
    for item in items[:limit]:
        title_tag = item
        if not title_tag:
            continue
        
        title = title_tag.get_text(strip=True)
        link = title_tag.get('href', '')
        if link and not link.startswith('http'):
            link = f"https://{url.split('/')[2]}{link}"
        
        parent = item.find_parent()
        summary_tag = parent.select_one('.summary, .search-summary, .search-snippet, .search-content, .content')
        summary_raw = summary_tag.get_text(strip=True) if summary_tag else title
        summary = clean_html_summary(summary_raw, max_len=400)
        
        if re.search(r'åŸºé‡‘|å®ç›˜|è§‚ç‚¹|ç»éªŒ|æ¨è|ç­–ç•¥|æŠ•èµ„|è‚¡ç¥¨|å®è§‚|é‡‘è', title + summary, re.IGNORECASE):
            filtered_items.append({
                'title': title,
                'link': link if link else 'N/A',
                'pubDate': 'N/A',
                'summary': summary,
                'source': source_name
            })
    return filtered_items

# --- æ ¸å¿ƒæ–°é—»åˆ†æå‡½æ•° ---
def analyze_news(news_items: List[Dict]) -> Dict:
    analysis = {
        'investment_clues': [],
        'experience_lessons': [],
        'industry_trends': [],
        'detailed_analyses': []
    }
    seen_clues = set()
    seen_lessons = set()
    seen_trends = set()

    for item in news_items:
        text = item['title'] + ' ' + item['summary']
        for pattern, info in CLUES_MAP.items():
            if re.search(pattern, text, re.IGNORECASE) and info['desc'] not in seen_clues:
                analysis['investment_clues'].append({
                    'focus': info['desc'],
                    'title': item['title'],
                    'link': item['link'],
                    'weight': info['weight']
                })
                seen_clues.add(info['desc'])
        for pattern, info in LESSONS_MAP.items():
            if re.search(pattern, text, re.IGNORECASE) and info['desc'] not in seen_lessons:
                analysis['experience_lessons'].append({
                    'lesson': info['desc'],
                    'title': item['title'],
                    'link': item['link'],
                    'weight': info['weight']
                })
                seen_lessons.add(info['desc'])
        for pattern, info in TRENDS_MAP.items():
            if re.search(pattern, text, re.IGNORECASE) and info['desc'] not in seen_trends:
                analysis['industry_trends'].append({
                    'trend': info['desc'],
                    'title': item['title'],
                    'link': item['link'],
                    'weight': info['weight']
                })
                seen_trends.add(info['desc'])
        
        detailed = detailed_analyze_news(item)
        analysis['detailed_analyses'].append(detailed)
    
    # æŒ‰æƒé‡æ’åº
    analysis['investment_clues'].sort(key=lambda x: x['weight'], reverse=True)
    analysis['experience_lessons'].sort(key=lambda x: x['weight'], reverse=True)
    analysis['industry_trends'].sort(key=lambda x: x['weight'], reverse=True)
    
    return analysis

# --- æ–°å¢ï¼šç”Ÿæˆè¯äº‘ ---
def generate_wordcloud(keywords: Dict[str, int], output_file: str):
    if not keywords:
        logger.info("No keywords for wordcloud generation.")
        return
    wordcloud = WordCloud(
        font_path='SimHei.ttf',  # ç¡®ä¿æœ‰ä¸­æ–‡å­—ä½“
        width=800, height=400, background_color='white', max_words=50
    ).generate_from_frequencies(keywords)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.savefig(f'{output_file}_wordcloud.png')
    plt.close()
    logger.info(f"Generated wordcloud: {output_file}_wordcloud.png")

# --- é•¿æœŸè¶‹åŠ¿åˆ†æå‡½æ•° ---
def generate_trend_analysis(db_manager: DatabaseManager) -> str:
    recent_topics = db_manager.get_topics_by_time_range(days=7)
    previous_topics = db_manager.get_topics_by_time_range(days=14)
    recent_keywords = db_manager.get_dynamic_keywords_by_time_range(days=7)
    
    p2_only_topics = {topic: count - recent_topics.get(topic, 0) for topic, count in previous_topics.items()}
    p2_only_topics = {k: v for k, v in p2_only_topics.items() if v > 0}
    
    trend_report = "\n### ğŸ“ˆ ä¸»é¢˜ä¸å…³é”®è¯è¶‹åŠ¿åˆ†æ (è¿‘ 7 å¤© vs å‰ 7 å¤©)\n"
    trend_report += "å¯¹æ¯”æ˜¾ç¤ºä¸»é¢˜å’ŒåŠ¨æ€å…³é”®è¯çš„å…³æ³¨åº¦å˜åŒ–ï¼Œå˜åŒ–ç‡ > 50% çš„ä¸»é¢˜é«˜äº®ã€‚\n\n"
    
    # ä¸»é¢˜è¶‹åŠ¿
    trend_report += "#### ä¸»é¢˜çƒ­åº¦å˜åŒ–\n"
    trend_report += "| ä¸»é¢˜ | è¿‘ 7 å¤© | å‰ 7 å¤© | å˜åŒ–ç‡ | è¶‹åŠ¿ |\n"
    trend_report += "| :--- | :---: | :---: | :---: | :---: |\n"
    
    all_topics = set(recent_topics.keys()) | set(p2_only_topics.keys())
    for topic in sorted(all_topics, key=lambda x: recent_topics.get(x, 0), reverse=True):
        count_p1 = recent_topics.get(topic, 0)
        count_p0 = p2_only_topics.get(topic, 0)
        if count_p1 == 0 and count_p0 == 0:
            continue
        if count_p0 > 0:
            change_rate = (count_p1 - count_p0) / count_p0
            trend_icon = "â¬†ï¸" if change_rate > 0.1 else ("â¬‡ï¸" if change_rate < -0.1 else "â†”ï¸")
            trend_str = f"{change_rate:.0%}"
        elif count_p1 > 0:
            change_rate = float('inf')
            trend_icon = "ğŸ”¥"
            trend_str = "NEW"
        else:
            change_rate = 0
            trend_icon = "â–"
            trend_str = "0%"
        if abs(change_rate) > 0.5 and change_rate != float('inf'):
            trend_str = f"**{trend_str}**"
        trend_report += f"| {topic} | {count_p1} | {count_p0} | {trend_str} | {trend_icon} |\n"
    
    # åŠ¨æ€å…³é”®è¯è¶‹åŠ¿
    trend_report += "\n#### åŠ¨æ€å…³é”®è¯ Top 5\n"
    top_keywords = sorted(recent_keywords.items(), key=lambda x: x[1], reverse=True)[:5]
    if top_keywords:
        trend_report += "| å…³é”®è¯ | å‡ºç°æ¬¡æ•° |\n"
        trend_report += "| :--- | :---: |\n"
        for keyword, count in top_keywords:
            trend_report += f"| {keyword} | {count} |\n"
    else:
        trend_report += "æš‚æ— åŠ¨æ€å…³é”®è¯ã€‚\n"
    
    return trend_report

# --- ç”Ÿæˆç»Ÿè®¡å›¾è¡¨ ---
def generate_stats_chart(analysis: Dict, output_file: str):
    clue_count = len(analysis['investment_clues'])
    lesson_count = len(analysis['experience_lessons'])
    trend_count = len(analysis['industry_trends'])
    
    categories = ['Investment Clues', 'Experience Lessons', 'Industry Trends']
    counts = [clue_count, lesson_count, trend_count]
    
    plt.figure(figsize=(8, 5))
    bars = plt.bar(categories, counts, color=['#1f77b4', '#ff7f0e', '#2ca02c'])
    plt.title('News Analysis Categories Count (Current Run)')
    plt.ylabel('Count')
    for bar in bars:
        yval = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.1, int(yval), ha='center', va='bottom')
    plt.savefig(f'{output_file}_stats.png')
    plt.close()
    logger.info(f"Generated stats chart: {output_file}_stats.png")

# --- ç”Ÿæˆåˆ†ææŠ¥å‘Š ---
def generate_analysis_report(analysis: Dict, total_count: int, trend_report: str) -> str:
    md_report = "\n---\n"
    md_report += "# ğŸ“° åŸºé‡‘æŠ•èµ„ç­–ç•¥åˆ†ææŠ¥å‘Š\n\n"
    md_report += f"æœ¬æŠ¥å‘Šæ ¹æ®ä» {total_count} æ¡æ–°é—»ä¸­æå–çš„é«˜ä»·å€¼ä¿¡æ¯ç”Ÿæˆï¼Œæ—¨åœ¨ä¸ºæ‚¨æä¾› **ä¹°å…¥æŒ‡å¼•ã€é£é™©è§„é¿å’Œè¡Œä¸šæ´å¯Ÿ**ã€‚\n\n"

    md_report += "## ğŸ“Š ç»Ÿè®¡æ¦‚è¿°\n"
    md_report += f"- æœ¬æ¬¡æŠ“å–æŠ•èµ„çº¿ç´¢æ•°é‡: {len(analysis['investment_clues'])}\n"
    md_report += f"- æœ¬æ¬¡æŠ“å–ç»éªŒæ•™è®­æ•°é‡: {len(analysis['experience_lessons'])}\n"
    md_report += f"- æœ¬æ¬¡æŠ“å–è¡Œä¸šè¶‹åŠ¿æ•°é‡: {len(analysis['industry_trends'])}\n"
    md_report += f"- æ€»æ–°é—»æ¡ç›®: {total_count}\n"
    md_report += f"- ç”Ÿæˆå›¾è¡¨: {output_file}_stats.png, {output_file}_wordcloud.png\n\n"
    
    md_report += "## é•¿æœŸè¶‹åŠ¿åˆ†æ\n"
    md_report += trend_report

    md_report += "\n## ğŸ’° æŠ•èµ„çº¿ç´¢ä¸å¸‚åœºç„¦ç‚¹ (ä¹°å…¥æŒ‡å¼•)\n"
    if analysis['investment_clues']:
        md_report += "| ç„¦ç‚¹æ ‡çš„/ç­–ç•¥ | åŸå§‹æ ‡é¢˜ (ç‚¹å‡»æŸ¥çœ‹) | æƒé‡ |\n"
        md_report += "| :--- | :--- | :---: |\n"
        for clue in analysis['investment_clues']:
            md_report += f"| **{clue['focus']}** | [{clue['title']}](<{clue['link']})> | {clue['weight']:.1f} |\n"
    else:
        md_report += "æš‚æ— æ˜ç¡®çš„æŠ•èµ„çº¿ç´¢æˆ–æœºæ„è§‚ç‚¹è¢«è¯†åˆ«ã€‚\n"
        
    md_report += "\n## âš ï¸ æŠ•èµ„ç»éªŒä¸é£é™©è§„é¿ (é¿å…è¸©å‘)\n"
    if analysis['experience_lessons']:
        md_report += "| æ•™è®­/ç»éªŒ | åŸå§‹æ ‡é¢˜ (ç‚¹å‡»æŸ¥çœ‹) | æƒé‡ |\n"
        md_report += "| :--- | :--- | :---: |\n"
        for lesson in analysis['experience_lessons']:
            md_report += f"| **{lesson['lesson']}** | [{lesson['title']}](<{lesson['link']})> | {lesson['weight']:.1f} |\n"
    else:
        md_report += "æš‚æ— æ˜ç¡®çš„ç»éªŒæ•™è®­æˆ–é£é™©æç¤ºè¢«è¯†åˆ«ã€‚\n"

    md_report += "\n## âœ¨ è¡Œä¸šç»“æ„ä¸æœªæ¥è¶‹åŠ¿ (é•¿æœŸæ´å¯Ÿ)\n"
    if analysis['industry_trends']:
        md_report += "| è¡Œä¸šè¶‹åŠ¿ | åŸå§‹æ ‡é¢˜ (ç‚¹å‡»æŸ¥çœ‹) | æƒé‡ |\n"
        md_report += "| :--- | :--- | :---: |\n"
        for trend in analysis['industry_trends']:
            md_report += f"| **{trend['trend']}** | [{trend['title']}](<{trend['link']})> | {trend['weight']:.1f} |\n"
    else:
        md_report += "æš‚æ— æ˜ç¡®çš„è¡Œä¸šè¶‹åŠ¿æˆ–ç»“æ„å˜åŒ–è¢«è¯†åˆ«ã€‚\n"

    md_report += "\n## ğŸ” æ‰€æœ‰æ–°é—»è¯¦ç»†åˆ†æä¸æ½œåœ¨å½±å“\n"
    if analysis['detailed_analyses']:
        md_report += "| æ–°é—»æ ‡é¢˜ | å…³é”®ä¸»é¢˜ | æƒ…æ„Ÿåˆ†æ (å¾—åˆ†) | åŠ¨æ€å…³é”®è¯ | æ½œåœ¨å½±å“ |\n"
        md_report += "| :--- | :--- | :--- | :--- | :--- |\n"
        for det in analysis['detailed_analyses']:
            topics_str = '; '.join(det['key_topics']) if det['key_topics'] else 'æ— ç‰¹å®šä¸»é¢˜'
            keywords_str = ', '.join(det['dynamic_keywords']) if det['dynamic_keywords'] else 'æ— '
            md_report += f"| {det['title']} | {topics_str} | {det['sentiment']} ({det['sentiment_score']:.1f}) | {keywords_str} | **{det['potential_impact']}** |\n"
    else:
        md_report += "æš‚æ— è¯¦ç»†åˆ†æã€‚\n"

    return md_report

# --- æ•°æ®æºé…ç½® ---
proxy_base = 'https://rsshub.rss.zgdnz.cc'
sources = [
    {'url': f'{proxy_base}/cls/telegraph/fund', 'name': 'è´¢è”ç¤¾-åŸºé‡‘ç”µæŠ¥', 'type': 'rss'},
    {'url': f'{proxy_base}/eastmoney/report/strategyreport', 'name': 'ä¸œæ–¹è´¢å¯Œ-ç­–ç•¥æŠ¥å‘Š', 'type': 'rss'},
    {'url': f'{proxy_base}/gelonghui/home/fund', 'name': 'æ ¼éš†æ±‡-åŸºé‡‘', 'type': 'rss'},
    {'url': f'{proxy_base}/stcn/article/list/fund', 'name': 'è¯åˆ¸æ—¶æŠ¥-åŸºé‡‘åˆ—è¡¨', 'type': 'rss'},
    {'url': f'{proxy_base}/21caijing/channel/%E8%AF%81%E5%88%B8/%E8%B5%A2%E5%9F%BA%E9%87%91', 'name': '21è´¢ç»-èµ¢åŸºé‡‘', 'type': 'rss'},
    {'url': f'{proxy_base}/xueqiu/fund', 'name': 'é›ªçƒ-åŸºé‡‘RSS', 'type': 'rss'},
    {'url': f'{proxy_base}/zhihu/topic/19550517', 'name': 'çŸ¥ä¹-åŸºé‡‘è¯é¢˜', 'type': 'rss'},
    {'url': f'{proxy_base}/sina/finance/fund', 'name': 'æ–°æµªè´¢ç»-åŸºé‡‘ (ä»£ç†)', 'type': 'rss'},
    {
        'url': 'https://xueqiu.com/k?q=%E5%9F%BA%E9%87%91',
        'name': 'é›ªçƒ-åŸºé‡‘æœç´¢ (Web)',
        'type': 'web',
        'selector': '.search__list .search-result-item .search-title a'
    },
    {
        'url': 'https://blog.csdn.net/category_10134701.html?spm=1001.2101.3001.5700',
        'name': 'CSDN-åŸºé‡‘åšå®¢ (Web)',
        'type': 'web',
        'selector': '.blog-list-box .title a'
    },
    {'url': 'http://rss.eastmoney.com/rss_partener.xml', 'name': 'ä¸œæ–¹è´¢å¯Œ-åˆä½œä¼™ä¼´ (RSS)', 'type': 'rss'},
    {'url': 'http://rss.sina.com.cn/finance/fund.xml', 'name': 'æ–°æµªè´¢ç»-åŸºé‡‘è¦é—» (RSS)', 'type': 'rss'},
    {'url': 'http://rss.sina.com.cn/roll/finance/hot_roll.xml', 'name': 'æ–°æµªè´¢ç»-è¦é—»æ±‡æ€» (RSS)', 'type': 'rss'},
    {'url': 'https://dedicated.wallstreetcn.com/rss.xml', 'name': 'åå°”è¡—è§é—» (RSS)', 'type': 'rss'},
    {'url': 'https://36kr.com/feed', 'name': '36æ°ª (RSS)', 'type': 'rss'},
    {'url': 'https://www.hket.com/rss/china', 'name': 'é¦™æ¸¯ç¶“æ¿Ÿæ—¥å ± (RSS)', 'type': 'rss'},
    {'url': 'http://news.baidu.com/n?cmd=1&class=stock&tn=rss&sub=0', 'name': 'ç™¾åº¦-è‚¡ç¥¨ç„¦ç‚¹ (RSS)', 'type': 'rss'},
    {'url': 'https://www.chinanews.com.cn/rss/finance.xml', 'name': 'ä¸­æ–°ç½‘è´¢ç» (RSS)', 'type': 'rss'},
    {'url': 'https://www.stats.gov.cn/sj/zxfb/rss.xml', 'name': 'å›½å®¶ç»Ÿè®¡å±€-æœ€æ–°å‘å¸ƒ (RSS)', 'type': 'rss'},
    # æ–°å¢é«˜è´¨é‡æ¥æº
    {'url': 'https://www.jisilu.cn/data/rss/fund', 'name': 'é›†æ€å½•-åŸºé‡‘åŠ¨æ€ (RSS)', 'type': 'rss'},
    {'url': 'https://feed.cnblogs.com/blog/sitehome/rss', 'name': 'åšå®¢å›­-è´¢ç»åšå®¢ (RSS)', 'type': 'rss'},
    {
        'url': 'https://www.jianshu.com/c/1b2f57a2a4b3?order_by=added_at',
        'name': 'ç®€ä¹¦-æŠ•èµ„ç†è´¢ (Web)',
        'type': 'web',
        'selector': '.title a'
    },
]

def generate_markdown(news_items: List[Dict], analysis_report: str, timestamp_str: str) -> str:
    md_content = f"# åŸºé‡‘æ–°é—»èšåˆ ({timestamp_str})\n\n"
    configured_sources = list(set([s['name'].split('(')[0].strip() for s in sources]))
    source_names = "ã€".join(configured_sources)
    md_content += f"æ¥æºï¼š{source_names}ï¼ˆå…³é”®è¯ï¼šåŸºé‡‘/å®ç›˜/è§‚ç‚¹/ç»éªŒ/æ¨è/ç­–ç•¥/æŠ•èµ„/å®è§‚/é‡‘èï¼‰ã€‚æ€»è®¡ {len(news_items)} æ¡ã€‚\n"
    md_content += analysis_report
    md_content += "\n---\n# åŸå§‹æ–°é—»åˆ—è¡¨\n\n"
    for i, item in enumerate(news_items, 1):
        md_content += f"## {i}. {item['title']} ({item['source']})\n"
        md_content += f"- **é“¾æ¥**: [{item['link']}]({item['link']})\n"
        md_content += f"- **æ—¶é—´**: {item['pubDate']}\n"
        md_content += f"- **æ‘˜è¦**: {item['summary']}\n\n"
    return md_content

def main():
    db_manager = DatabaseManager()
    tz = pytz.timezone('Asia/Shanghai')
    now = datetime.now(tz)
    timestamp_str = now.strftime('%Y-%m-%d %H:%M:%S')
    global output_file
    output_file = f'fund_news_{now.strftime("%Y%m%d")}'
    
    all_news = []
    logger.info(f"[{timestamp_str}] å¼€å§‹æŠ“å–åŸºé‡‘æ–°é—»...")
    
    existing_links = db_manager.get_existing_links()
    
    for source in sources:
        logger.info(f"å¤„ç†æ¥æº: {source['name']} ({source['url']})")
        try:
            if source['type'] == 'rss':
                items = fetch_rss_feed(source['url'], source['name'], limit=20)
            else:
                items = fetch_web_page(source['url'], source['name'], source.get('selector'), limit=15)
            all_news.extend(items)
        except Exception as e:
            logger.error(f"Failed to process source {source['name']}: {e}")
    
    unique_news = []
    batch_seen = set()
    for news in all_news:
        link = news.get('link', 'N/A')
        if link != 'N/A' and link in existing_links:
            continue
        if (news['title'], news['source']) not in batch_seen:
            unique_news.append(news)
            batch_seen.add((news['title'], news['source']))
            if link != 'N/A':
                existing_links.add(link)

    unique_news.sort(key=lambda x: datetime.strptime(x['pubDate'], '%Y-%m-%d %H:%M:%S') if x['pubDate'] != 'N/A' else datetime(1900, 1, 1), reverse=True)
    
    analysis_results = analyze_news(unique_news)
    
    # æ‰¹é‡å­˜å‚¨æ–°é—»
    for item, detailed_analysis in zip(unique_news, analysis_results['detailed_analyses']):
        db_manager.store_news_and_analysis(item, detailed_analysis)
    
    # ç”Ÿæˆè¯äº‘
    recent_keywords = db_manager.get_dynamic_keywords_by_time_range(days=7)
    generate_wordcloud(recent_keywords, output_file)
    
    trend_report_md = generate_trend_analysis(db_manager)
    analysis_report_md = generate_analysis_report(analysis_results, len(unique_news), trend_report_md)
    generate_stats_chart(analysis_results, output_file)
    
    md_content = generate_markdown(unique_news, analysis_report_md, timestamp_str)
    
    with open(f'{output_file}.md', 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    logger.info(f"æ”¶é›†åˆ° {len(unique_news)} æ¡ç‹¬ç‰¹åŸºé‡‘æ–°é—»ã€‚åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆå¹¶ä¿å­˜è‡³ {output_file}.md")
    logger.info("\n--- åˆ†ææŠ¥å‘Šæ‘˜è¦ ---")
    logger.info(analysis_report_md.split('## ğŸ’°')[0])
    
    db_manager.close()

if __name__ == "__main__":
    main()
