import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict
import re
import xml.etree.ElementTree as ET
from dateutil import parser
import pytz

# --- ã€æ ¸å¿ƒé…ç½®ã€‘åˆ†æè§„åˆ™åº“ï¼šé€šç”¨ã€å¯æ‰©å±•çš„åˆ†æé€»è¾‘ ---

# 1. æŠ•èµ„çº¿ç´¢ (æ ‡çš„/ç­–ç•¥ -> æ€»ç»“)
CLUES_MAP = {
    # ç­–ç•¥é€šç”¨è¯ï¼šè¯†åˆ«ä»»ä½•è¡¨è¾¾æ˜ç¡®ä¹°å…¥/çœ‹å¥½/é…ç½®æ„å›¾çš„æ–‡ç«  (å¼ºåŒ–é€šç”¨è¯ï¼ŒæŠ“ä½è¡Œä¸º)
    r'çœ‹å¥½|å»ºè®®é…ç½®|ç­–ç•¥ä¸»çº¿|èšç„¦|å¸ƒå±€|æ¨è|é‡‘è‚¡|å®ç›˜': 'ã€é€šç”¨ç­–ç•¥ä¿¡å·ã€‘è¯†åˆ«åˆ°æ˜ç¡®çš„é…ç½®å»ºè®®æˆ–ç­–ç•¥ä¸»çº¿',
    
    # å®è§‚ç­–ç•¥ä¸å‘¨æœŸ
    r'å®è§‚|ç­–ç•¥æŠ¥å‘Š|å‘¨æœŸ|å››ä¸­å…¨ä¼š|åäº”äº”': 'ã€å®è§‚ç­–ç•¥ä¿¡å·ã€‘å®è§‚æˆ–å‘¨æœŸæ€§ä¸»é¢˜æŠ¥å‘Š',
    
    # ç‰¹æŒ‡ä¿¡å·ï¼šä¿ç•™é«˜ä»·å€¼äººç‰©/æœºæ„çš„å…³é”®è¯ï¼Œä½†å…¶å®šä½æ˜¯â€œä¿¡å·â€ï¼Œè€Œéç‰¹å®šäºº
    r'æè““|åŠå¤|ä¸­è¯500|IC': 'ç§å‹Ÿè§‚ç‚¹ï¼šä¸­è¯500/ç§‘æŠ€æˆé•¿ç­–ç•¥',
    r'åå®‰è¯åˆ¸|æˆé•¿äº§ä¸š|AI|å†›å·¥': 'åˆ¸å•†è§‚ç‚¹ï¼šAI/å†›å·¥/æ–°æˆé•¿äº§ä¸šé“¾é…ç½®',
    r'å¼€æºè¯åˆ¸|é‡‘è‚¡ç­–ç•¥|ç§‘æŠ€|æ¸¯è‚¡': 'åˆ¸å•†è§‚ç‚¹ï¼šAI+è‡ªä¸»å¯æ§ç§‘æŠ€ä¸»çº¿',
    r'ETF|è‚¡ç¥¨ETF|ç™¾äº¿ä¿±ä¹éƒ¨|å¸é‡‘': 'èµ„é‡‘æµå‘/è‚¡ç¥¨ETF/å¸é‡‘èµ›é“',
    r'è´µé‡‘å±|é»„é‡‘|é¿é™©': 'èµ„äº§å¯¹å†²/é¿é™©é…ç½® (è´µé‡‘å±)',
    r'å‡è¡¡é…ç½®|å…‰ä¼|åŒ–å·¥|å†œä¸š|æœ‰è‰²|é“¶è¡Œ': 'ä½ä½/å‡è¡¡æ¿å—é…ç½®å»ºè®®',
}

# 2. ç»éªŒæ•™è®­ (è¡Œä¸º/ç»“æœ -> é£é™©/æ•™è®­)
LESSONS_MAP = {
    r'è­¦æƒ•|é£é™©|æ•™è®­|æ¶‰èµŒ|è·‘è¾“|å†…æ§': 'ã€é€šç”¨é£é™©ä¿¡å·ã€‘è¯†åˆ«åˆ°è¡Œä¸šé£é™©æˆ–è´Ÿé¢ç»éªŒæ•™è®­',
    r'è·‘è¾“å¤§ç›˜|æœªèƒ½æ»¡ä»“|çº¢åˆ©æ¿å—': 'æ–°åŸºé‡‘å»ºä»“ç­–ç•¥ä¸å¸‚åœºé”™é…é£é™©',
    r'åŸºé‡‘ç»ç†|æ¶‰èµŒ|å…èŒ': 'åŸºé‡‘ç»ç†é“å¾·é£é™©ä¸å…¬å¸å†…æ§è­¦ç¤º',
    r'æœºæ„å¤§ä¸¾å¢æŒ|ä¸»åŠ¨æƒç›ŠåŸºé‡‘': 'æœºæ„è¡Œä¸ºï¼šä¸»åŠ¨æƒç›ŠåŸºé‡‘ä»æ˜¯é…ç½®é‡ç‚¹',
}

# 3. è¡Œä¸šè¶‹åŠ¿ (ç»“æ„å˜åŒ– -> è¡Œä¸šæ´å¯Ÿ)
TRENDS_MAP = {
    r'AI|æŠ•ç ”|å·¥ä¸šåŒ–|èš‚èšè´¢å¯Œ': 'è¡Œä¸šè¶‹åŠ¿ï¼šæŠ•ç ”å·¥ä¸šåŒ–å’ŒAIèµ‹èƒ½',
    r'è´¹ç‡|ä¸‹è°ƒ|æ‰˜ç®¡è´¹|ä½™é¢å®': 'è¡Œä¸šè¶‹åŠ¿ï¼šå…³æ³¨è´¹ç‡æˆæœ¬çš„é•¿æœŸä¸‹è¡Œ',
    r'ç§å‹Ÿè‚¡æƒ|å­å…¬å¸|å¹¿å‘åŸºé‡‘': 'è¡Œä¸šè¶‹åŠ¿ï¼šå¤´éƒ¨å…¬å‹Ÿçš„ä¸šåŠ¡å¤šå…ƒåŒ–',
    r'é‡åŒ–åŸºé‡‘ç»ç†|ä¸»åŠ¨åŸºé‡‘|ä¸€æ‹–å¤š': 'è¡Œä¸šè¶‹åŠ¿ï¼šé‡åŒ–ä¸ä¸»åŠ¨æŠ•èµ„è¾¹ç•Œæ¨¡ç³Š',
}

# æ–°å¢ï¼šæ½œåœ¨å½±å“æ¨¡æ¿åº“ (åŸºäºå…³é”®è¯ç”Ÿæˆæ–°é—»å½±å“æ€»ç»“)
IMPACT_TEMPLATES = {
    r'AI|ç®—åŠ›|ç§‘æŠ€é¾™å¤´|äº§ä¸šé“¾': 'æ½œåœ¨å½±å“ï¼šå¯èƒ½æ¨åŠ¨ç§‘æŠ€æ¿å—çŸ­æœŸä¸Šæ¶¨ï¼Œä½†éœ€è­¦æƒ•ä¼°å€¼æ³¡æ²«é£é™©ï¼Œå»ºè®®å…³æ³¨äº§ä¸šé“¾ä¸­ä½ä¼°å€¼æ ‡çš„ã€‚',
    r'è´¹ç‡|é™è´¹|è´§å¸åŸºé‡‘': 'æ½œåœ¨å½±å“ï¼šé™ä½æŠ•èµ„è€…æˆæœ¬ï¼Œæå‡åŸºé‡‘å¸å¼•åŠ›ï¼Œé•¿æœŸåˆ©å¥½å…¬å‹Ÿè¡Œä¸šè§„æ¨¡æ‰©å¼ ï¼Œä½†çŸ­æœŸå¯èƒ½æŒ¤å‹åŸºé‡‘å…¬å¸åˆ©æ¶¦ã€‚',
    r'é£é™©|è­¦æƒ•|è·‘è¾“|é“å¾·é£é™©': 'æ½œåœ¨å½±å“ï¼šå¢åŠ å¸‚åœºæ³¢åŠ¨æ€§ï¼ŒæŠ•èµ„è€…åº”åŠ å¼ºé£é™©ç®¡ç†ï¼Œé¿å…è¿½é«˜çƒ­é—¨èµ›é“ã€‚',
    r'ETF|å¸é‡‘|èµ„é‡‘æµå‘': 'æ½œåœ¨å½±å“ï¼šåŠ é€Ÿèµ„é‡‘å‘çƒ­é—¨ä¸»é¢˜å€¾æ–œï¼Œå¢å¼ºå¸‚åœºæµåŠ¨æ€§ï¼Œä½†å¯èƒ½æ”¾å¤§æ¿å—è½®åŠ¨æ•ˆåº”ã€‚',
    r'å®è§‚|ç­–ç•¥æŠ¥å‘Š|éœ‡è¡ä¸Šè¡Œ': 'æ½œåœ¨å½±å“ï¼šå››å­£åº¦å¸‚åœºæˆ–å‘ˆNå‹èµ°åŠ¿ï¼Œç§‘æŠ€ä¸åå†…å·æ¿å—å—ç›Šï¼Œå»ºè®®å‡è¡¡é…ç½®ã€‚',
    r'ç§å‹Ÿè§‚ç‚¹|ä¸­è¯500': 'æ½œåœ¨å½±å“ï¼šä¸­è¯500æŒ‡æ•°å¯èƒ½å¸å¼•æ›´å¤šèµ„é‡‘æµå…¥ç§‘æŠ€æˆé•¿è‚¡ï¼Œæå‡æŒ‡æ•°è¡¨ç°ã€‚',
    r'è´µé‡‘å±|é¿é™©': 'æ½œåœ¨å½±å“ï¼šåœ¨åœ°ç¼˜é£é™©ä¸‹ï¼Œè´µé‡‘å±ä½œä¸ºå¯¹å†²å·¥å…·éœ€æ±‚ä¸Šå‡ï¼Œé…ç½®ä»·å€¼æå‡ã€‚',
    r'ä¸šåŠ¡å¤šå…ƒåŒ–|å­å…¬å¸': 'æ½œåœ¨å½±å“ï¼šå…¬å‹Ÿæ‰©å±•ç§å‹Ÿè‚¡æƒç­‰é¢†åŸŸï¼Œå¢å¼ºç»¼åˆç«äº‰åŠ›ï¼Œåˆ©å¥½é•¿æœŸæŠ•èµ„è€…ã€‚',
    # é»˜è®¤æ¨¡æ¿
    r'.*': 'æ½œåœ¨å½±å“ï¼šè¯¥æ–°é—»å¯èƒ½å¯¹ç›¸å…³æ¿å—äº§ç”Ÿä¸­æ€§å½±å“ï¼Œå»ºè®®ç»“åˆå¸‚åœºåŠ¨æ€è¿›ä¸€æ­¥è¯„ä¼°ã€‚'
}

# -----------------------------------------------------------------


# --- è¾…åŠ©å‡½æ•°ï¼šæ—¶é—´è§£æå’Œæ ¼å¼åŒ– ---
def parse_and_format_time(pub_date: str) -> str:
    """è§£ææ—¶é—´å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´å¹¶æ ¼å¼åŒ–ã€‚"""
    if pub_date == 'N/A' or not pub_date:
        return 'N/A'
    try:
        dt_utc = parser.parse(pub_date).replace(tzinfo=pytz.utc)
        dt_local = dt_utc.astimezone(pytz.timezone('Asia/Shanghai'))
        return dt_local.strftime('%Y-%m-%d %H:%M:%S')
    except Exception:
        return pub_date

# --- è¾…åŠ©å‡½æ•°ï¼šHTMLæ¸…ç†å’Œæ‘˜è¦å¤„ç† ---
def clean_html_summary(summary: str, max_len: int = 400) -> str:
    """æ¸…ç†æ‘˜è¦ä¸­çš„HTMLæ ‡ç­¾å’Œå¤šä½™ç©ºæ ¼ï¼Œå¹¶è¿›è¡Œæˆªæ–­ã€‚"""
    if not summary:
        return 'æ— æ‘˜è¦'
    
    clean_soup = BeautifulSoup(summary, 'html.parser')
    clean_text = clean_soup.get_text(strip=True)
    clean_text = re.sub(r'\s+', ' ', clean_text)
    
    if len(clean_text) > max_len:
        return clean_text[:max_len] + '...'
    return clean_text

# --- æ–°å¢ï¼šè¯¦ç»†æ–°é—»åˆ†æå‡½æ•° ---
def detailed_analyze_news(item: Dict) -> Dict:
    """ä¸ºå•æ¡æ–°é—»ç”Ÿæˆè¯¦ç»†åˆ†æå’Œæ½œåœ¨å½±å“ã€‚"""
    text = item['title'] + ' ' + item['summary']
    analysis = {
        'title': item['title'],
        'detailed_summary': f"æ ‡é¢˜ï¼š{item['title']}\næ‘˜è¦ï¼š{item['summary']}",
        'key_topics': [],
        'potential_impact': ''
    }
    
    # æå–å…³é”®ä¸»é¢˜ï¼ˆåŸºäºç°æœ‰MAPæ‰©å±•ï¼‰
    for map_dict in [CLUES_MAP, LESSONS_MAP, TRENDS_MAP]:
        for pattern, desc in map_dict.items():
            if re.search(pattern, text, re.IGNORECASE):
                analysis['key_topics'].append(desc)
    
    # ç”Ÿæˆæ½œåœ¨å½±å“
    impact_found = False
    for pattern, impact in IMPACT_TEMPLATES.items():
        if re.search(pattern, text, re.IGNORECASE):
            analysis['potential_impact'] = impact
            impact_found = True
            break
    if not impact_found:
        analysis['potential_impact'] = IMPACT_TEMPLATES['.*']
    
    return analysis

# --- æ ¸å¿ƒæŠ“å–å‡½æ•°ï¼šRSS ---
def fetch_rss_feed(url: str, source_name: str, limit: int = 15) -> List[Dict]:
    """è·å–å¹¶è§£æRSS feedï¼Œè¿‡æ»¤åŒ…å«'åŸºé‡‘'ã€'å®ç›˜'ã€'è§‚ç‚¹'ç­‰å…³é”®è¯çš„æ¡ç›®ã€‚"""
    filtered_items = []
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, timeout=10, headers=headers)
        response.raise_for_status()
        
        try:
            root = ET.fromstring(response.content)
        except ET.ParseError:
            print(f"[{source_name}] Error parsing XML. Trying content decoding...")
            root = ET.fromstring(response.text.encode('utf-8'))

        items = root.findall('.//item')
        
        for item in items[:limit]:
            title = item.find('title').text if item.find('title') is not None and item.find('title').text else ''
            link = item.find('link').text if item.find('link') is not None and item.find('link').text else 'N/A'
            pub_date_raw = item.find('pubDate').text if item.find('pubDate') is not None and item.find('pubDate').text else 'N/A'
            summary_raw = item.find('description').text if item.find('description') is not None and item.find('description').text else ''
            
            summary = clean_html_summary(summary_raw, max_len=400)
            pub_date = parse_and_format_time(pub_date_raw)
            
            if re.search(r'åŸºé‡‘|å®ç›˜|è§‚ç‚¹|ç»éªŒ|æ¨è|ç­–ç•¥', title + summary, re.IGNORECASE):
                filtered_items.append({
                    'title': title.strip(),
                    'link': link.strip(),
                    'pubDate': pub_date,
                    'summary': summary,
                    'source': source_name
                })
        return filtered_items
        
    except requests.exceptions.Timeout:
        print(f"[{source_name}] Error fetching RSS {url}: Request timed out.")
    except requests.exceptions.RequestException as e:
        print(f"[{source_name}] Error fetching RSS {url}: Network or HTTP error: {e}")
    except Exception as e:
        print(f"[{source_name}] Error fetching RSS {url}: An unexpected error occurred: {e}")
    return []

# --- æ ¸å¿ƒæŠ“å–å‡½æ•°ï¼šWeb (é›ªçƒ) ---
def fetch_web_page(url: str, source_name: str, selector: str, limit: int = 15) -> List[Dict]:
    """æŠ“å–ç½‘é¡µï¼ˆä¸“ç”¨äºé›ªçƒï¼‰ï¼Œè¿‡æ»¤'åŸºé‡‘'ã€'å®ç›˜'ç­‰å…³é”®è¯ã€‚"""
    filtered_items = []
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://xueqiu.com/'
        }
        response = requests.get(url, timeout=10, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        items = soup.select(selector)
        
        for item in items[:limit]:
            title_tag = item
            if not title_tag:
                continue
            
            title = title_tag.get_text(strip=True)
            link = title_tag.get('href', '')
            
            if link and not link.startswith('http'):
                link = f"https://xueqiu.com{link}"
            
            parent = item.parent.parent
            summary_tag = parent.select_one('.search-summary, .search-snippet, .search-content')
            
            summary_raw = summary_tag.get_text(strip=True) if summary_tag else title
            summary = clean_html_summary(summary_raw, max_len=400)
            
            if re.search(r'åŸºé‡‘|å®ç›˜|è§‚ç‚¹|ç»éªŒ|æ¨è|ç­–ç•¥', title + summary, re.IGNORECASE):
                filtered_items.append({
                    'title': title,
                    'link': link if link else 'N/A',
                    'pubDate': 'N/A', 
                    'summary': summary,
                    'source': source_name
                })
        return filtered_items
        
    except requests.exceptions.Timeout:
        print(f"[{source_name}] Error fetching web {url}: Request timed out.")
    except requests.exceptions.RequestException as e:
        print(f"[{source_name}] Error fetching web {url}: Network or HTTP error: {e}")
    except Exception as e:
        print(f"[{source_name}] Error fetching web {url}: An unexpected error occurred: {e}")
    return []


# --- ã€æ ¸å¿ƒã€‘æ–°é—»åˆ†æå‡½æ•°ï¼šåŸºäºè§„åˆ™åŒ¹é…ï¼ˆä¾§é‡é€šç”¨æ€§ï¼‰ ---
def analyze_news(news_items: List[Dict]) -> Dict:
    """
    åŸºäºå…³é”®è¯å’Œè§„åˆ™åº“ï¼Œä»æ–°é—»åˆ—è¡¨ä¸­æå–æŠ•èµ„çº¿ç´¢å’Œç»éªŒæ•™è®­ã€‚
    é€»è¾‘ç®€åŒ–ä¸ºï¼šåªè¦åŒ¹é…åˆ°ä»»æ„ä¸€ä¸ªé€šç”¨æˆ–ç‰¹å®šçš„è§„åˆ™ï¼Œå°±æç‚¼è¯¥ä¿¡å·ã€‚
    æ–°å¢ï¼šä¸ºæ‰€æœ‰æ–°é—»ç”Ÿæˆè¯¦ç»†åˆ†æå’Œæ½œåœ¨å½±å“ã€‚
    """
    analysis = {
        'investment_clues': [],
        'experience_lessons': [],
        'industry_trends': [],
        'detailed_analyses': []  # æ–°å¢ï¼šæ‰€æœ‰æ–°é—»çš„è¯¦ç»†åˆ†æ
    }

    seen_clues = set()
    seen_lessons = set()
    seen_trends = set()

    for item in news_items:
        text = item['title'] + item['summary']
        
        # 1. åŒ¹é…æŠ•èµ„çº¿ç´¢
        for pattern, focus in CLUES_MAP.items():
            if re.search(pattern, text, re.IGNORECASE) and focus not in seen_clues:
                analysis['investment_clues'].append({
                    'focus': focus,
                    'title': item['title'],
                    'link': item['link'],
                })
                seen_clues.add(focus)
                
        # 2. åŒ¹é…ç»éªŒæ•™è®­
        for pattern, lesson in LESSONS_MAP.items():
            if re.search(pattern, text, re.IGNORECASE) and lesson not in seen_lessons:
                analysis['experience_lessons'].append({
                    'lesson': lesson,
                    'title': item['title'],
                    'link': item['link'],
                })
                seen_lessons.add(lesson)

        # 3. åŒ¹é…è¡Œä¸šè¶‹åŠ¿
        for pattern, trend in TRENDS_MAP.items():
            if re.search(pattern, text, re.IGNORECASE) and trend not in seen_trends:
                analysis['industry_trends'].append({
                    'trend': trend,
                    'title': item['title'],
                    'link': item['link'],
                })
                seen_trends.add(trend)
        
        # æ–°å¢ï¼šä¸ºæ‰€æœ‰æ–°é—»ç”Ÿæˆè¯¦ç»†åˆ†æ
        detailed = detailed_analyze_news(item)
        analysis['detailed_analyses'].append(detailed)
                
    return analysis

# --- ç”Ÿæˆåˆ†ææŠ¥å‘Š ---
def generate_analysis_report(analysis: Dict, total_count: int) -> str:
    """æ ¹æ®åˆ†æç»“æœç”Ÿæˆç»“æ„åŒ– Markdown æŠ¥å‘Šã€‚æ–°å¢è¯¦ç»†åˆ†æéƒ¨åˆ†ã€‚"""
    md_report = "\n---\n"
    md_report += "# ğŸ“° åŸºé‡‘æŠ•èµ„ç­–ç•¥åˆ†ææŠ¥å‘Š\n\n"
    md_report += f"æœ¬æŠ¥å‘Šæ ¹æ®ä» {total_count} æ¡æ–°é—»ä¸­æå–çš„é«˜ä»·å€¼ä¿¡æ¯ç”Ÿæˆï¼Œæ—¨åœ¨ä¸ºæ‚¨æä¾› **ä¹°å…¥æŒ‡å¼•ã€é£é™©è§„é¿å’Œè¡Œä¸šæ´å¯Ÿ**ã€‚\n\n"

    # 1. æŠ•èµ„çº¿ç´¢
    md_report += "## ğŸ’° æŠ•èµ„çº¿ç´¢ä¸å¸‚åœºç„¦ç‚¹ (ä¹°å…¥æŒ‡å¼•)\n"
    if analysis['investment_clues']:
        md_report += "| ç„¦ç‚¹æ ‡çš„/ç­–ç•¥ | åŸå§‹æ ‡é¢˜ (ç‚¹å‡»æŸ¥çœ‹) |\n"
        md_report += "| :--- | :--- |\n"
        
        for clue in analysis['investment_clues']:
            md_report += f"| **{clue['focus']}** | [{clue['title']}](<{clue['link']}>) |\n"
    else:
        md_report += "æš‚æ— æ˜ç¡®çš„æŠ•èµ„çº¿ç´¢æˆ–æœºæ„è§‚ç‚¹è¢«è¯†åˆ«ã€‚\n"
        
    # 2. ç»éªŒä¸æ•™è®­
    md_report += "\n## âš ï¸ æŠ•èµ„ç»éªŒä¸é£é™©è§„é¿ (é¿å…è¸©å‘)\n"
    if analysis['experience_lessons']:
        md_report += "| æ•™è®­/ç»éªŒ | åŸå§‹æ ‡é¢˜ (ç‚¹å‡»æŸ¥çœ‹) |\n"
        md_report += "| :--- | :--- |\n"
        
        for lesson in analysis['experience_lessons']:
            md_report += f"| **{lesson['lesson']}** | [{lesson['title']}](<{lesson['link']}>) |\n"
    else:
        md_report += "æš‚æ— æ˜ç¡®çš„ç»éªŒæ•™è®­æˆ–é£é™©æç¤ºè¢«è¯†åˆ«ã€‚\n"

    # 3. è¡Œä¸šç»“æ„ä¸è¶‹åŠ¿
    md_report += "\n## âœ¨ è¡Œä¸šç»“æ„ä¸æœªæ¥è¶‹åŠ¿ (é•¿æœŸæ´å¯Ÿ)\n"
    if analysis['industry_trends']:
        md_report += "| è¡Œä¸šè¶‹åŠ¿ | åŸå§‹æ ‡é¢˜ (ç‚¹å‡»æŸ¥çœ‹) |\n"
        md_report += "| :--- | :--- |\n"
        
        for trend in analysis['industry_trends']:
            md_report += f"| **{trend['trend']}** | [{trend['title']}](<{trend['link']}>) |\n"
    else:
        md_report += "æš‚æ— æ˜ç¡®çš„è¡Œä¸šè¶‹åŠ¿æˆ–ç»“æ„å˜åŒ–è¢«è¯†åˆ«ã€‚\n"

    # æ–°å¢ï¼šè¯¦ç»†æ–°é—»åˆ†æä¸æ½œåœ¨å½±å“
    md_report += "\n## ğŸ” æ‰€æœ‰æ–°é—»è¯¦ç»†åˆ†æä¸æ½œåœ¨å½±å“\n"
    if analysis['detailed_analyses']:
        md_report += "| æ–°é—»æ ‡é¢˜ | å…³é”®ä¸»é¢˜ | æ½œåœ¨å½±å“ |\n"
        md_report += "| :--- | :--- | :--- |\n"
        
        for det in analysis['detailed_analyses']:
            topics_str = '; '.join(det['key_topics']) if det['key_topics'] else 'æ— ç‰¹å®šä¸»é¢˜'
            md_report += f"| {det['title']} | {topics_str} | **{det['potential_impact']}** |\n"
    else:
        md_report += "æš‚æ— è¯¦ç»†åˆ†æã€‚\n"

    return md_report


# --- æ•°æ®æºé…ç½®å¤–éƒ¨åŒ– (ä¿æŒä¸å˜ï¼Œå¹¶æ‰©å±•æ–°æ¥æº) ---
proxy_base = 'https://rsshub.rss.zgdnz.cc'
sources = [
    {'url': f'{proxy_base}/cls/telegraph/fund', 'name': 'è´¢è”ç¤¾-åŸºé‡‘ç”µæŠ¥', 'type': 'rss'},
    {'url': f'{proxy_base}/eastmoney/report/strategyreport', 'name': 'ä¸œæ–¹è´¢å¯Œ-ç­–ç•¥æŠ¥å‘Š', 'type': 'rss'},
    {'url': f'{proxy_base}/gelonghui/home/fund', 'name': 'æ ¼éš†æ±‡-åŸºé‡‘', 'type': 'rss'},
    {'url': f'{proxy_base}/stcn/article/list/fund', 'name': 'è¯åˆ¸æ—¶æŠ¥-åŸºé‡‘åˆ—è¡¨', 'type': 'rss'},
    {'url': f'{proxy_base}/21caijing/channel/%E8%AF%81%E5%88%B8/%E8%B5%A2%E5%9F%BA%E9%87%91', 'name': '21è´¢ç»-èµ¢åŸºé‡‘', 'type': 'rss'},
    {
        'url': 'https://xueqiu.com/k?q=%E5%9F%BA%E9%87%91',
        'name': 'é›ªçƒ-åŸºé‡‘æœç´¢',
        'type': 'web',
        'selector': '.search__list .search-result-item .search-title a' 
    },
    # æ–°å¢æ‰©å±•æ¥æºï¼šç¤¾åŒºå’Œä¸ªäººåšå®¢ï¼ˆåŸºäºå¯ç”¨æµ‹è¯•ï¼‰
    {'url': f'{proxy_base}/xueqiu/fund', 'name': 'é›ªçƒ-åŸºé‡‘RSS', 'type': 'rss'},  # é›ªçƒåŸºé‡‘ç¤¾åŒºRSS
    {'url': f'{proxy_base}/zhihu/topic/19550517', 'name': 'çŸ¥ä¹-åŸºé‡‘è¯é¢˜', 'type': 'rss'},  # çŸ¥ä¹åŸºé‡‘ä¸“æ ç¤¾åŒº
    {'url': 'https://dbarobin.com/rss.xml', 'name': 'åŒºå—é“¾ç½—å®¾-æŠ•èµ„åšå®¢', 'type': 'rss'}  # ä¸ªäººæŠ•èµ„åšå®¢ï¼ˆåŒºå—é“¾/åŸºé‡‘ç›¸å…³ï¼‰
]

def generate_markdown(news_items: List[Dict], analysis_report: str, timestamp_str: str) -> str:
    """
    ç”ŸæˆMarkdownã€‚åœ¨æ–°é—»åˆ—è¡¨å‰æ’å…¥åˆ†ææŠ¥å‘Šã€‚
    """
    md_content = f"# åŸºé‡‘æ–°é—»èšåˆ ({timestamp_str})\n\n"
    configured_sources = list(set([s['name'].split('-')[0] for s in globals().get('sources', [])]))
    source_names = "ã€".join(configured_sources)
    md_content += f"æ¥æºï¼š{source_names}ï¼ˆå…³é”®è¯ï¼šåŸºé‡‘/å®ç›˜/è§‚ç‚¹/ç»éªŒ/æ¨è/ç­–ç•¥ï¼‰ã€‚æ€»è®¡ {len(news_items)} æ¡ã€‚\n"
    
    # æ’å…¥åˆ†ææŠ¥å‘Š
    md_content += analysis_report
    
    # æ’å…¥åŸå§‹æ–°é—»åˆ—è¡¨
    md_content += "\n---\n# åŸå§‹æ–°é—»åˆ—è¡¨\n\n"
    for i, item in enumerate(news_items, 1):
        md_content += f"## {i}. {item['title']} ({item['source']})\n"
        md_content += f"- **é“¾æ¥**: [{item['link']}]({item['link']})\n"
        md_content += f"- **æ—¶é—´**: {item['pubDate']}\n"
        md_content += f"- **æ‘˜è¦**: {item['summary']}\n\n"
    return md_content

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°ï¼Œåè°ƒæŠ“å–ã€åˆ†æã€å»é‡å’Œæ–‡ä»¶ç”Ÿæˆã€‚"""
    
    # --- å…³é”®ä¿®æ”¹ 1: è·å–å¸¦æ—¥æœŸæ—¶é—´æˆ³çš„æ–‡ä»¶å ---
    # ä½¿ç”¨å½“å‰åŒ—äº¬æ—¶é—´ä½œä¸ºæ—¶é—´æˆ³ï¼Œç”¨äºå‘½åå’ŒæŠ¥å‘Šæ ‡é¢˜
    tz = pytz.timezone('Asia/Shanghai')
    now = datetime.now(tz)
    timestamp_str = now.strftime('%Y-%m-%d %H:%M:%S')
    # æ–‡ä»¶åä½¿ç”¨ YYYYMMDD æ ¼å¼ï¼Œé¿å…è¦†ç›–
    date_str = now.strftime('%Y%m%d') 
    output_file = f'fund_news_{date_str}.md'
    # --------------------------------------------------
    
    all_news = []
    print(f"[{timestamp_str}] å¼€å§‹æŠ“å–åŸºé‡‘æ–°é—» (å·²æ‰©å±•å…³é”®è¯å’Œæ·±åº¦)...")
    
    for source in sources:
        print(f"å¤„ç†æ¥æº: {source['name']} ({source['url']})")
        if source['type'] == 'rss':
            items = fetch_rss_feed(source['url'], source['name'], limit=15)
        else:
            # è¿™é‡Œçš„ source_name å˜é‡éœ€è¦ä» source å­—å…¸ä¸­è·å–
            items = fetch_web_page(source['url'], source['name'], source.get('selector'), limit=15)
        all_news.extend(items)
    
    # å»é‡
    unique_news = []
    seen_links = set()
    for news in all_news:
        if news['link'] and news['link'] != 'N/A' and news['link'] not in seen_links:
            seen_links.add(news['link'])
            unique_news.append(news)
        # å¦‚æœé“¾æ¥ä¸å¯ç”¨ï¼Œåˆ™æ ¹æ®æ ‡é¢˜å’Œæ¥æºè¿›è¡Œå»é‡
        elif news['link'] == 'N/A' and (news['title'], news['source']) not in seen_links:
             seen_links.add((news['title'], news['source']))
             unique_news.append(news)

    # æ’åºï¼šæŒ‰æ—¶é—´å€’åºæ’åˆ—
    def sort_key(item):
        time_str = item['pubDate']
        if time_str and time_str != 'N/A':
            try:
                return datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
            except ValueError:
                pass
        return datetime(1900, 1, 1)

    unique_news.sort(key=sort_key, reverse=True)
    
    # ã€æ ¸å¿ƒã€‘è¿è¡Œåˆ†æ
    analysis_results = analyze_news(unique_news)
    analysis_report_md = generate_analysis_report(analysis_results, len(unique_news))
    
    # ç”ŸæˆMD
    # å…³é”®ä¿®æ”¹ 2: ä¼ å…¥ timestamp_str åˆ° generate_markdown
    md_content = generate_markdown(unique_news, analysis_report_md, timestamp_str)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    print(f"æ”¶é›†åˆ° {len(unique_news)} æ¡ç‹¬ç‰¹åŸºé‡‘æ–°é—»ã€‚åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆå¹¶ä¿å­˜è‡³ {output_file}")
    
    print("\n--- åˆ†ææŠ¥å‘Šæ‘˜è¦ ---")
    print(analysis_report_md.split('## ğŸ’°')[0])

if __name__ == "__main__":
    main()
