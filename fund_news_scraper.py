import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from typing import List, Dict
import re
import xml.etree.ElementTree as ET
from dateutil import parser
import pytz
import matplotlib.pyplot as plt
from collections import Counter
import sqlite3
import json # ç”¨äºåœ¨ SQLite ä¸­å­˜å‚¨ List/Dict å­—æ®µ

# --- ã€æ ¸å¿ƒé…ç½®ã€‘åˆ†æè§„åˆ™åº“ï¼šé€šç”¨ã€å¯æ‰©å±•çš„åˆ†æé€»è¾‘ ---

# 1. æŠ•èµ„çº¿ç´¢ (æ ‡çš„/ç­–ç•¥ -> æ€»ç»“)
CLUES_MAP = {
    # ç­–ç•¥é€šç”¨è¯ï¼šè¯†åˆ«ä»»ä½•è¡¨è¾¾æ˜ç¡®ä¹°å…¥/çœ‹å¥½/é…ç½®æ„å›¾çš„æ–‡ç«  (å¼ºåŒ–é€šç”¨è¯ï¼ŒæŠ“ä½è¡Œä¸º)
    r'çœ‹å¥½|å»ºè®®é…ç½®|ç­–ç•¥ä¸»çº¿|èšç„¦|å¸ƒå±€|æ¨è|é‡‘è‚¡|å®ç›˜': 'ã€é€šç”¨ç­–ç•¥ä¿¡å·ã€‘è¯†åˆ«åˆ°æ˜ç¡®çš„é…ç½®å»ºè®®æˆ–ç­–ç•¥ä¸»çº¿',
    
    # å®è§‚ç­–ç•¥ä¸å‘¨æœŸ
    r'å®è§‚|ç­–ç•¥æŠ¥å‘Š|å‘¨æœŸ|å››ä¸­å…¨ä¼š|åäº”äº”': 'ã€å®è§‚ç­–ç•¥ä¿¡å·ã€‘å®è§‚æˆ–å‘¨æœŸæ€§ä¸»é¢˜æŠ¥å‘Š',
    
    # ç‰¹æŒ‡ä¿¡å·ï¼šä¿ç•™é«˜ä»·å€¼äººç‰©/æœºæ„çš„å…³é”®è¯ï¼Œä½†å…¶å®šä½æ˜¯â€œä¿¡å·â€ï¼Œè€Œéç‰¹å®šäºº
    r'æè““|åŠå¤|ä¸­è¯500|IC': 'ç§å‹Ÿè§‚ç‚¹ï¼šä¸­è¯500/ç§‘æŠ€æˆé•¿ç­–ç•¥',
    r'åå®‰è¯åˆ¸|æˆé•¿äº§ä¸š|AI|å†›å·¥': 'åˆ¸å•†è§‚ç‚¹ï¼šAI/å†›å·¥/æ–°æˆé•¿äº§ä¸šé“¾é…ç½®',
    r'å¼€æºè¯åˆ¸|é‡‘è‚¡ç­–ç•¥|ç§‘æŠ€|æ¸¯è‚¡': 'åˆ¸å•†è§‚ç‚¹ï¼šAI+è‡ªä¸»å¯æ§ç§‘æŠ€ä¸»çº¿',
    r'ETF|è‚¡ç¥¨ETF|ç™¾äº¿ä¿±ä¹éƒ¨|å¸é‡‘': 'èµ„é‡‘æµå‘/è‚¡ç¥¨ETF/å¸é‡‘èµ›é“',
    r'è´µé‡‘å±|é»„é‡‘|é¿é™©': 'èµ„äº§å¯¹å†²/é¿é™©é…ç½® (è´µé‡‘å±)',
    r'å‡è¡¡é…ç½®|å…‰ä¼|åŒ–å·¥|å†œä¸š|æœ‰è‰²|é“¶è¡Œ': 'ä½ä½/å‡è¡¡æ¿å—é…ç½®å»ºè®®',
    # æ–°å¢è§„åˆ™ä»¥ä¸°å¯Œå†…å®¹
    r'ç§‘æŠ€è‚¡|AIç®—åŠ›|äººå½¢æœºå™¨äºº': 'ç§‘æŠ€åˆ›æ–°/AIé©±åŠ¨äº§ä¸šé“¾æœºä¼š',
    r'æ¶ˆè´¹|ETF|ç§‘æŠ€èåˆ': 'æ¶ˆè´¹ç§‘æŠ€èåˆé…ç½®',
    r'å…¬å‹Ÿè§„æ¨¡|çªç ´|å¢é•¿': 'å…¬å‹Ÿè¡Œä¸šè§„æ¨¡æ‰©å¼ ä¿¡å·',
}

# 2. ç»éªŒæ•™è®­ (è¡Œä¸º/ç»“æœ -> é£é™©/æ•™è®­)
LESSONS_MAP = {
    r'è­¦æƒ•|é£é™©|æ•™è®­|æ¶‰èµŒ|è·‘è¾“|å†…æ§': 'ã€é€šç”¨é£é™©ä¿¡å·ã€‘è¯†åˆ«åˆ°è¡Œä¸šé£é™©æˆ–è´Ÿé¢ç»éªŒæ•™è®­',
    r'è·‘è¾“å¤§ç›˜|æœªèƒ½æ»¡ä»“|çº¢åˆ©æ¿å—': 'æ–°åŸºé‡‘å»ºä»“ç­–ç•¥ä¸å¸‚åœºé”™é…é£é™©',
    r'åŸºé‡‘ç»ç†|æ¶‰èµŒ|å…èŒ': 'åŸºé‡‘ç»ç†é“å¾·é£é™©ä¸å…¬å¸å†…æ§è­¦ç¤º',
    r'æœºæ„å¤§ä¸¾å¢æŒ|ä¸»åŠ¨æƒç›ŠåŸºé‡‘': 'æœºæ„è¡Œä¸ºï¼šä¸»åŠ¨æƒç›ŠåŸºé‡‘ä»æ˜¯é…ç½®é‡ç‚¹',
    # æ–°å¢è§„åˆ™
    r'ä¼ªæˆé•¿|æ‹¥æŒ¤|é™·é˜±': 'æˆé•¿èµ›é“æ‹¥æŒ¤ä¸ä¼ªæˆé•¿é£é™©',
    r'å‡æŒ|é«˜ä½': 'è‚¡ä¸œå‡æŒä¸é«˜ä½å›è°ƒé£é™©',
}

# 3. è¡Œä¸šè¶‹åŠ¿ (ç»“æ„å˜åŒ– -> è¡Œä¸šæ´å¯Ÿ)
TRENDS_MAP = {
    r'AI|æŠ•ç ”|å·¥ä¸šåŒ–|èš‚èšè´¢å¯Œ': 'è¡Œä¸šè¶‹åŠ¿ï¼šæŠ•ç ”å·¥ä¸šåŒ–å’ŒAIèµ‹èƒ½',
    r'è´¹ç‡|ä¸‹è°ƒ|æ‰˜ç®¡è´¹|ä½™é¢å®': 'è¡Œä¸šè¶‹åŠ¿ï¼šå…³æ³¨è´¹ç‡æˆæœ¬çš„é•¿æœŸä¸‹è¡Œ',
    r'ç§å‹Ÿè‚¡æƒ|å­å…¬å¸|å¹¿å‘åŸºé‡‘': 'è¡Œä¸šè¶‹åŠ¿ï¼šå¤´éƒ¨å…¬å‹Ÿçš„ä¸šåŠ¡å¤šå…ƒåŒ–',
    r'é‡åŒ–åŸºé‡‘ç»ç†|ä¸»åŠ¨åŸºé‡‘|ä¸€æ‹–å¤š': 'è¡Œä¸šè¶‹åŠ¿ï¼šé‡åŒ–ä¸ä¸»åŠ¨æŠ•èµ„è¾¹ç•Œæ¨¡ç³Š',
    # æ–°å¢è§„åˆ™
    r'REITs|è·æ‰¹|åŸºç¡€è®¾æ–½': 'REITså¸‚åœºæ‰©å¼ ä¸åŸºç¡€è®¾æ–½æŠ•èµ„è¶‹åŠ¿',
    r'ESG|å‡æ’|ç»¿è‰²é‡‘è': 'ESGä¸ç»¿è‰²æŠ•èµ„è¶‹åŠ¿',
    r'å…»è€|ç¬¬ä¸‰æ”¯æŸ±': 'å…»è€åŸºé‡‘ä¸é•¿æœŸæŠ•èµ„ä½“ç³»å»ºè®¾',
}

# èšåˆæ‰€æœ‰ä¸»é¢˜ï¼Œç”¨äºé•¿æœŸè¶‹åŠ¿åˆ†æ
ALL_TOPICS_MAP = {**CLUES_MAP, **LESSONS_MAP, **TRENDS_MAP}


# æ–°å¢ï¼šæ½œåœ¨å½±å“æ¨¡æ¿åº“ (åŸºäºå…³é”®è¯ç”Ÿæˆæ–°é—»å½±å“æ€»ç»“)
IMPACT_TEMPLATES = {
    r'AI|ç®—åŠ›|ç§‘æŠ€é¾™å¤´|äº§ä¸šé“¾': 'æ½œåœ¨å½±å“ï¼šå¯èƒ½æ¨åŠ¨ç§‘æŠ€æ¿å—çŸ­æœŸä¸Šæ¶¨ï¼Œä½†éœ€è­¦æƒ•ä¼°å€¼æ³¡æ²«é£é™©ï¼Œå»ºè®®å…³æ³¨äº§ä¸šé“¾ä¸­ä½ä¼°å€¼æ ‡çš„ã€‚',
    r'è´¹ç‡|é™è´¹|è´§å¸åŸºé‡‘': 'æ½œåœ¨å½±å“ï¼šé™ä½æŠ•èµ„è€…æˆæœ¬ï¼Œæå‡åŸºé‡‘å¸å¼•åŠ›ï¼Œé•¿æœŸåˆ©å¥½å…¬å‹Ÿè¡Œä¸šè§„æ¨¡æ‰©å¼ ï¼Œä½†çŸ­æœŸå¯èƒ½æŒ¤å‹åŸºé‡‘å…¬å¸åˆ©æ¶¦ã€‚',
    r'é£é™©|è­¦æƒ•|è·‘è¾“|é“å¾·é£é™©': 'æ½œåœ¨å½±å“ï¼šå¢åŠ å¸‚åœºæ³¢åŠ¨æ€§ï¼ŒæŠ•èµ„è€…åº”åŠ å¼ºé£é™©ç®¡ç†ï¼Œé¿å…è¿½é«˜çƒ­é—¨èµ›é“ã€‚',
    r'ETF|å¸é‡‘|èµ„é‡‘æµå‘': 'æ½œåœ¨å½±å“ï¼šåŠ é€Ÿèµ„é‡‘å‘çƒ­é—¨ä¸»é¢˜å€¾æ–œï¼Œå¢å¼ºå¸‚åœºæµåŠ¨æ€§ï¼Œä½†å¯èƒ½æ”¾å¤§æ¿å—è½®åŠ¨æ•ˆåº”ã€‚',
    r'å®è§‚|ç­–ç•¥æŠ¥å‘Š|éœ‡è¡ä¸Šè¡Œ': 'æ½œåœ¨å½±å“ï¼šå››å­£åº¦å¸‚åœºæˆ–å‘ˆNå‹èµ°åŠ¿ï¼Œç§‘æŠ€ä¸åå†…å·æ¿å—å—ç›Šï¼Œå»ºè®®å‡è¡¡é…ç½®ã€‚',
    r'ç§å‹Ÿè§‚ç‚¹|ä¸­è¯500': 'æ½œåœ¨å½±å“ï¼šä¸­è¯500æŒ‡æ•°å¯èƒ½å¸å¼•æ›´å¤šèµ„é‡‘æµå…¥ç§‘æŠ€æˆé•¿è‚¡ï¼Œæå‡æŒ‡æ•°è¡¨ç°ã€‚',
    r'è´µé‡‘å±|é¿é™©': 'æ½œåœ¨å½±å“ï¼šåœ¨åœ°ç¼˜é£é™©ä¸‹ï¼Œè´µé‡‘å±ä½œä¸ºå¯¹å†²å·¥å…·éœ€æ±‚ä¸Šå‡ï¼Œé…ç½®ä»·å€¼æå‡ã€‚',
    r'ä¸šåŠ¡å¤šå…ƒåŒ–|å­å…¬å¸': 'æ½œåœ¨å½±å“ï¼šå…¬å‹Ÿæ‰©å±•ç§å‹Ÿè‚¡æƒç­‰é¢†åŸŸï¼Œå¢å¼ºç»¼åˆç«äº‰åŠ›ï¼Œåˆ©å¥½é•¿æœŸæŠ•èµ„è€…ã€‚',
    # æ–°å¢æ¨¡æ¿
    r'REITs|è·æ‰¹': 'æ½œåœ¨å½±å“ï¼šREITsè·æ‰¹å°†æ³¨å…¥æ–°æ´»åŠ›ï¼Œä¿ƒè¿›åŸºç¡€è®¾æ–½æŠ•èµ„ï¼Œå¸å¼•æ›´å¤šèµ„é‡‘è¿›å…¥ç›¸å…³é¢†åŸŸã€‚',
    r'ESG|å‡æ’': 'æ½œåœ¨å½±å“ï¼šESGæ”¿ç­–å¼ºåŒ–å°†æ¨åŠ¨ç»¿è‰²è½¬å‹ï¼Œåˆ©å¥½å¯æŒç»­æŠ•èµ„ä¸»é¢˜åŸºé‡‘ã€‚',
    r'å…»è€|ç¬¬ä¸‰æ”¯æŸ±': 'æ½œåœ¨å½±å“ï¼šå…»è€ä½“ç³»å®Œå–„å°†å¢åŠ é•¿æœŸèµ„é‡‘ä¾›ç»™ï¼Œç¨³å®šèµ„æœ¬å¸‚åœºã€‚',
    # é»˜è®¤æ¨¡æ¿
    r'.*': 'æ½œåœ¨å½±å“ï¼šè¯¥æ–°é—»å¯èƒ½å¯¹ç›¸å…³æ¿å—äº§ç”Ÿä¸­æ€§å½±å“ï¼Œå»ºè®®ç»“åˆå¸‚åœºåŠ¨æ€è¿›ä¸€æ­¥è¯„ä¼°ã€‚'
}

# æ–°å¢ï¼šç®€å•æƒ…æ„Ÿåˆ†æå…³é”®è¯
POSITIVE_WORDS = ['çœ‹å¥½', 'ä¸Šæ¶¨', 'å¢é•¿', 'æœºä¼š', 'å¸ƒå±€', 'æ¨è']
NEGATIVE_WORDS = ['é£é™©', 'è­¦æƒ•', 'è·‘è¾“', 'å‡æŒ', 'æ•™è®­', 'é™·é˜±']

# -----------------------------------------------------------------


# --- æ•°æ®åº“ç®¡ç†ç±» ---
class DatabaseManager:
    def __init__(self, db_name='fund_news_analysis.db'):
        self.db_name = db_name
        self.conn = None
        self._connect()
        self._create_table()

    def _connect(self):
        self.conn = sqlite3.connect(self.db_name)
        self.conn.row_factory = sqlite3.Row # å…è®¸æŒ‰åˆ—åè®¿é—®
        
    def _create_table(self):
        cursor = self.conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS analyzed_news (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                link TEXT UNIQUE,
                pubDate TEXT,
                source TEXT,
                crawl_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                sentiment TEXT,
                key_topics_json TEXT -- å­˜å‚¨ä¸»é¢˜åˆ—è¡¨çš„ JSON å­—ç¬¦ä¸²
            )
        ''')
        self.conn.commit()

    def get_existing_links(self) -> set:
        """ä»æ•°æ®åº“è·å–æ‰€æœ‰å·²å­˜åœ¨çš„é“¾æ¥ï¼Œç”¨äºè·¨æ¬¡è¿è¡Œå»é‡ã€‚"""
        cursor = self.conn.cursor()
        cursor.execute("SELECT link FROM analyzed_news WHERE link IS NOT NULL AND link != 'N/A'")
        return {row['link'] for row in cursor.fetchall()}

    def store_news_and_analysis(self, news_item: Dict, analysis_result: Dict):
        """å­˜å‚¨å•æ¡æ–°é—»åŠå…¶åˆ†æç»“æœã€‚"""
        # æå–å…³é”®å­—æ®µ
        title = news_item['title']
        link = news_item.get('link', 'N/A')
        pubDate = news_item.get('pubDate', datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d %H:%M:%S'))
        source = news_item['source']
        
        # æå–åˆ†æç»“æœ
        sentiment = analysis_result.get('sentiment', 'ä¸­æ€§ (Neutral)')
        key_topics_json = json.dumps(analysis_result.get('key_topics', []))
        
        cursor = self.conn.cursor()
        try:
            # æ’å…¥æˆ–æ›¿æ¢ï¼Œä»¥å¤„ç†æ— é“¾æ¥çš„é‡å¤æ ‡é¢˜
            cursor.execute('''
                INSERT INTO analyzed_news (title, link, pubDate, source, sentiment, key_topics_json)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (title, link, pubDate, source, sentiment, key_topics_json))
            self.conn.commit()
        except sqlite3.IntegrityError:
            # é“¾æ¥é‡å¤æ—¶ï¼Œå¿½ç•¥è¯¥æ¡ç›®
            pass
        except Exception as e:
            print(f"Error storing news to DB: {e}")

    def get_topics_by_time_range(self, days: int) -> Dict[str, int]:
        """è·å–è¿‡å» N å¤©çš„ä¸»é¢˜è®¡æ•°ã€‚"""
        since_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d %H:%M:%S')
        cursor = self.conn.cursor()
        
        # ä»…ä½¿ç”¨ pubDate å­—æ®µè¿›è¡Œæ—¶é—´è¿‡æ»¤ï¼Œå› ä¸ºå®ƒç»è¿‡æ ‡å‡†åŒ–å¤„ç†
        cursor.execute("""
            SELECT key_topics_json
            FROM analyzed_news
            WHERE pubDate > ?
        """, (since_date,))
        
        topic_counter = Counter()
        for row in cursor.fetchall():
            try:
                topics = json.loads(row['key_topics_json'])
                topic_counter.update(topics)
            except (json.JSONDecodeError, TypeError):
                continue
        
        return dict(topic_counter)

    def close(self):
        if self.conn:
            self.conn.close()

# --- è¾…åŠ©å‡½æ•°ï¼šæ—¶é—´è§£æå’Œæ ¼å¼åŒ– ---
def parse_and_format_time(pub_date: str) -> str:
    """è§£ææ—¶é—´å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´å¹¶æ ¼å¼åŒ–ã€‚"""
    if pub_date == 'N/A' or not pub_date:
        return 'N/A'
    try:
        # å°è¯•è§£æ pub_dateï¼Œå‡å®šå®ƒå¯èƒ½æ˜¯ UTC æˆ–åŒ…å«æ—¶åŒºä¿¡æ¯
        dt_utc = parser.parse(pub_date).replace(tzinfo=pytz.utc)
        dt_local = dt_utc.astimezone(pytz.timezone('Asia/Shanghai'))
        return dt_local.strftime('%Y-%m-%d %H:%M:%S')
    except Exception:
        return pub_date

# --- è¾…åŠ©å‡½æ•°ï¼šHTMLæ¸…ç†å’Œæ‘˜è¦å¤„ç† ---
def clean_html_summary(summary: str, max_len: int = 400) -> str:
    """æ¸…ç†æ‘˜è¦ä¸­çš„HTMLæ ‡ç­¾å’Œå¤šä½™ç©ºæ ¼ï¼Œå¹¶è¿›è¡Œæˆªæ–­ã€‚"""
    if not summary:
        return 'æ— æ‘˜è¦'
    
    clean_soup = BeautifulSoup(summary, 'html.parser')
    clean_text = clean_soup.get_text(strip=True)
    clean_text = re.sub(r'\s+', ' ', clean_text)
    
    if len(clean_text) > max_len:
        return clean_text[:max_len] + '...'
    return clean_text

# --- æ–°å¢ï¼šç®€å•æƒ…æ„Ÿåˆ†æå‡½æ•° ---
def simple_sentiment_analysis(text: str) -> str:
    """åŸºäºå…³é”®è¯çš„ç®€å•æƒ…æ„Ÿåˆ†æã€‚"""
    pos_count = sum(1 for word in POSITIVE_WORDS if re.search(word, text, re.IGNORECASE))
    neg_count = sum(1 for word in NEGATIVE_WORDS if re.search(word, text, re.IGNORECASE))
    if pos_count > neg_count:
        return 'æ­£é¢ (Positive)'
    elif neg_count > pos_count:
        return 'è´Ÿé¢ (Negative)'
    else:
        return 'ä¸­æ€§ (Neutral)'

# --- æ–°å¢ï¼šè¯¦ç»†æ–°é—»åˆ†æå‡½æ•° ---
def detailed_analyze_news(item: Dict) -> Dict:
    """ä¸ºå•æ¡æ–°é—»ç”Ÿæˆè¯¦ç»†åˆ†æå’Œæ½œåœ¨å½±å“ã€‚"""
    text = item['title'] + ' ' + item['summary']
    analysis = {
        'title': item['title'],
        'detailed_summary': f"æ ‡é¢˜ï¼š{item['title']}\næ‘˜è¦ï¼š{item['summary']}",
        'key_topics': [],
        'potential_impact': '',
        'sentiment': simple_sentiment_analysis(text)Â  
    }
    
    # æå–å…³é”®ä¸»é¢˜ï¼ˆåŸºäºç°æœ‰MAPæ‰©å±•ï¼‰
    for map_dict in [CLUES_MAP, LESSONS_MAP, TRENDS_MAP]:
        for pattern, desc in map_dict.items():
            if re.search(pattern, text, re.IGNORECASE):
                analysis['key_topics'].append(desc)
    
    # ç”Ÿæˆæ½œåœ¨å½±å“
    impact_found = False
    for pattern, impact in IMPACT_TEMPLATES.items():
        if re.search(pattern, text, re.IGNORECASE):
            analysis['potential_impact'] = impact
            impact_found = True
            break
    if not impact_found:
        analysis['potential_impact'] = IMPACT_TEMPLATES['.*']
    
    return analysis

# --- æ ¸å¿ƒæŠ“å–å‡½æ•°ï¼šRSS ---
def fetch_rss_feed(url: str, source_name: str, limit: int = 20) -> List[Dict]:
    """è·å–å¹¶è§£æRSS feedï¼Œè¿‡æ»¤åŒ…å«'åŸºé‡‘'ã€'å®ç›˜'ã€'è§‚ç‚¹'ç­‰å…³é”®è¯çš„æ¡ç›®ã€‚"""
    filtered_items = []
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        
        # å°è¯•ç›´æ¥è¯·æ±‚
        response = requests.get(url, timeout=10, headers=headers)
        response.raise_for_status()
        
        try:
            root = ET.fromstring(response.content)
        except ET.ParseError:
            print(f"[{source_name}] Error parsing XML. Trying content decoding...")
            root = ET.fromstring(response.text.encode('utf-8'))

        items = root.findall('.//item') or root.findall('.//entry') # å…¼å®¹ RSS å’Œ Atom
        
        # ç»Ÿä¸€å¤„ç†
        for item in items[:limit]:
            title_element = item.find('title')
            link_element = item.find('link')
            pub_date_element = item.find('pubDate') or item.find('{http://purl.org/dc/elements/1.1/}date') or item.find('published')
            summary_element = item.find('description') or item.find('summary') or item.find('content')

            title = title_element.text.strip() if title_element is not None and title_element.text else ''
            
            # é“¾æ¥å¤„ç† (RSS <link> vs Atom <link href="...">)
            link = 'N/A'
            if link_element is not None:
                if link_element.text: # Standard RSS
                    link = link_element.text.strip()
                elif link_element.attrib.get('href'): # Atom/Other formats
                    link = link_element.attrib['href'].strip()

            pub_date_raw = pub_date_element.text.strip() if pub_date_element is not None and pub_date_element.text else 'N/A'
            summary_raw = summary_element.text.strip() if summary_element is not None and summary_element.text else ''
            
            summary = clean_html_summary(summary_raw, max_len=400)
            pub_date = parse_and_format_time(pub_date_raw)
            
            # å¢åŠ å¯¹æ›´é€šç”¨å…³é”®è¯ï¼ˆå¦‚è‚¡ç¥¨ã€æŠ•èµ„ï¼‰çš„è¿‡æ»¤
            if re.search(r'åŸºé‡‘|å®ç›˜|è§‚ç‚¹|ç»éªŒ|æ¨è|ç­–ç•¥|æŠ•èµ„|è‚¡ç¥¨|å®è§‚|é‡‘è', title + summary, re.IGNORECASE):
                filtered_items.append({
                    'title': title,
                    'link': link,
                    'pubDate': pub_date,
                    'summary': summary,
                    'source': source_name
                })
        return filtered_items
        
    except requests.exceptions.Timeout:
        print(f"[{source_name}] Error fetching RSS {url}: Request timed out.")
    except requests.exceptions.RequestException as e:
        print(f"[{source_name}] Error fetching RSS {url}: Network or HTTP error: {e}")
    except Exception as e:
        print(f"[{source_name}] Error fetching RSS {url}: An unexpected error occurred: {e}")
    return []

# --- æ ¸å¿ƒæŠ“å–å‡½æ•°ï¼šWeb (é›ªçƒ) ---

def fetch_web_page(url: str, source_name: str, selector: str, limit: int = 15) -> List[Dict]:
    """æŠ“å–ç½‘é¡µï¼ˆä¸“ç”¨äºé›ªçƒï¼‰ï¼Œè¿‡æ»¤'åŸºé‡‘'ã€'å®ç›˜'ç­‰å…³é”®è¯ã€‚"""
    filtered_items = []
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://xueqiu.com/'
        }
        response = requests.get(url, timeout=10, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        items = soup.select(selector)
        
        for item in items[:limit]:
            title_tag = item
            if not title_tag:
                continue
            
            title = title_tag.get_text(strip=True)
            link = title_tag.get('href', '')
            
            if link and not link.startswith('http'):
                link = f"https://xueqiu.com{link}"
            
            # Web æŠ“å–çš„æ—¶é—´å’Œæ‘˜è¦é€šå¸¸ä¸å‡†ç¡®ï¼Œæˆ–éœ€è¦æ·±åº¦è§£æï¼Œè¿™é‡Œæ²¿ç”¨ N/A
            parent = item.parent.parent
            summary_tag = parent.select_one('.search-summary, .search-snippet, .search-content')
            
            summary_raw = summary_tag.get_text(strip=True) if summary_tag else title
            summary = clean_html_summary(summary_raw, max_len=400)
            
            if re.search(r'åŸºé‡‘|å®ç›˜|è§‚ç‚¹|ç»éªŒ|æ¨è|ç­–ç•¥', title + summary, re.IGNORECASE):
                filtered_items.append({
                    'title': title,
                    'link': link if link else 'N/A',
                    'pubDate': 'N/A',Â 
                    'summary': summary,
                    'source': source_name
                })
        return filtered_items
        
    except requests.exceptions.Timeout:
        print(f"[{source_name}] Error fetching web {url}: Request timed out.")
    except requests.exceptions.RequestException as e:
        print(f"[{source_name}] Error fetching web {url}: Network or HTTP error: {e}")
    except Exception as e:
        print(f"[{source_name}] Error fetching web {url}: An unexpected error occurred: {e}")
    return []


# --- ã€æ ¸å¿ƒã€‘æ–°é—»åˆ†æå‡½æ•°ï¼šåŸºäºè§„åˆ™åŒ¹é…ï¼ˆä¾§é‡é€šç”¨æ€§ï¼‰ ---
def analyze_news(news_items: List[Dict]) -> Dict:
    """
    åŸºäºå…³é”®è¯å’Œè§„åˆ™åº“ï¼Œä»æ–°é—»åˆ—è¡¨ä¸­æå–æŠ•èµ„çº¿ç´¢å’Œç»éªŒæ•™è®­ã€‚
    æ–°å¢ï¼šä¸ºæ‰€æœ‰æ–°é—»ç”Ÿæˆè¯¦ç»†åˆ†æå’Œæ½œåœ¨å½±å“ã€‚
    """
    analysis = {
        'investment_clues': [],
        'experience_lessons': [],
        'industry_trends': [],
        'detailed_analyses': []Â  # æ–°å¢ï¼šæ‰€æœ‰æ–°é—»çš„è¯¦ç»†åˆ†æ
    }

    seen_clues = set()
    seen_lessons = set()
    seen_trends = set()

    for item in news_items:
        text = item['title'] + item['summary']
        
        # 1. åŒ¹é…æŠ•èµ„çº¿ç´¢
        for pattern, focus in CLUES_MAP.items():
            if re.search(pattern, text, re.IGNORECASE) and focus not in seen_clues:
                analysis['investment_clues'].append({
                    'focus': focus,
                    'title': item['title'],
                    'link': item['link'],
                })
                seen_clues.add(focus)
                
        # 2. åŒ¹é…ç»éªŒæ•™è®­
        for pattern, lesson in LESSONS_MAP.items():
            if re.search(pattern, text, re.IGNORECASE) and lesson not in seen_lessons:
                analysis['experience_lessons'].append({
                    'lesson': lesson,
                    'title': item['title'],
                    'link': item['link'],
                })
                seen_lessons.add(lesson)

        # 3. åŒ¹é…è¡Œä¸šè¶‹åŠ¿
        for pattern, trend in TRENDS_MAP.items():
            if re.search(pattern, text, re.IGNORECASE) and trend not in seen_trends:
                analysis['industry_trends'].append({
                    'trend': trend,
                    'title': item['title'],
                    'link': item['link'],
                })
                seen_trends.add(trend)
        
        # æ–°å¢ï¼šä¸ºæ‰€æœ‰æ–°é—»ç”Ÿæˆè¯¦ç»†åˆ†æ
        detailed = detailed_analyze_news(item)
        analysis['detailed_analyses'].append(detailed)
                
    return analysis

# --- æ–°å¢ï¼šé•¿æœŸè¶‹åŠ¿åˆ†æå‡½æ•° ---
def generate_trend_analysis(db_manager: DatabaseManager) -> str:
    """å¯¹æ¯”è¿‡å» 7 å¤©å’Œå‰ 7 å¤©çš„ä¸»é¢˜çƒ­åº¦å˜åŒ–ã€‚"""
    
    # è·å–è¿‘ä¸¤å‘¨æ•°æ®
    recent_topics = db_manager.get_topics_by_time_range(days=7) # P1: è¿‘ 7 å¤©
    previous_topics = db_manager.get_topics_by_time_range(days=14) # P2: è¿‘ 14 å¤©
    
    # è·å– P2 ä¸­çš„ä¸»é¢˜ï¼Œç„¶åå‡å» P1 ä¸­åŒ…å«çš„ä¸»é¢˜ï¼Œå¾—åˆ° P2-P1 (å‰ 7 å¤©)
    previous_period_topics = {
        topic: count for topic, count in previous_topics.items() 
        if topic not in recent_topics or count > recent_topics.get(topic, 0)
    }
    
    # è°ƒæ•´ P2-P1 è®¡æ•°
    p2_only_topics = {}
    for topic, total_count in previous_topics.items():
        recent_count = recent_topics.get(topic, 0)
        p2_only_topics[topic] = total_count - recent_count
    
    p1_topics = recent_topics
    p0_topics = {k: v for k, v in p2_only_topics.items() if v > 0} # ç¡®ä¿æ˜¯å‰ 7 å¤©çš„ç‹¬æœ‰è®¡æ•°
    
    # ç»Ÿè®¡æ‰€æœ‰å‡ºç°è¿‡çš„ä¸»é¢˜
    all_topics = set(p1_topics.keys()) | set(p0_topics.keys())
    
    # ç”ŸæˆæŠ¥å‘Š
    trend_report = "\n### ğŸ“ˆ ä¸»é¢˜çƒ­åº¦å˜åŒ– (è¿‘ 7 å¤© vs å‰ 7 å¤©)\n"
    trend_report += "å¯¹æ¯”æ˜¾ç¤ºäº†ä¸»è¦æŠ•èµ„çº¿ç´¢å’Œè¡Œä¸šè¶‹åŠ¿çš„å…³æ³¨åº¦å˜åŒ–ï¼Œå˜åŒ–ç‡ $> 50\%$ çš„ä¸»é¢˜å°†è¢«é«˜äº®ã€‚\n\n"
    trend_report += "| ä¸»é¢˜ | è¿‘ 7 å¤© (P1) | å‰ 7 å¤© (P0) | å˜åŒ–ç‡ | è¶‹åŠ¿ |\n"
    trend_report += "| :--- | :---: | :---: | :---: | :---: |\n"
    
    sorted_topics = sorted(list(all_topics), key=lambda x: p1_topics.get(x, 0), reverse=True)
    
    for topic in sorted_topics:
        count_p1 = p1_topics.get(topic, 0)
        count_p0 = p0_topics.get(topic, 0)
        
        if count_p1 == 0 and count_p0 == 0:
            continue

        if count_p0 > 0:
            change_rate = (count_p1 - count_p0) / count_p0
            trend_icon = "â¬†ï¸" if change_rate > 0.1 else ("â¬‡ï¸" if change_rate < -0.1 else "â†”ï¸")
            trend_str = f"{change_rate:.0%}"
        elif count_p1 > 0:
            # P0 ä¸º 0ï¼ŒP1 > 0ï¼Œè§†ä¸ºæ–°çƒ­ç‚¹
            change_rate = float('inf')
            trend_icon = "ğŸ”¥"
            trend_str = "NEW"
        else:
            change_rate = 0
            trend_icon = "â–"
            trend_str = "0%"

        # é«˜äº®æ˜¾è‘—å˜åŒ–
        if abs(change_rate) > 0.5 and change_rate != float('inf'):
             trend_str = f"**{trend_str}**"
        
        trend_report += f"| {topic} | {count_p1} | {count_p0} | {trend_str} | {trend_icon} |\n"

    if not all_topics:
        trend_report += "æš‚æ— è¶³å¤Ÿå†å²æ•°æ®è¿›è¡Œé•¿æœŸè¶‹åŠ¿åˆ†æã€‚\n"

    return trend_report


# --- æ–°å¢ï¼šç”Ÿæˆç»Ÿè®¡å›¾è¡¨ ---
def generate_stats_chart(analysis: Dict, output_file: str):
    """ä½¿ç”¨matplotlibç”Ÿæˆç®€å•æ¡å½¢å›¾ï¼Œå±•ç¤ºç±»åˆ«è®¡æ•°ã€‚"""
    clue_count = len(analysis['investment_clues'])
    lesson_count = len(analysis['experience_lessons'])
    trend_count = len(analysis['industry_trends'])
    
    categories = ['Investment Clues', 'Experience Lessons', 'Industry Trends']
    counts = [clue_count, lesson_count, trend_count]
    
    plt.figure(figsize=(8, 5))
    plt.bar(categories, counts, color=['blue', 'orange', 'green'])
    plt.title('News Analysis Categories Count (Current Run)')
    plt.ylabel('Count')
    plt.savefig(f'{output_file}_stats.png')
    plt.close()
    print(f"Generated stats chart: {output_file}_stats.png")

# --- ç”Ÿæˆåˆ†ææŠ¥å‘Š ---
def generate_analysis_report(analysis: Dict, total_count: int, trend_report: str) -> str:
    """æ ¹æ®åˆ†æç»“æœç”Ÿæˆç»“æ„åŒ– Markdown æŠ¥å‘Šã€‚æ–°å¢è¯¦ç»†åˆ†æéƒ¨åˆ†ã€ç»Ÿè®¡æ¦‚è¿°å’Œè¶‹åŠ¿åˆ†æã€‚"""
    md_report = "\n---\n"
    md_report += "# ğŸ“° åŸºé‡‘æŠ•èµ„ç­–ç•¥åˆ†ææŠ¥å‘Š\n\n"
    md_report += f"æœ¬æŠ¥å‘Šæ ¹æ®ä» {total_count} æ¡æ–°é—»ä¸­æå–çš„é«˜ä»·å€¼ä¿¡æ¯ç”Ÿæˆï¼Œæ—¨åœ¨ä¸ºæ‚¨æä¾› **ä¹°å…¥æŒ‡å¼•ã€é£é™©è§„é¿å’Œè¡Œä¸šæ´å¯Ÿ**ã€‚\n\n"

    # æ–°å¢ï¼šç»Ÿè®¡æ¦‚è¿°
    md_report += "## ğŸ“Š ç»Ÿè®¡æ¦‚è¿°\n"
    md_report += f"- æœ¬æ¬¡æŠ“å–æŠ•èµ„çº¿ç´¢æ•°é‡: {len(analysis['investment_clues'])}\n"
    md_report += f"- æœ¬æ¬¡æŠ“å–ç»éªŒæ•™è®­æ•°é‡: {len(analysis['experience_lessons'])}\n"
    md_report += f"- æœ¬æ¬¡æŠ“å–è¡Œä¸šè¶‹åŠ¿æ•°é‡: {len(analysis['industry_trends'])}\n"
    md_report += f"- æ€»æ–°é—»æ¡ç›®: {total_count}\n\n"
    
    # å¼•å…¥é•¿æœŸè¶‹åŠ¿åˆ†æ
    md_report += "## é•¿æœŸè¶‹åŠ¿åˆ†æ\n"
    md_report += trend_report

    # 1. æŠ•èµ„çº¿ç´¢
    md_report += "\n## ğŸ’° æŠ•èµ„çº¿ç´¢ä¸å¸‚åœºç„¦ç‚¹ (ä¹°å…¥æŒ‡å¼•)\n"
    if analysis['investment_clues']:
        md_report += "| ç„¦ç‚¹æ ‡çš„/ç­–ç•¥ | åŸå§‹æ ‡é¢˜ (ç‚¹å‡»æŸ¥çœ‹) |\n"
        md_report += "| :--- | :--- |\n"
        
        for clue in analysis['investment_clues']:
            md_report += f"| **{clue['focus']}** | [{clue['title']}](<{clue['link']}>) |\n"
    else:
        md_report += "æš‚æ— æ˜ç¡®çš„æŠ•èµ„çº¿ç´¢æˆ–æœºæ„è§‚ç‚¹è¢«è¯†åˆ«ã€‚\n"
        
    # 2. ç»éªŒä¸æ•™è®­
    md_report += "\n## âš ï¸ æŠ•èµ„ç»éªŒä¸é£é™©è§„é¿ (é¿å…è¸©å‘)\n"
    if analysis['experience_lessons']:
        md_report += "| æ•™è®­/ç»éªŒ | åŸå§‹æ ‡é¢˜ (ç‚¹å‡»æŸ¥çœ‹) |\n"
        md_report += "| :--- | :--- |\n"
        
        for lesson in analysis['experience_lessons']:
            md_report += f"| **{lesson['lesson']}** | [{lesson['title']}](<{lesson['link']}>) |\n"
    else:
        md_report += "æš‚æ— æ˜ç¡®çš„ç»éªŒæ•™è®­æˆ–é£é™©æç¤ºè¢«è¯†åˆ«ã€‚\n"

    # 3. è¡Œä¸šç»“æ„ä¸è¶‹åŠ¿
    md_report += "\n## âœ¨ è¡Œä¸šç»“æ„ä¸æœªæ¥è¶‹åŠ¿ (é•¿æœŸæ´å¯Ÿ)\n"
    if analysis['industry_trends']:
        md_report += "| è¡Œä¸šè¶‹åŠ¿ | åŸå§‹æ ‡é¢˜ (ç‚¹å‡»æŸ¥çœ‹) |\n"
        md_report += "| :--- | :--- |\n"
        
        for trend in analysis['industry_trends']:
            md_report += f"| **{trend['trend']}** | [{trend['title']}](<{trend['link']}>) |\n"
    else:
        md_report += "æš‚æ— æ˜ç¡®çš„è¡Œä¸šè¶‹åŠ¿æˆ–ç»“æ„å˜åŒ–è¢«è¯†åˆ«ã€‚\n"

    # æ–°å¢ï¼šè¯¦ç»†æ–°é—»åˆ†æä¸æ½œåœ¨å½±å“
    md_report += "\n## ğŸ” æ‰€æœ‰æ–°é—»è¯¦ç»†åˆ†æä¸æ½œåœ¨å½±å“\n"
    if analysis['detailed_analyses']:
        md_report += "| æ–°é—»æ ‡é¢˜ | å…³é”®ä¸»é¢˜ | æƒ…æ„Ÿåˆ†æ | æ½œåœ¨å½±å“ |\n"
        md_report += "| :--- | :--- | :--- | :--- |\n"
        
        for det in analysis['detailed_analyses']:
            topics_str = '; '.join(det['key_topics']) if det['key_topics'] else 'æ— ç‰¹å®šä¸»é¢˜'
            md_report += f"| {det['title']} | {topics_str} | {det['sentiment']} | **{det['potential_impact']}** |\n"
    else:
        md_report += "æš‚æ— è¯¦ç»†åˆ†æã€‚\n"

    return md_report


# --- æ•°æ®æºé…ç½®å¤–éƒ¨åŒ– (ä½¿ç”¨ä¸Šä¸€ä¸ªç‰ˆæœ¬çš„æ‰©å±•é…ç½®) ---
proxy_base = 'https://rsshub.rss.zgdnz.cc'
sources = [
    # --- åŸæœ‰ RSS Hub ä»£ç†æº ---
    {'url': f'{proxy_base}/cls/telegraph/fund', 'name': 'è´¢è”ç¤¾-åŸºé‡‘ç”µæŠ¥', 'type': 'rss'},
    {'url': f'{proxy_base}/eastmoney/report/strategyreport', 'name': 'ä¸œæ–¹è´¢å¯Œ-ç­–ç•¥æŠ¥å‘Š', 'type': 'rss'},
    {'url': f'{proxy_base}/gelonghui/home/fund', 'name': 'æ ¼éš†æ±‡-åŸºé‡‘', 'type': 'rss'},
    {'url': f'{proxy_base}/stcn/article/list/fund', 'name': 'è¯åˆ¸æ—¶æŠ¥-åŸºé‡‘åˆ—è¡¨', 'type': 'rss'},
    {'url': f'{proxy_base}/21caijing/channel/%E8%AF%81%E5%88%B8/%E8%B5%A2%E5%9F%BA%E9%87%91', 'name': '21è´¢ç»-èµ¢åŸºé‡‘', 'type': 'rss'},
    {'url': f'{proxy_base}/xueqiu/fund', 'name': 'é›ªçƒ-åŸºé‡‘RSS', 'type': 'rss'},Â  
    {'url': f'{proxy_base}/zhihu/topic/19550517', 'name': 'çŸ¥ä¹-åŸºé‡‘è¯é¢˜', 'type': 'rss'},Â 
    {'url': f'{proxy_base}/sina/finance/fund', 'name': 'æ–°æµªè´¢ç»-åŸºé‡‘ (ä»£ç†)', 'type': 'rss'},Â  
    
    # --- åŸæœ‰ Web æŠ“å–æº ---
    {
        'url': 'https://xueqiu.com/k?q=%E5%9F%BA%E9%87%91',
        'name': 'é›ªçƒ-åŸºé‡‘æœç´¢ (Web)',
        'type': 'web',
        'selector': '.search__list .search-result-item .search-title a'Â 
    },
    {'url': 'https://blog.csdn.net/category_10134701.html?spm=1001.2101.3001.5700', 'name': 'CSDN-åŸºé‡‘åšå®¢ (Web)', 'type': 'web', 'selector': '.blog-list-box .title a'},Â 

    # --- æ–°å¢ç›´æ¥ RSS æº ---
    {'url': 'http://rss.eastmoney.com/rss_partener.xml', 'name': 'ä¸œæ–¹è´¢å¯Œ-åˆä½œä¼™ä¼´ (RSS)', 'type': 'rss'},
    {'url': 'http://rss.sina.com.cn/finance/fund.xml', 'name': 'æ–°æµªè´¢ç»-åŸºé‡‘è¦é—» (RSS)', 'type': 'rss'},
    {'url': 'http://rss.sina.com.cn/roll/finance/hot_roll.xml', 'name': 'æ–°æµªè´¢ç»-è¦é—»æ±‡æ€» (RSS)', 'type': 'rss'},
    {'url': 'https://dedicated.wallstreetcn.com/rss.xml', 'name': 'åå°”è¡—è§é—» (RSS)', 'type': 'rss'},
    {'url': 'https://36kr.com/feed', 'name': '36æ°ª (RSS)', 'type': 'rss'},
    {'url': 'https://www.hket.com/rss/china', 'name': 'é¦™æ¸¯ç¶“æ¿Ÿæ—¥å ± (RSS)', 'type': 'rss'}, 
    {'url': 'http://news.baidu.com/n?cmd=1&class=stock&tn=rss&sub=0', 'name': 'ç™¾åº¦-è‚¡ç¥¨ç„¦ç‚¹ (RSS)', 'type': 'rss'},
    {'url': 'https://www.chinanews.com.cn/rss/finance.xml', 'name': 'ä¸­æ–°ç½‘è´¢ç» (RSS)', 'type': 'rss'},
    {'url': 'https://www.marketwatch.com/rss/topstories', 'name': 'MarketWatch-å›½é™…è¦é—» (RSS)', 'type': 'rss'},
    {'url': 'https://www.stats.gov.cn/sj/zxfb/rss.xml', 'name': 'å›½å®¶ç»Ÿè®¡å±€-æœ€æ–°å‘å¸ƒ (RSS)', 'type': 'rss'}, 
]

def generate_markdown(news_items: List[Dict], analysis_report: str, timestamp_str: str) -> str:
    """
    ç”ŸæˆMarkdownã€‚åœ¨æ–°é—»åˆ—è¡¨å‰æ’å…¥åˆ†ææŠ¥å‘Šã€‚
    """
    md_content = f"# åŸºé‡‘æ–°é—»èšåˆ ({timestamp_str})\n\n"
    # è°ƒæ•´æºåç§°æå–ï¼Œé¿å…å†—ä½™ä¿¡æ¯
    configured_sources = list(set([s['name'].split('(')[0].strip() for s in globals().get('sources', [])]))
    source_names = "ã€".join(configured_sources)
    md_content += f"æ¥æºï¼š{source_names}ï¼ˆå…³é”®è¯ï¼šåŸºé‡‘/å®ç›˜/è§‚ç‚¹/ç»éªŒ/æ¨è/ç­–ç•¥/æŠ•èµ„/å®è§‚/é‡‘èï¼‰ã€‚æ€»è®¡ {len(news_items)} æ¡ã€‚\n"
    
    # æ’å…¥åˆ†ææŠ¥å‘Š
    md_content += analysis_report
    
    # æ’å…¥åŸå§‹æ–°é—»åˆ—è¡¨
    md_content += "\n---\n# åŸå§‹æ–°é—»åˆ—è¡¨\n\n"
    for i, item in enumerate(news_items, 1):
        md_content += f"## {i}. {item['title']} ({item['source']})\n"
        md_content += f"- **é“¾æ¥**: [{item['link']}]({item['link']})\n"
        md_content += f"- **æ—¶é—´**: {item['pubDate']}\n"
        md_content += f"- **æ‘˜è¦**: {item['summary']}\n\n"
    return md_content

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°ï¼Œåè°ƒæŠ“å–ã€åˆ†æã€å»é‡å’Œæ–‡ä»¶ç”Ÿæˆã€‚"""
    
    # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
    db_manager = DatabaseManager()
    
    tz = pytz.timezone('Asia/Shanghai')
    now = datetime.now(tz)
    timestamp_str = now.strftime('%Y-%m-%d %H:%M:%S')
    date_str = now.strftime('%Y%m%d')Â 
    output_file = f'fund_news_{date_str}.md'
    
    all_news = []
    print(f"[{timestamp_str}] å¼€å§‹æŠ“å–åŸºé‡‘æ–°é—» (å·²æ‰©å±•æ¥æºå’Œé€šç”¨å…³é”®è¯)...")
    
    # 1. è·å–å·²å­˜åœ¨çš„é“¾æ¥ï¼Œç”¨äºè·¨æ¬¡å»é‡
    existing_links = db_manager.get_existing_links()
    
    for source in sources:
        print(f"å¤„ç†æ¥æº: {source['name']}")
        if source['type'] == 'rss':
            items = fetch_rss_feed(source['url'], source['name'], limit=20) 
        else:
            items = fetch_web_page(source['url'], source['name'], source.get('selector'), limit=15)
        
        # é’ˆå¯¹å½“å‰æ‰¹æ¬¡è¿›è¡Œå»é‡
        current_batch_unique = []
        batch_seen = set()
        for item in items:
            link = item.get('link', 'N/A')
            # è·¨æ¬¡å»é‡
            if link and link != 'N/A' and link in existing_links:
                continue
            # æ‰¹æ¬¡å†…å»é‡
            if (item['title'], item['source']) not in batch_seen:
                current_batch_unique.append(item)
                batch_seen.add((item['title'], item['source']))
                if link and link != 'N/A':
                    existing_links.add(link) # æå‰åŠ å…¥ï¼Œé¿å…æœ¬æ‰¹æ¬¡å†…é‡å¤

        all_news.extend(current_batch_unique)

    # 2. æ’åºï¼šæŒ‰æ—¶é—´å€’åºæ’åˆ—
    def sort_key(item):
        time_str = item['pubDate']
        if time_str and time_str != 'N/A':
            try:
                return datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
            except ValueError:
                pass
        return datetime(1900, 1, 1)

    unique_news = all_news # all_news å·²ç»åŒ…å«äº†æ‰¹æ¬¡å»é‡å’Œè·¨æ¬¡å»é‡çš„é€»è¾‘
    unique_news.sort(key=sort_key, reverse=True)
    
    # 3. è¿è¡Œåˆ†æå’Œå­˜å‚¨
    analysis_results = analyze_news(unique_news)
    
    # å°†æ–°çš„æ–°é—»åŠå…¶åˆ†æç»“æœå­˜å…¥æ•°æ®åº“
    for item, detailed_analysis in zip(unique_news, analysis_results['detailed_analyses']):
        # item: åŸå§‹æ–°é—»æ•°æ®
        # detailed_analysis: åŒ…å« sentiment å’Œ key_topics çš„åˆ†æç»“æœ
        db_manager.store_news_and_analysis(item, detailed_analysis)
    
    # 4. ç”Ÿæˆé•¿æœŸè¶‹åŠ¿åˆ†æ
    trend_report_md = generate_trend_analysis(db_manager)
    
    # 5. ç”ŸæˆæŠ¥å‘Š
    analysis_report_md = generate_analysis_report(analysis_results, len(unique_news), trend_report_md)
    
    # ç”Ÿæˆç»Ÿè®¡å›¾è¡¨
    generate_stats_chart(analysis_results, date_str)
    
    # ç”ŸæˆMD
    md_content = generate_markdown(unique_news, analysis_report_md, timestamp_str)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    print(f"æ”¶é›†åˆ° {len(unique_news)} æ¡ç‹¬ç‰¹åŸºé‡‘æ–°é—»ã€‚åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆå¹¶ä¿å­˜è‡³ {output_file}")
    
    print("\n--- åˆ†ææŠ¥å‘Šæ‘˜è¦ ---")
    print(analysis_report_md.split('## ğŸ’°')[0])

    # å…³é—­æ•°æ®åº“è¿æ¥
    db_manager.close()

if __name__ == "__main__":
    main()
